{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a classifier to idenify images containing rust.\n",
    "\n",
    "The classifier will be trained on a small set of images taken from a small subsample of HCC bridge inspection photographs.\n",
    "\n",
    "As the set of training images is small transfer learning will be used using the Xception neural net in Keras pretrained on the Imagenet dataset.\n",
    "\n",
    "To supplement the training data image preprocessing will be used generate additional transformed images for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lnath0\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras import applications\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras import backend as k \n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "\n",
    "img_width, img_height = 224, 224\n",
    "train_data_dir = \"data/train\"\n",
    "validation_data_dir = \"data/validation\"\n",
    "nb_train_samples = 777 + 761\n",
    "nb_validation_samples = 194 + 190\n",
    "batch_size = 8\n",
    "epochs = 50\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=30,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.1,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 777 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# Test the datagenerator on a sample image from the rust training images\n",
    "\n",
    "# img = load_img('data/train/rust/5_0052 (31) (Small).jpg')  # this is a PIL image\n",
    "# x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "# x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "\n",
    "# the .flow() command below generates batches of randomly transformed images\n",
    "# and saves the results to the `preview/` directory\n",
    "i = 0\n",
    "for batch in datagen.flow_from_directory(\"data/train\", batch_size=1, target_size=(img_width, img_height),\n",
    "                                         save_to_dir='preview', save_prefix='no_rust', save_format='jpeg'):\n",
    "    i += 1\n",
    "    if i > 2000:\n",
    "        break  # otherwise the generator would loop indefinitely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a\nHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIy\nMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAHgAoADASIA\nAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQA\nAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3\nODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWm\np6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEA\nAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSEx\nBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElK\nU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3\nuLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwClZX8W\nooHRgs46g9G9jWvY6i0B8twdndT1X/61eeq0trMHQ7XH5NXTafqkd+gSRtk69G/xrhlBxOyFRS3P\nQLa6Rotr4kgb9Kimt3smE0BLwHuOq1ztney2su0jj+JOx9xXR2d6rR7kIeJuqmsZR6o6IzaZYtrg\n582Ajd/EnZq1Y7uGW3YsNyEYdCM/gaw57Qxf6TZndH1ZO4qxbtIW8xmETMNpBHJ96y2NWlNXRyOp\nvDZa9ILciNGOdp5xWlHq0EEfnjDyPjzQvbHTFLr2nxXG2+CbpMFWZR6Vz6AR3DRocoQMNnv6VrBX\ndzCbsrI7CS4ttRsQkaSfvB9zNVbSyMUy4PBIChznH0rM027aAvbsSFPKn+63+FbccrItmJso27LH\nb1GetbtJxMaU9dTMkSW51uSOGZXwmT/Dg1HNBJiRJ1RZByFbILLV7V8aXeC7jgV0L581O49xVCfW\nG1q6a5KhJdmxV7ZFcy5k9C5OLdmQNo8eqWbtal4ZoMbFJydh7ZrA1Gyks4VtLiIpLuyjY5auhtZJ\n7LcybUbPzg+npVLWNXtrrBnt5D5XG8H7td0JJqxyVItPQw/DaXE2onyDCZI0cgyHG0Ac496iSbUd\nXvraC5lM0oiwrSt/COeTWxBaw28lzcafOLg/Z2ZcrtILdazLGye58RW6CUxKNo3Eeg6YppKMrkt3\nRk3IlhuTFNGFdeoU5wTSOkhk8rawZhkAjGa6DxHAljrZuBHHmTEgQ5OMf44qLW9cs9RtIprSJops\ngyrtGAcYODVKa3HyXRjzRXVjMiSsPmHUHgcVZigR7Tz2ZSMkdec1o6xGuoafBcxuJJCFJOcnpg5r\nAldkiMZYMFBHFUpJiceV2JTtDEbsD7wK11fh7T5byIeZM377KYA5bvzXGQ7QCsYJZvu/Suo8GazF\npmpg3EriJATjbmplqioWT1DUdHQ3iy2cTBFH70ds9Mj3qxZwafM7wyXm5pCqRwhcKD0zn1rbtXk1\nMSu7RQws++MZ+8vNcpd2FldT3ckMjRPESy89fpURm3oy5wW6HDRtt7cNcfuYoGKl0cEZxxQ1hNdW\nwjWBZpEQtviHOPeqVpqCxhrWbJhkGJFPcf41e0e+j0+6VWIeI4CnPOM96ttozXLcz00y4uFBSMrG\nSFZyOEPvTXtpLOea3UowhOXxzu966fVNagsbO7trDDfaG+bPIBrmLW8nsrz7YqZz8rqy8HPrThPm\n1FJJbFuzdWgLRkHuQT1q0shcZYd+uKzIJETeVQJvJIA6Cr9u+VB6joapbkjnBQHsp7mkMqPCNyjI\nGMjjmn79wPtxzUMiokZc8j+RotcBkLRzajHvkEURyHJGR071JZavepp80LRh7ZTsQuM4+lM0qwa6\nDzTsi2o6uTnn6VPdtHFp0kNvdcdGTAxkHgipsUmY8YdbjhTx6DvWjGfOH3iGzjHr+NT6Fqcdk6GZ\nAHBKvkZyD7VAwH2mQRHbHuJXIqnZk7FyFygVFUHaaWZrd1fzFYSqMqwPH40y2jEibpTtUNtOG59q\nWHETvuTsRnrQBftYImmETuvIyGVuOlTI0kMIjjbEjHkoe1Z0dsiqZhnGQuM1ZjuCNxiC5HRsUWC5\nPAArtIyFlX7xBp8k0M9oSGO8HAVvSqALKpBfduHNSp8yqFwpA796VgZDKgVvl5OOM0y0tfvO2Aq8\nketWfKcjcACcHNQLM0q7CpTacPgdqYg5upvRB+gqLU70W8Qij++3AHpWnfNbwQ+bA6lWQEKFxz7+\n9csC80rTSfeJ4HpVjFij2jnqeSakHPOOOwoAzx27mpFGBuP4VD1GH3R7nrUkMeTuNMjQyNk9Kt5E\nSbjSk+hDGyyeUmB941Ci9SfqTSDMj7j1PT2p5OMBevb/ABppWGkGDI+wfj/hWpbw+WoUdT1PoKgt\nINihiMk9PerMzsgEUfMsnFS2UNkIncwhtsKDMjeg9Kz5ZTfXA2jbBHwg9B61JeSAAWMByAcysP4m\n9Kkt4VVTuOI05dvX2oQ0PTEEYlK5J+WJPU+tWWzp9uUzuvJ+XP8AdHpRBhFOo3AwBxBGf502ytZN\nSvG8xsLjdM/9xfT6mspO5SRY0u2iiibUbnBghP7sEf6x/X6CsjUL6S6neeQks3Qegq7rOorcSCGE\nBbaEbUUdOKxC2f3rdP4R/WiKvqDYjExqc/fbr7e1LBCZXCj8TTFDO3qxrRRBbRYH32rVLqR5iuQi\niJOg61LGnlqDj526D0FMhjHMj/dH6mnyOwOF/wBa/T2FYzld2HFdWNb943lA/IvMjevtXK+JdZ3H\n7PCflHHFamuammnWht4m+cj5jXBySNLIZGPJpxRTY33PJNaeladJdToiqSzGqlpbtNIDj6CvR9Fs\nItE0039yv70j5F710fCiSaUxeH9KW2hx9pkHJHatXwF4Vk1a+F7cofKU5Ge9Ymi6ZdeKNbGQShbL\nHtj0r33R9Lh0uxjt4lACjnFctSTb5EUlbVl2CFIYlRBhVGABUvU0ewpegrohFU42IbuB9KBQKUU0\nr6gLRRRVCCiiigAooooA+TiAV2PyvY+lQsrwSBlbBH3WHepEkz8r4z2PY0/G0FSMoe3pUNXEpNGz\npmrpdKLe5O1x0buK27e5ltJgcjnv2cf41wkkRjIZScfwsO1b2h6hNduLJ4zKx+6ByT9K5ZwcdUdl\nOqnozu4b53i3QnaSOQfWqzXlw14IpI/McjDMPXtioXkjt5LbbLmVMeaP6Gt6WOCKS1u4wrAtnHYV\nnyKSujdTcSrFcLEY7Mq+yTCurr0PrVTVdGW2ZwqKD1yT1q3dXaPMJgQZA3Qf410Plyajai5dI9u3\nB3LyMd6TstBt63OBghmtlRzEpIP385BFasFgzybfPyTHui+bI69qgvLXZdj7PlkB5APyk+lVrdLy\nGXb80bKSyqeqilqupm49UaF9eXN5Z3GnTxiJo0OOMbq5vSrWaK9FvcKcHlWrpGmjvZIxNJ8xG0OO\nK523le31B4p5t+HKqwPIqk5a2E4qWrNG6t5Sk0SgMQfmB61zl/8AZ4pwEdzFJhZYz1I711gLeXv8\nsltud5BBzVHUfDz3VrJM8JhlCbo5c4ANVTbJqQVro5yS6t47aUogJAAhKnDAdfxpG1q62wTOEKqB\ngqMHj1rNvZjHPsA+QDk4wSe9JDJCFEaMQWGCD0rq0a1OVa6o0dYv59WitpUhISM7euck9s+lZEsd\nuoAJYnlXQnoc1IHeBWiR9oBDhc8E02O3+1F5IY2JC5KZ7+1VBNIl2bHWO795DDKQrcbM9aoSxyRq\n8TDbIvBBHNStG8E27DAqQwx1qzIhu4hO4b5DyccGnYCrbTGBY5lYB1PTFXbhg5+1RqMPySOx+lLf\n2FrC9tJFOsolTLIn8B96ht45I5DCMYcdqncoabq7eNY47l9kZysYP5it7Qda/s63WC5jURtJujkk\nTOG9M1zzxSQS70BVkOKjnurlkEc5LRE7h6UnEfNY3LiC0vfELqsiwwOww+MjOOaqxQRLqcsErYiD\n7fMzwPeo9H0261O4lWJ0CRgPhjz17UmpiZL+SLAAPBYimn0DfUsaraCx1BrcTeciLuWQc5zUkWtG\nW1khaBWVsBsjris6zebzgo+d1br2x71IoIc7CCS5+UD7pppWJZPA3l/eTdn7vPSrSTBXXC4yOcVX\ntrYXMwhLqGJ6noKnubCTTmQSyRyCToyn+dMW5YZE371fnoRUNzGyg7iQvepIypXnAJHGR1p826U4\n254piM4z+UPJj3BX+8M8Gle1J2+WxPHIqw1m0qGdGjyDt2nrUCStDlGBIBxx1pWGN2sTkgE9zU3n\nEkDuOoNPjaKUMvmbSp7jrShQ3JAzjqKBDGfcVJUgA8g02W6aOXB4A9OakABbBHGKZBcxWryLLAJm\nZCqk9j2NAy7DKjJliTk5qRLgFiAMDoTWZa5jU5bJIyVIqyksscasgDrnkd6EBoI6MCCwyPSnZcJh\neuMgmqkrLIvnBAuOCB1FTRStKhBGD2zQBZjYsm8kjIxwahkhZd7q3B6mpF+zRo6uXL4+THY047C3\nliTKEcketAEBAkAQkkdMH1qpNbfd2LyeK05omtRHG0iN8oIK89aiJ81GVVII6UhmV5ZVirDGOtHL\ntgVektzIgxwaiih29evrTbsiWx0aBV9hUEj+a/8Asj9afPJn92h+ppqLgZ/KlFAhfurz+NT2kBdt\n78D+VRQxmaQY6D9a1Y0VVx/CvX3obKFZ1hjMjccfKKqTzNaRFyf9KmHH+wtSNKpDXcv+pjOI1/vt\nVGMPcTNcTZJJ6f0qdxofa25GAPvt69h61fhhW5k8sHFrBzI394+lMCSblt4ubiX7x/uLT7uSOGIW\nUBxFHzIw7mpnLohpXElkkv7tFiTIzshj/r9BVvUp002zGm2z5c/NPJ6mnxY0aw+1SDF5OuI0/wCe\naf4mucnld3POZHPJqEr6FPQjc+YxXoi9T/Sombe2f4R0FK5AAjU8DqfWp7SDzG3H7i1qkZvUmtYR\nGnnP17CpEVppMn8T6ChyZHCqOBwBU4UIuwH3dqmpO2iBK7Ed1Vd2PkXhR6mql5drp9q88pHnOOB6\nVM0iYNxJxFH90HvXDa9qz31yyq3yg1nFFtmff3j3tyzsSRmoY4zK+B070wAkhR1NdJ4d0V765RAv\nyDljXTCNldkmx4V0NWP2y4GIY+ee9XbyebXNSS2tlOzO1FHp61PrF4kES6ZZj5V4bHc+ld78OvCP\nkRjUbtPnblQRWVWpbXqOKudL4M8MxaJpqZQecwyTius6DHekVQo6fSlAzyadGnyrme4pO4oGBmjr\nQeaUVpuxABS0UVQgooooAKKKKACiiigD5MmheJ2ilQqwOCDSJIV+Vzx2b/GvZvF3ge31mF7ywUJc\ngZZR/FXj13aTWU7wXEZR1OCCKzjO4mhrHywTxtPVTRHfy6dcQ3WnM8MoUqzdQCfSmwDz/wDR2Cr8\n3DMeabLGHhYbSrqcEDvVNXBOxt6fftd2rfaDl+Tu759667SbmbUdPaHcCUHy4rzfTJRDPtYHcD3P\nauz0bVYdMJ8wkhugWudxUWdVOV1qdH5g8gojIXVgjDb3qy+uz2jRB/lVMA46VlprtpMrARYwxf5R\nncf8atqYb8bX+Utxg/SsqiudEJdDoorGJYnuTFGEYbiMdc1h6xHPbtHLIm+MNuEi9dvoa6XSlnuL\na384xpCqhdq87gO5qPU0jnkZNpkjU846AVk482241K2hy1/baebKG9tCzK74aMdVNRRaKZolItSZ\nJOeB8w9akWVdLuPOij8623YdNv8AL1rWF5DLYmWK4KliCkinO0VlGUoablcsWrmMmm3Ns6+dM8pJ\n2KHGAp68j+tM8R2+oRW6AxfuCMJIjZH0rrpb9BaQtf7FAGWbb+tZDX1t9hnUutypUmJAQMVrBvmT\nOerscJouhWd/fPHqtwbZPKOxscFqoX+nx+G9fihleK6gjYSpIhyHX0q9PZXdxIzR4whyF3YJ9qwr\nqJppjuLROOoYZ+tdfK23fY520uh2viOy0bWtBGtaTtgukP72AccfSuNiMST5MpjDp8uB3pYZHtS8\nW9ir4wO1NkjbeGQZ2DLBu1FKMqel7oJ2eo2EGK4cvy5AIDD7wpHJhcqmRAzZ21Y2JdOp3bWX+LpT\nJZiYzFKg3dNwHUVsQhjvC8SrjbOvKug+8Pf3qLf9nJJXLMeueR9Kt/ZIFi8xJ285O2MCodsSuolV\nmDd+4qRjYpDKDvAcAH5e+fWleNLi3RflYxnHlkY4NSRrHDco5QsqEHaB96rJWK4upZGBQMflB45q\nriKlstxpt8stuSpXGQeQcVoJqfzvutkxMMMCM5aoZLfptdl45UnpSOqqV4+ZW6+tS99BplBrKeCZ\nn3EEfepYVzMOhx1NarX6yQtDLbb88iQfeH1pbeztpCJEcNjl1PBFUnbcLXM+OMwypIucZ5UHmnSs\n07ZLblHoORV7UbSFJswbygxknrmoEZ0mVW4yADgdRTQhI8hVJIOfWpFVuueKgKKly5VW2Z7mnKSj\nBV3BSe/OaLgNKlZs9VPQ1Y8mNzgLyabONuGAytPjYkjbkZ6YouFirLCI2O8cHutOlMYJKOxXaOtP\nnDKfmB6ZzUGWDDzF+XGBgUCsJ88jKIj1OOaaqP5jKwVj0PsKt2ysrlFxgnGQOgqrPE0bl1BOOMA4\nouBKllcGZIo42MrfKFPp1pZIporgoymN88A0/wC0z3M4uFTaRggIfSmXN29xN86YHpSY0S2s7MXV\nyCM81cnnjjAUqN2Mbs81TghUuArHLdjT76DABUHK9c96AaJo7iQYWRAwI4apoHUSMFwAeoNUoLlg\nQmMjoPrV3yTLHkpgAZLH1oQiaQiFsSAEE5FRrJslLgEqe1QK+6MxzElxwDViGMNDt5Dj9aYCO4yW\nTpUGVVjlvvnHsKtxzW4jEMkDbxnLZ60G0SVdyE7eoNDaFYzPL2scnPcn1pcF22AfX29qmuVIdGCn\ngYNS2UaEFxyc/rRcZYt4fLUKPvH9BSyHz5PIRtsajMjegp0rmNRGnzTOcAVRvJNi/YYSSc5lYfxH\n0qBkc0v224CoNsEfCD29atx7YYxMVzjiNPU1HbwKFIJwi8u39Ku2+0A6hOMRpxAh7+9EnZFB82n2\n5z815cfe/wBkelS6XaxhGv7k5toTlc/8tX/wFQWltLqd6QzFc/NK/wDcT/E0azqCTMttbjbawDaq\njvWRWyKWpag93cPcSE5P3R6Cs5iUUk/fb9BTi24+Y3QfdHr71GMu2erHpWkUQ2OhiMrhR+JrQfEa\nCJPxpI0FtD/ttT4Ywcu/3R+ppylyonfRD4k8tQ2Pnb7o9Kaw8xvJU/KOZG/pTpHYYxzK/wB0eg9a\nyda1JNNtDDG37xh8xrBau7NNlYzPEusjH2aA/KOOK5LPc8k0+SRppDI55NSWsBmkBx9K3hG+pLLe\nl2ElzOqKpLMa9DbyvDulLDHj7VKOT6VX0LTotH086hcqN+PkU0ulafdeKNbAwSpbLHsB6U5zW/QE\nrmx4E8LyazqAvLhT5KHIz3r3K2gSCFY0ACqMCqOiaTDpVhHbxKBgc1qewrGlB1Jc8ipO2iDqaU+l\nHQUCulu+iIAU6kFLTSsIKKKKACiiigAooooAKKKKAOPs79o2CSHBHeuY+I2jw3mnR39vEgmRx5h9\nRWxbE3QIZCHTr71V1mWNLRtNeGWVpgeV52/Wudxs7oDxq4UhyWVVI5BHtTHkaRmVVBVlBIHUe9X7\n63+z3bW8nO1iD6gVWkVYQDA4kbGfQj2Nap6CZmO4gm+vRvWr6F54lwSHB6dqZdr58KNsCnqcVDbX\nL28qhidnf6USjccW0zotPYrPDuZlz8rgetdhp+n3E7syuWRfl3Y5FYel6cb6UeWM5XerAcfjXSaR\nNJAzZm2jOGX0Nc0lbQ7I33I4vGb6Fq7WOoWf/EsSAnz0RiWb09B6flVXWPiVZ6dcR/ZrOcpLFvfa\nwOzI4z6GtK+gj1GKWNpFQEHP+0KwhpXnM6QW8YtwADhclqTjFrQzlKSlqcvL41v7lld9IdkGdh3H\nHXrgfSoE8WXsE5WS1WFGO4cHBz3rvngFtbKIIU3EgYI/IVW/si3vIZBdWyMSPujgj1xQ+VKzQKTv\noZNn4vsdRtibu7EYVtpV2I3fQGtKa30eUAQbi5GcpLgEVnXHwwiuwl1bSSxozbWVxlRmuW1rw34g\n8MXT20DzmAqDuQZGM9qv2d9Yg5aanUvqUljbyhra28rJYMxwU+hrll8Y6VLORcRzYySXXoazT4a8\nRah5cksE5jflTITgj1rah8AW1zYwxstzHdHO6TaNgPaqSUfekzN3bskCa9oMy7V1BomHIEsWR9M9\nqt2l3aTvIY7yDD9G3DB/Osa7+Gt3Am+K7imUjqoIwfeqcfw91zBItwwGVznqRVxSktGErrRo6VXF\njdpKixuituAJBH6VJqUEN7dG5tflSXkoONp/wrBuPAWu2mmtcrFMAqgkLnvXPA65aSGP/SkKc454\no5Zb3DWx3FlEjmSGdMEA7WPXNRRA5KNlQTj5h0NcWuu6vCdxnPB/iXvV6LxndD5biKOQYzkCmlIR\n1rsVlQhSoVdvI60x/LkQ5xvHQnvWDF41gklRri2dVXoU5/SrEviPSpEMizOWzkqRjFS0xdTcSSL7\nrIwbbyT61KWjcBSq4xw1ZNv4o08KQs0eX5Xd/D7H2qebUbadg8LRRHGWCyfKffFF7a2B2uWXh/iQ\n4I9KhjLo/wAvG4YJ9acl/aFVImTf0PzCrC+VKFXeMA4JHNJyQJDYwVYElnXGCppZZ1Ur8mAO9Wo4\nAgIEpAHIPWmvYOxUsCQ2ccdapSQ7XK0iK6+YXIwOKriZolG05I7VdaylUFACQOeRVRrdkfnkn0FV\ndMViNrhnIZ1wx6gdKkt5nhmQjCgHqelMeNwRkBgvHFK0bxRlnU7M8E0aAXGuly4nTeCeoqJ1IU4H\nysPlyKrh0aP5cjB61KXGNrBsgcelJMGhkzsEjQIVKZDMp+8KqOweQdT2ALcfjVskOVbnHfbzmnIF\njlQyIHTkFehp3Cw20YwXDKoBGeeeKfqEafaT5CsQe/vVGVZUkAgUmPdgc1bDSw3BhbZ5gOCQc80X\nEIGMkcYB2sDhj3rTuoY4be3c3CzJKueDyvPes6SL94TuKqx71Omz7KyhxuBG3ApMZPOqxRBFjXcr\n556kVbi2Swxs6ME6Ng5qissnmFnQtGB8xpxuI1hRoyQg6gUkwaLEhjRsxN6gEjmnooRVIJEn86qT\nTjaHzuyc5FSpNJGYySMHpmquJFmL9/dMCF3BScEU+SN7a42owVWAYDPQ1VeQh2dSDwd2DUUs/wA/\nmctwMetTuMdNJL5+2Ue+exp0Up3sVUR89fWnxuXCEsGBbG09cVNGjKzS+XkDsOcU2xEUzvbRSTAF\np3+VT/cHrVW1gYY7yP0z/OrnmPMxZD83QqR0pUjkYNHAMzOdpbsopXsMWKAXMnkKcW0XMr/3j6UT\nyvfXKRwoSoOyGMDqaddOlvCLKBsIvMrjuauW4XR7D7bMuLqVcQof4F9fqaybvqWl1G6hMml2X9nw\nPumf5p5PU1zTt5jbQcKPvGpbiZ5JCSd0jnmqznA8tT9T61UV1JbEZt7eijoKu2kIRfOcfQVDawea\n+T9xatu3mMFUfKOAPWtdkQwVWmk/zwKmZlVd3/LNOg9TShdi+WCM9XNQl0OZn4hj+6D39652+Zmk\nVZEV1dLY2z3MxHmsOB6V57qF699cs7EkZrQ1/Vnvbkop+UViAEkKOprSMb6CbHRoZH2jp3rt/Cuh\niVvtU4xDHzz3rI8P6M99dIgX5c5Y11+r3aWluumWnQDDkdz6VrJ2VkIr6hcy6zqCWtsp8sHaij+d\ney+CfDEeiacjOg85xkk1zPw68I7FGo3afMeVBFeqIoVen0rld6srLYr4UO6DHelAwM0gGeaU8116\nRVkQFKBQKWhKwBRRRTEFFFFABRRRQAUUUUAFFFFAHEaUJDamSbAJ5GKuGGN5Wm25dlxzWDY6kIkW\nKZgdq8EmtSe9iiKxF/mkGVArCTuxHlPi2yls/EEsksW0Pzknr71zayO0m5YwSO47ivQfG0X2u0SR\ngzSJ93jnFefohChQTyPnHpVQAdJKzxEGP5QeDWdKQuD1I6VqbUa0KAZZGyrDjjvVe5gijdjncGXK\n/WtEJnT+G/FjaZp8dgIASxJ87I79jWxZXkN3PIwcpKD8w9681ty0MylslGPOf6V1mnXsMd6rs2UY\nDtXPUppO6OinP7LO0eNQsX/LRuVIHGafeXUVjZu6IfNAx5Y7e5otdRsZo2ihIWXbkbu5pEjbUEFs\nSombjc/AxWUWlLU1mrx0KWnXMt1Ikq7VIXcoakt9QlhuXunQOjHawVfU9qrXUEouPs1qrqWfbvPG\nFHWtezt4NOAFwslxvdVjiPoT1rpUYyZyxk0dbpIjexezKt13sG6gdsVVNtDeNL9ruEhO0Ig69+K6\nAx29q7PFGqyzAA4HJOP6VzWtl47po9iFCmHUDbzjrn1rOo1TNYXmzC1eFrZ0tyzPGuAGB6j/ACay\n4buExtLJJ+/HESyHj/69OX7RIAJZGkjwVUjkr6GoJI7kyxW8jEiLJwQPn/H1rmSurPqdluTY1re9\nSWBUvNrNjnYmAa0F1TTlwzTZjyCVjWuahlkS4HCtGv30Y9OPWqqTvfKTp7Jtj4kJ6LzQoOKumDlG\nb2Ot1nWIJbZRbykRYwyAcmsNZfP6wxgNkncATiqkVjcBpJ5biBxtwuchc1H9nNy+55l37uETgEd6\nSg3LcqKSNKDR9Hnt2WWwt2kK/wASjDHrmsO78HaJeuudLMfmcAodoHvWybG4SAmefcEHyqhPHNT3\nOoLDCluxYxKmUBHIP1pyc4tcrE4wlpY4S6+Fdms+ItRZMgEIVyR9awbj4aapDKwjkjaMtgOOlesW\ncOy0uHnlaSZiCoTnA75NTWdnZGGaS51HykHIjAya1jWk3ZmcqMUeNN8ONWMDyRvE7L/D0NVW8Da+\nW8uOxZ/9pTxXt6rb26SlZgIA3E55GD2x61Xm1e3UCO0mSQuMlgMYq1Xa0MpUo7nil94F8Sachkn0\n24SNFDFgc8dqzWt9asiSyXce3B5Br6Hv9RmS3tY7woy9VbGVcDnFZiumovIwjBVs5QDIBqlXVveR\nn7PXQ8Rj8Ra1ani5k44Ida0rfxzq0CAS4kwe9ek/2XZ6iZVexhCDrhcHA4NYt54b8MyIIw7xyDj7\nmDTUodRcriYcPxByCZLFie+Gq/F4w0W6t+Xmt5ByVkXK/nUk/wAOrLyRPbXkjoSOmO9Ubn4Zzklr\nW6jEWeDL1zTSg9mLVamtd+IfDsk21rmNR5a4aLJGcdTT4LzSZUUR6pAQx+VSf5g1yFx4A1iB9p8p\nj2CuOap33g3W7AKZLNvYrzim4dmB37W9vE5aC4hmDHDIrc1WkjbDEq/Bxx0FebfZ7+0YNiaIg9Rm\npodZ1aBjtu5Ru+9uOc01CSFc7+IGM7N21W/Q1MzSRNicq5Y9Qc1wC+JdRHEjpJj1HWraeLplGHtI\nmB78ik4yXQLnVl5UOfIEka/MRt4PpmlggjmkM0cTtGpy2W5Wse28a2wgdGglTzB8wRgc1dl8S6WV\nVLedk+UbiV25z2NK/kGhpW8oQqJVMkQ5AOen1p9vbTXbyJbKq4JfBPIqtJ4htfsqW/2mLyhyuCCc\nVPaajaLIrxXCMxIwSaLsLIsm4a3VF2Hf0f3plxGJiFTcG/unirN0YLqUmOeJ2b5i3T8KhkysqpvB\n29CBSTQNDDEywYZdrg4xTra6WKXMmCOgU1aFtJKm+P5n7hun1rPkfBy8ajaew6iqbuTY2fsxuEeS\nNfLyNyr2I9qppHG0qscryAx9KWyv54ZUaFy0ceMqVyAPSuj1L7PfaOb3T7MRqCRKEGccdTUpllOe\n8fTXNnuiljaPCEICRnnOayvNu4JBIpRFbs3ce4pqSPG6tE24hTknkg4qsq8lvMIY9jzmhAzWn2WF\n3E7SRMzjJEbbhT70i8ZZYWWBAMEocZIrKcfORlBgA7lFTpPAkceA5dT84Y8MKLEkunSWou1a63Mi\nkttx95u2faq+p6g95cvPISP7o9BTQwuZNqIQztgL2z2qKaBUOWJZl7DkZpcuo+YqklBk/fb9BSRR\nNI4UdT1NNIZn+YHce1aMUYtosn77Vol1J8xWxEgiT8TUsSeWobHzH7o/rTIY9xLv90dff2p8kjLg\ngZkfhR6e9ZTld2HFdWNfLt5KnjrI39K5rxJrARfssB+UccVpazqKaZZmJW/esPmNcDLK08pkc9aI\nopsYSeSeSa0NNsXuZ1VVyzGqttCZpAccdq9F8P6ZFpVidRugA2PkBroXuIktKsXhzSQiYN1KPyq7\n4I8My65qQup1JhVs5Pc1k6fZXXijWwoBKlvmx0A9K980HR4dI0+O3iUAgcmuapJt8q3KStqy/a2y\nW0KxRgBFGKn6mj2FL0HvW8IKnEhu4H0oFFKKa11AWiiiqEFFFFABRRRQAUUUUAFFFFABRRRQB4ZD\nP51wrtwAPmB710ltqdnKFMpG9RtLVxF9cGADzEKORnFS6VdIVZHOdw45rOwG9qrxSWwSFgYgSqsT\nn61wGpQiJ2KSjI6sFrqdVncWNnGdqKAwwtcvdBSSuSc9OaEgTM4MJIkdnA55Ud6jLFc7Xye69fyq\npcExTFT0piz4BUD6H0qxMvNJ5sahz937pHatG1hRtOVlf5lbqP4axhLhf97irenztZ3PzcxtwQeh\nFTPYcWrnYaSskaGQ8kD5WPIzXSxjy1SaV8FwMe1c1ZTy2WQoDQS/dJ5C1qQ3p8yJG3FMgdK47ptp\nnarW0G3F3PFqUv7qSRY13ucYUL9fetXwFe6jqN+Zr2FI3ZD9mjxnavY1ZitvtVrLFcfMlxnCg4zj\ntWNLeXWntPLauqSxOI1Kjt/dFb0pK1kYVI2d+h02s6tc3uy3inEH2U/v2j5Lt6Cql3cPemSVyCAO\nCT1rOtplltWzGCwO5z/tVbDxy2ojCYRRnI6k1yYluRrQSRQu7mexmgWNT5TYKkL371VvbtpXEqHO\nwE7ehzWvMIZdNg85tm1uG9KwdWtngwUbPmN1I4we9YwqHXyXRm21zNNHNsTyozgOHzk5OTWpZTQI\npiSNFTuBxmqd1FJbICCCpHGeQaasaEAiTYWGQGGBitHLmK5OQ0XiYsBCGCMclDyM+tVpn+zSowCA\ng9fSpbe5kSPagz1ApqRLcOI5eON2fSqi0iZRvqPiu7i5aNTwXbC/NwSOoqc3EayCKWHqMvuxgGov\n7IuJZRJDOmxeQpOMEd6z2SSdnhdAxDcYPIqmmQkTi+PnmNJnMf3SgbHFC36xyK8tukoi+cE/KQBz\n+NZxjW2uQ7IVb+IYoubgSyIilGTBwrHFRy3Y5TSWol/4ljvhJCVEUXmEkIMBfwrJs0kuLstZ3JZ4\nl+eN16jNVtS04XRMqbYm6urHAI+oosNPcTRwPerE0oKqySD8M11KmuW5xOV5FiXVbl3e3eUr5ZLK\njOcA+1JYa3qFncMbaYqxTk9cH1rMl0e8hvZIflkILJ5pOVb6UsGnXUbAmPDYw20k1SpprUhzaZpy\n+JNTVyDKxZgQzDoR+FZqpezOJCWIblSATTrRbi3eaKJfM35GzGT+Fev6VdWFnpdjbraxBlhDyK6j\nK/8A68VVlFWsF3LU8ptp7yJXiXzQynkDIq5/aN26TPF5yyhNxVjkcYzXfXGqWF0zPdW0SdeYlALZ\n6VHdaErs8JaFFeHMcq9cEcg01GMtbE3scCNbF1GgePZMh35D5Bq8/iuQtvaIiRgASDxx3rEv9Imh\nuJYCxMinjaPvCqO2UKiOpA3A7jx+FHs0CZ1z+KoXeXzrKORmIxwOOabNJod3KRd6fHHvA2uqdfeu\nPkGC4Dcg5qeG/uo4gqsGVeRkU1BrZjczqX8FeHrgxuu4bhkjBFV7z4b6dC2f9IjVuU7gmquna89p\nLG9yjmDq46/lXXTeKbOW2G6+CxcFFZcnGPWocpxY+ZNHH2vw40172M3V/Pb2xcKTsDcd/wAK2dY+\nFFidQjttJ1+By45jZOemevTOKurrFvfWrKT8jHAH9aqi8FpOBA5KLzweauNWWxLOR17wNd6VKsFo\n8Fzlfm8vnB9K52fR9TtlJezmyD1Cn+le0WUlpM8UrTukSn51IqODVYrXUpfLYeRI2QZVyB70pVV2\nBR7nh4uL+25Ek6HPqcVcj8SapENvnFh/tjNe1X8tld7fMgs2WIkFfLHzE9+KwNR8O6NqkQdooo3U\nBU8sbQcmqUoN7A00cVZ+ObiIKlzH5ijglTircXi2ykVvPgcbs9D2qLUfB8EGfJuG9OR39Kgg8Hy3\nB2RzqH6AN3pOKDmNa38U6eQUR3iDjB3jg1qWuuW7RvDb3QRG4ZUlwGHvXOXfgjWGg2KYpUgHGysK\n50LU7VT5trIB7dKTpphdnqFtNDDcJloCrjAXd2PQ+9PktkDt5TK204z1FeRLBeKwGyXjjoeBUyX1\n9bnCXEq44xuNHI72RV+56n0QoBx0bA602UNNKP3RCdSccVw+k+M76ycJcxrdwZyyvw34Grr+PrmR\nsCwgVAcAKTxU2knqhXR3CWNtHb7zdI7t2HY1SyVKhEwwPRu9ZNp420e5jX7ZZTQTg8tGdytWw/jD\nw5vGz94Uxwwxmldj0GSvHDLmWHkDhlP9KArSy/X9BUCa3o9z5pS4j8wnAUnFWLfUreSN4U8ly38Q\nbkUOTsS1dkrMqpn/AJZp09zVa4uVs7d7qYjzGHyj0qZ9uTI5AhjGRz1NcR4g1dry4MaH5RwKiKKb\nM7Ur5765ZmJK5qqiGR9o6d6aB0A6mug0DR3vrpI1XjPJrohGyuyTX8LaEJ5BcTDEMfJJ71p6ldya\nrfJZ2ozGDtQDv71Y1a7jsLVdMtMcDDkfyrsvh14RJI1K7Tk8qCKzqVLK40rnTeB/C0ejaeksiDz3\nGSTXZ/dHvSIoRRgYA6CnAZNFGnb3pBJ3FAxzR1oPNAFa7skUClooqhBRRRQAUUUUAFFFFABRRRQA\nUUUUAFFFFAHzTdu87h3PQY+lMhk8psjg1NDYSXMDlTyvO3PNP020jcOJQ5PIGB0NQhEs8zSWau/Y\n8ZrIm2l8H8DWxd2dwluAFJT37VmhFlYLJ8oz1HUUPQEZN5Ak8ZAGCO9ZBQrOEfjtmujeJiSoI44N\nY17ZlZPMAOOhPpTGI9nNHuVkJUAMGHTHrT4XLY3c7aZHdTGEwGQhSPlJ7UsAUMu9STnkilcLHQaT\nftKn2aSUfL91T3rsdHvIQy29wilWPyk9PpXmtwn2K8idTkBgwOK6mCRdR08vExWTOSB2NcmIhfVH\nTRnY9GGo6e8myKAiNMYycYIqhfafEkhuFdvLZvNKDnB9awdKuvLG2UgscDc3Y1uTy+WqRylnhkGC\n6jjFZwi6etzaXvrQpQjzXd0wI/vDjnNbEVuBbMgckgjOB19qotm3nWFEIPRAO49a11GyFVYDf1NY\nVZ3ZMI2KUtsl1ItkQVUqdxz0qJbCSe6SWOIvZwqyMp7/AONW1kW2vV86IlWG0N9a2NNiiitmgjXc\ngZixNbYaNOzvuKrKS2ON1XSV2BY2cAdVkXABHvWE6bR5cj429AT/ACr0S78pbi3hMYlW4lwVbPyi\nuY8V6B/Ztz5qqWhLYVs9D6GqnScH5G9KsqmnU54zSAq6ttUdgcVKt3+6DONzDjCnoKgu9vkKptxG\nRkO+7k/hWXdXq2MgtpkKuBlWHQipUW9i5z5TpLe9VJEb7S6Jg8gdfapXnWVkmQbjjcccGuY03U/7\nQukt2TaW+UEfzp+q6odJdoxA4mVsYLcYpuM76E+0ha7N+4ltrtnk8t1bbwT1rJuQiIcwAEHhqz7b\nxQsxKMjI5/uN270Xviewn094ooHeYcAPVJSUtUYVJRcbpmZdvJbuyhm2EZGDVT7NBNGZDPM75zgI\nMA1Xe+cuqnfwuNp5H50+NmEihGKk967Ix01ORvUsgtgYMoI6ZOfxrSgnuIXB2b1xkmq9laPcRSsZ\nCsinIJPUU6AzpIB5hKgEH5qqxJrwrawqs/luGZvnIOMqetdt9ltNQCXenMzhbYIyqPQV5+18dnky\nOGUD5WxWzZeJJrO1jiXbsTlkj+UsfXPcVE77opPoT25glvSmobkAIU7V6rW7cSQ2eh/u5/lJYRlu\nTil1J7a4skuJ/Ke4kUbQuFIHXmuKvbvaWJlmwTwhpp9hNNFU38sUrXK7WCEpsbq3vWdLqBuXx5Sg\nE9Kllgkmtyd5I3FguelZqxYK5k59PemkIbKuSSABzjIpi7gCM/hViS1lRCxO4EZqvtbrjk9KvYC3\nDG7psL5GOhq2kaQyJHdQs0HcAcj3FU7aOSVgv3WzgVZ33SsYpl3qOBzyPxpAi1PGLEqYiXtX5X/6\n/vSP5yhZow2CMBh0P1qsJpNpiOdhP3SK6DQ8GEfaYVltkfOw+tS7FbklhbPc6cJzcqRkq6L/AA0l\nzoF9GqTIrNG3BzyK3X0m0igDWRH74gZX7o+tXrbUJZv9Bgj3+UNrjPT3HrUxfcGjFh8OCexaa2vV\niuVXc0DtgEVlveXVqGiuoYXMBKoQOGOeme+K67WLKJ7W3miBjmWIiQA4BxXH3LNISJN4KN90LkY9\n6p2khajUurS5eQ3MQ2twoQ/cIqG+8qN/9CkMjMuVk24wPpToNMFxBviU4H3j70saq06xsQhB2t6j\nFLYNGWdMmv4bVTAUKyAs4c4xj0ra0/S7/U9HMzWTMc8MnINYd3ElzN5cJZkC4GD0rvPCHi2w0mwF\npeybQq4BY1MlbUZx6BLG98q7t/IlHXenBqaXSdK1KNlW3h3tk+Zt5qT4gaxa6veQixEcgUZZgex6\nVzenvGxJjllgkThgx+U1Mou/MmVzosaj4J06SFXQ7Gzt3Ie/0rJl+H4iXcbwAejDrmtx3aSQKDJn\npuByM+tayGW2sPMMgneLJdZB/Ce9VzTTtcNGeezeDrlHWO3cO+fujjP41mzeHdShlaN4CGU8j0rt\nrq+eWWMowXdzmOho5Elylxv3EZDnBxTUn2JaOEl0LUYFDvaybcfeAyKqvDcQ87ZEOevIyK9NkvvK\nQEtux0JHBp8DW+pPHHNbIdwxwB0p37j6Hmkd/dxx7PtD7emCeKgJOSScmvS9T8LQWk277NujABzi\nok8EW81m93L+4T+AHuabUbpiOI06ye4mVQuWY4r0aKOLw5pIAx9rlHHtVPQ7Cy0tpriZwWj+6pp1\nna3XifW1jUEqW5x0UelE5r5Alc1PBfhubX9UFzOpMKtkk9zXu9nax2sCRRqFRRjis7w9osOj6dHB\nGoDY5NbPsKwpxdSXM9ipO2iDqaU+lHQUV0t9EQAp1IKWmlYQUUUUAFFFFABRRRQAUUUUAFFFFABR\nRRQAUUUUAfNcUs9k4X7qsM9a2dFRWIm8zaNxBTH3qo6rbsYElj2kHgY9KWy1P7LDAsQWRySSMdKz\nJOxFlBeWfAYeYMgHtXB3tsbW/kiZSuwnrXa6bfk2wa4ZUZScc9q5HXLyO91V5Ih8o+UH1pNlGSxB\nbkYOevrTZLjfE0W0EHnildN4yM9aiKqMNnmqAwpifuOzArwM9MVNFxHvVhj7rAH9ak1BC43KuAKo\nI5jJUY2t1p2A3UCSQxxXG19vQirGkah9i1AxED7PJx9DWNb72JRTz1wakVSyE5+ZTkY9azlFSVmU\nnZnbxyLDc+XJjDHcGPeumsZfPs5oASA2CFzwPWuK0aY6lZOZMCaIcGuh0OTdEx8wJIDgZ71zeTOy\nEnFHR28SIqJneYhmJj/KqsepQuzOX3kk5J9R2pLhpY081Gw8fzDb6U+3a1uFEqJtLDJUdAa56ysu\nY0hHndi1KzTR27NhWLqzKOwrqdKtYp7Ga3kyGIIdh1GawdOtlub6IkHaHySD97A4BrqYVeWRvLPy\nl2V/b0rXBxS1Zni9NEWrbT4obZYmUPjox61y/jLT4cLPM7NE7AFey46Gum1G6+x6bLJvAdV+Xjkm\nuVg1KTW5mNwoW0eMKu4Dg9cn0rtqrmizlpvllc4a+srWO4d3ZlWRGxs5Ibtn61jzWUOuWoscHzo2\nJWQjLAY/UV29tZ295qM8d3tMSSHYqrznHrWBc6b5eoTGECGZXwFb6ciuVO0U0d2klZnm96k+i6ii\nLOrPGd0bqexqC/vpr9mmuJMvx9a6PxLoX+jz3sLhsOoVSuMjHOD9a5OF2kiwq/vByD2z6V0wSkrn\nBO8XYYxjlSMRxlZEyGbPU1Iu8A71BY8biMVdnieO6kSdDFcqOVK4xVsqk8C3BGGBAOBxmtUSxLbR\npprP7W1sTaiTy/OVuASOAa2J9MsbXTYbmPLl1I+YZwe9UIJ7u2065shI32S7G4rkYypzmvRPCVjY\n6v4QvLF3QzwxEjJ5BPORWc5NalRSseawzyW7eYoyvQj1FS3VsZFaeBxsYZKjtSjTniywfKk4x70+\n12xOwc8OeVNWZvcoRwSLCrgqy5walLGGUKy/LjpntU8kAS48pASG5AFLNaKFyjKzAYIVskUwH28s\n8ojcufLV8LuPAPvVwwvIu6RUYEnkdKxjCyxkbG3Fh0PAq3ZXUYhNvceYvYEHNGhV9C1DDs3R7Rjt\nVW703fl4l+b0ou5FhePypDIAcsfartrc+agLLkHvQSYaxTYOAwMfVTSrAZG+Y9e1dCYkYna+VJ5A\nqJrNZMAHGDwaYGaLVowWH3B0NOjiOCxb6g1rFWWAoyAqThvUe9V2QwggjIpAVvJWVDlsHsaI0mgX\nCu4U919aXftlIyAtXbYF0OGBIoKuX9E1ea2cKz7t3GCvBPvW0NUg+2RhIfJkb5ZGU8KfUVziLsOd\nuR7djTJC5BKN8x65NQ4hc7HV4pJmZY0Lbzyc9vX+tZWqoEhtkKBsLndH3PvWVBrNxGhSUmSQDCkn\nt7Van1SGbTdksZ81WHI6YoS6DuVLm7ezaKMgJ8uCR3+tUHvBO5Yxjd/eFOlSJQWZnfP3c8ii1hRo\n5M4XHY9apIgZBe3MBfyPLVyOdwpGk89D58IL9Q46fiKJ7Mk7gdwHpUphiNsrK+HHaiw07FK4+0SX\nqySRxxJtAHljirsllDeRKsT7H744B+tPiXd0VcDsalK+YApKr6EU9g8zMha6srsqobaD25FbWnaz\naW90wvIXbzEMTDPUHvToU2YE3PoQKQ29tLKrMgIXnPrUtJ7DGSaKkOoSWU5KZA8qT1B6H6VuWHhG\nCzie61G7BbaVRV5zT5EGo2NrdW29JbRhG+7nK9quagZ5GYG337cgljjHHXFQpMq2hjLoSSxyvbv5\nqoMtvGPwrKWy8m42WiG3uAchZDw30Paux0R7oB4lhBHc4yDVzxHpsN3YQ3cnlwSJlcgYzxVqz0IO\nG1PXJ7i4Atj8qDa4P3WOOajg1u58hfMAlQHDI/Qiqd40VsyB1ztbDBfvEVo6h4g03+z0jsbLy0HG\nW5J96myTsUYt9ItzP5lopHOSh7fSvUfhLa20thPM0QW5jfBB9K8kk1EEjagVc9R616T8ItQeTVru\nBsDcgbArOqr2uOOx7EBtHvTgMc0nVqU1urRVkSHWlApBTqaQBRRRTEFFFFABRRRQAUUUUAFFFFAB\nRRRQAUUUUAFIaDR1oA+eJbyJ9MMKhi68YAxVCWMWEo2vucqD8vODUcY3B5N5cjueAajd9yA/xE5z\nnpWYid7xnTJ3lsbQAar4JYsSR60xdwOc5IpytyQeWNCQx7rsxg5OM4quc4yQD7VOBuO0fj7VG6KG\n5J+tMCtJGsnB6elU59HuEljRIyGm+6GOM1fcBTn0NP1S+n1KGJGAHlAFWXg5FDbWwGFMzRTLgNG6\nDBDHuKdHcOSzjG09cdqdIrzMxlz5ufmDDk+9RRoUywQlOh/GgZuaUHjvI3gmKr/GB3rp58wSq6A+\nWeeO1clbSNZX0MgUmIgdq6u6UPGs0T5DD5RXHXTUkzqpO8bHQxs5s1ZD8kq4Lnmq1huilaJnOzBZ\nc/rVPS9Qe3geJ8EKcBGGdtXQYHJ2FtxwEYdhUrVNM3WlmjtdBX7PbxlZAy7dxf69q6DTUG03AkZh\nIxIzXGeGNQCXX2W5kQK+GXjjiu5tkER2Im1D93HT1zVUIWkYYiV2cz41vGtlGwZZk2hf7xY4Aqyu\nnm28PbHUC5faTvXIB7CtDVNGj1F8SgEGRGUnkqRVPxFdrGiJk/IRkDjP+cV01WlB3Maa5pJI89m8\nQatBdJbH7iMSwRApbnuasX7/AGjUhISWzGXYqP48VDd3Ym1KWZIS7cdEycE1Z8ZWkVppz3VmzRYI\nOc45Ayea41bRI7ZaalC406W/tnmaYBY4iX+XqcenY15PaxjzTnIwTz0rsoPE80ei6gzSiP7Ug2ll\nyc98fnXL3ECi2EhGD7nmuymraHHVlzO7LBYyzh2kaR3T52fsewqeGUQ27xnB7kHvVOAuFGfTIHc1\naR42ELNHkK+XGeorQxZIkojs+vJyAoGcZ7CpIru4sZgYZnjkcbflbqD2NVVdXu5CRhFJKL2BqTaI\n5QQN2Tuz7+lJpAW4yY2ZeS3VdxzTxJHtLyquWHQColy1xySCgyce9OOzKYyWGQQe9PToAy3zJMJA\nxIzgGnXEEZlEmMYbt3pbSF4Y2ywKZyq+hqXO5SGQ4NMCCO33biCfzqT7NHJGcnDdj70/aY92AcGl\nB56cUARtaoIyQD7inxwiAZjyUYZK+lSDJJHTHalwQ2M0CI4QSG27kYnNSD53CMxVzzj1p45bnr7U\nPAso3EkMO4oARmkQbCMoep7ilJfyxwGGPzoEUiREGTcexpFWSMKRzz8woAiMMNxhlbkdR3qaJVjc\nDB+tOeGIkN91s/eHBNOJCE55Xse4oAcjorEI3XtUVyrAMycgd6XywkqzoRjvSsCznaeGPIoAqfZ2\naFJFJP06ipGjdYhuJxjpVhAVLRltpAyAe4oklcwqhAJzwaYFe3ZJYmVvvL2qOVVcjOQR0xUgXa+7\n7rdPrStE7DcKAI42aM7d3y9qdlFbcyZHvSBXAwV+YVKHVlCuvNIBhhWYbo22t6VHbnaWSV8EHjNS\nCNQwCkntimywMhPmA7h2I5pjLULShgVkyOwNW4JgHCSoDk1nQhk5LY44z2p9vIdzeZ8zdiKliOzs\nJINPAmU74J22Pzwpqa7YxtLIrEtgrg8gg1xXmSujDc3lgcrmt2+up9IFpJDICJIFZ0kGQTWbi76F\npl7RZJ1uXRIpCiqzBscH2puo6qlzYRwT27h0bhSeM1AnjJ2ttklv5XXLRDOBWZPqMV2TNAkkj7CA\nzrwD60K42UtUso1neQ3ESsFG6JeSM+9c6RGNyhsqeladxHcJLvSPdnrnv9ailQGJT9mCyDrkVol1\nIMplTaY1A+Xkt3Ndj8ML5YvF8CO+N6Fc+tcb5LIxLZ68nFa/g+6TT/E9jK//AD0wOKyrL3blw1dj\n6eB6UtRRSCSNWXkEZqYcitINOKJejFoooqxBRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRSGgBKd\n0pAKKAPmNCrRqR8qk4IzUbQnl+OuMVAhLPx90DrUqsSgO7v+dQIkZCDuJGfanpAJGAVvmPcjvSpG\nkiszswbGBgdTTUhliUNuPJ60AW7XTZdxLAD154pb+COAbDFtYYznvW7o8V60JaMxlTjLMOnvUupa\nNdagzXJl89zwT6gVLZbV0cUNgBLkBcgA4psiQK4VbgvhckbelSzxtHcMpXG3iod2c5OTmne+xDVj\nMvNqTCQtKSDxmqwmOSQgPOSD3rUuVEoO4fNjOazJI9r7Tx2qhl9r6QrGH2ugHRRgitrSdWRF+y3C\nHyiOGbnBrlFysnGSB3qUyOpHzEc4IrKdNSVi4yszuiWt3DRMQH4z1yK2raMyWyJGdm4ZB9x61xOn\nak0kCxsc7eOvNdFZ3TyhYh8yt271y8rhodlOSloX4zJbXAkc7TvwR1Iz3Fek6LdnVrDBuCrR44Tj\njPrXDSRozMQjPKECMM4A+lSeF9UfSLlkx5gbhFdsE+orWNSN+YUqbceU9TBkEjcZGQAP61zPiJUS\n3mZn5kf5c+la2naj9oWJSwMsiGRlH8PtWL4tvo7PRAJolWZslV64q68la5z0bqVjmtMQnULdYRgy\nnYSRlQeoFUPGTGfbbXLiPO5GGeC3bir0Wuw6VYQlYx8pVzuHAPc/UU/xRNBPPBPDAsgeISqGXJ5H\nH+faubR2Z1SbvY8ZvoHAjiZ2FvCCI27+wNRNKWjJI+9056V2mpaB/bLqILj94R86AYCnFcQgO7yC\nwDJwSeld0JKS0OKpHlZahb5Q5JDDmnPIyEDHUbj+NNs3UecLgB8LhMetEiSRXJhlGGHBFUQOBLPV\nxB+6QYwV6mo4BHDNub5jjge9KS4BLcd8e1AEsLfvd5Iw3U1IZFjjkIIz1XPeqiNlcH7oqRJVEW9u\nSp4B6UAXNx2q/GNu4jNOychecP0PpVQvjT2lkOXY4AHYVL5hNsmcj0oAsM2AoBzjrSr84wTjmoEw\n6K3p1JqUMzxYAwM9aYEgVi2M5p3A5/A0RMMjcMAd6Cy5KgdaBEiYU4/iNP6BmLD6VHkABiTnFOjU\nEcHNABnkHNPA3AtmgxlHAPJ9KRQdxHfsKAFAyM96ATx709lYSHKlTj8KbyByBmgYEBBhfxp8WxJA\nSMrnJHtSRxoWYnccjj60u3b16UALcGKeUsqkBfuN6CoLg7ocMMEd6lA8tSTkgdqQgNkdj2oARTG8\nS7/TrRt2AYJK0oT5SBjPemqxzjtQIUpkburfypgjBPNTgYO6gYLYxwaAK6x+XIJEO7BBxSzyvdXD\nyMQGJ3EA9K2f7ClZVaJkI25wTjNZ93ZTW4BdSu7kH1oTuN6FaOWFI3WVN7lcJzwD7/hmoFHBOcYG\nMCnMgVD1NMCkxHC9+tAbksUq70R8+WzAPtPNdN4wt2jSylUKIfL2Bu/SuRhjaW4ijXOWcLj8a7Tx\nZqNiIYdMErPfQ7QVx91cc/596z+0UtjjftMsaeRtV1PORUTNIp3RSFD0ABpGdVYhg2AeDVgfZpoR\nGEEbZJ3+tWSVWu73ecODg5Ix6VKupTblLxBsfezSrObUSRooYnjcRzVWaTepI4OeaALkdxazkGVB\nGc8j2qJ/skGqW7w8xrIpzj3rNLLHIWlBI7YqFZgz7TymePUVMl7rSKi7NH1TpjrJYQMp+VkB/Sr9\nYPhCRZfCunyBt26Ec1uCinfkVwnuOooorQkKKKKACiiigAooooAKKKKACiiigAoopDQAGgCjrS0A\nFJRRQB8rjCdOp4xUihtxB4xzT7KPzLiPfF5qg4YD0qS5nRrnfHHsVRgKepqGIlt3MaMNuW9fStK3\ni82ILIRsOPwqjZysCzeWCT1zV+JC7g9F9jQM6GzjGwEEGFRyoOMmn2l99llYBiAP4W/pSqsb2a7Z\nQgUYIA6n1rK1GNLe4jSKYswGXO7PP/6qmQ0S+J9Ntn0xb6D5ZN4Doq9j3rlIYIDFMXb51XKD1rqb\n2+I0KVRIrFiI+ecVycikMOu055NKO4MrOjEbsH8qz5EV5mYggqMg/wCNdPpMVq91su2IVkIB96xr\nmNYJplVjgNjpnNaX1EVbG3WezvH34ZV4qhKHU7JDk11Z8sWEyxW8cZGnqJHXB3tuzk+9ctKrNyMn\n8KSd2MW3lMMgI/MV2uiX8Fn5MrzRyeaCNo6ofeuGRgQBt49K0LVlwvXcp/MVnVjdWNKc+VnqP/CQ\nAujpEpaNfnz/ABVflEF1YmeVQsrEFF7gmuFtyoWMspAxxk9a2ba5Uwqyg9hy3Q1xxXKmjsvd3O20\nUFbVbmN285VJQfjyCK5rX7+W/wBRQXH7xlDb/m+UDsBVnSbiRbgvDIPlJYLnrnrWHdWu2+u5ZJNu\nVZgSpxn2pOV1ZlwilK5PaSPNJbI0SfZJU2+YOSrA9KTxVdw2kFp9knYbodmWGRlT2/Os+21WS1Nj\nbLuaCIMz57nPH86iNslxp0Us6vIrPIpYKSY1yc4H65qae93ogrxb0RVt9UufsU1taNsnZfvY+b5j\ngkVxWoadNp960cxGcbs7sk5rV0G7AuHaU4jjBGO5rX1tbOTTniOwTOeXxyo69a7F7stDklHmjqcj\nBLhRnOxeSR1qZSxk3btzddx71VESwnLONoJwSetWbceduMILkgkg9hXTuctrGlEoV1dmy7DI9qkO\nJXKL9STVWEk7S3BAxirLHbEQuAW4JPaiwEeQwGwjLNtyaV2jV9gG7Bxk9KU2zzR70jPlxjkjjJq1\nZaYZ8vM6xxgZ56mkMr791u+4KM9BSCRnUR9l4pZYtsrKD8qg81AuRGBuyxOc0IDRikVIioGdvrU1\nuyu5V+nWs5dwz3wf1q3GfKR2YYO3qaYEiPv3AcAGpVKsoPGelUwXWMH+92qVVXaMnA6mgCxvHHIO\nOKlhJ3DaeM8mqkbJIyhUIHUipRMFLBSMCkBZ+9IcNg9aQcMcEH3NRK2Vz3NP+V39B3pgW1mZ4wCc\n4HX0qtI+F3deaRXDFkU47Cmk4XaR0oAkgaQShsYA7GpGcs7MPwxUYk3qSOgHWmO7+UAgOScYFAF9\nmU2CZwrNIc45OBVbCp3pbgpGVhU58te3rUJySDggetAiQZWQYOeaVjmTB4pZGRoI/LXDrwxB601i\nGVD370ASbj6cCpbQZuk6Bc5IqB2zgjoalt9quzZI2r1ND2A7CzFvckpzs2bgw/hNYmpPEJWgCl0U\n4jJP61Xt79o4mXGeCo/HvVZ2ACnJyw71EY2ZbehU25yCep6UwRYGQefSrBT+8aYEVm4z9avYjqJa\ngw3Ec2zJVwfrzV/xDsfW7iZF+ZiCR9R0ro9D0CGG0W8ugrs2CAxwqjtXO6r9lmurmSGYPmU4K9DU\np3dymtDEkLM5JTPHQUxSkjFVBUj1q2yeWQd3BOKjePJyvSqJKssYLHceQaruuM5FXpVPB2gjHWoC\nrHrjB70DK00QePdjisxkKMec4radTjB4BrLkixIQASxNS9gT1PoP4Z3YuvBdmvOYgUOfrXY15z8J\n5THoLREH/WE49K9HFTSelipbi0UU3IzjvWhI6igUUwCiiigAooooAKKKKACiiigAptKaAKAF6UlF\nFABRRS0gPmC0untgzwtyxK5xTobZn3tKQM+tVLcNJGuOVFasSiK3VW++fmNS2BZjQJGBgYPenySe\nRpxnTJBbHA61nSS+ZuQkjHI5p6CSS0SIPtTJIpASnXJPMUMpMQ52g4xVGfUme5Xb6cZOc0TWkob5\nogyH9au2OhS+aTK6xoMEE8mjTqCJXjuG0tAIWSJCWZ8YycVmFWZcEk/QdK7qe+0+Pwdc2AcNJ2Hf\n8a4MSEnYG5AwalNXG1qPb92QUzkd6qTkn5m5zwc9asfMD1H1NQ3CliuMHuatIRZhtyug3s5mK7ts\nYWsNB+9PGNvUGtpZYP7MMTv87PuK9uKybqNFO6NvlPB9RQtwKUybZGZc7Scg1JDKwjDKct3FJIWd\nFBBBH601GA+UDjrkU3sM3dO1SRYikh3xdge1bsFyI0TK7gffiuKVSJAuTtZhxmt+wuo0ZonCnHA5\nrmnHqdNOXQ6u2kkjLEAbWHBB6VPqqvNZCaNjIjqEcE9MdDWXFJ5sG5RtZccg8YrV0iMyJNaSrtDL\nuBI9e/51i0t0dVNnPCGSW9wx/djsDx0qSC5njsmj8seSGLj1HY/mK1tNs9v2l5oz5cTkMxPf2qhc\nDy9xVmfJ6A9BUyWly7pt3OWvrNraLeIzFDIWdXI5bPaqxtZbi0ONxkkYMobjC+v5Cut1eSK/kiS3\nKraQYB8zqpx0P1rLUw+VHLPGyyHKMV547VvTneOpxShaRhNppeDzm58pAigDj8fxqstrfxac9yI2\nW2V8GQD7zegNb6g3JjtEV2DneVHFULgyRh7JJHltom4iH981vCVzKpCxGty0xhYRxooGCB1z61Zk\nJVAnDDqOO9RWunXctu91HauIoiN7EHFT3ylCispLPyB0qzKw9XcghmO3vT4ZXVGO7JwQq1FKn2ZT\n5zAyYGEz9361XFxIUzGmW7H3oAUgDzGlJzjp700YIj4G0dfc06cB2ViRlk+b0BpqEkAEA7eBjtTA\nkCvLLtACKORzTpZUVJIlbc+QuTzmoVAx948nBPtSxKqvK2zHofSgC05WOJV37nA5PpSs4ELGQ4yM\nVDu2xfNgHjC96kEQkkjac4T0pATpIv2SOOILgDLHuakfaNpVBlqayW8Vi0q45faMdTUJlLRIqxlN\noOXI60AW4wBneOg6jtTxhwWHpxiqzTNIiooVR/Hn0FSW0rfaYwu1AzDBfgUAP4VAeN3cU4qxAYjJ\nPYVLcw/YLqb7S6yOWz8nQ/SmWuyRnkmcoAOnSgBEBYFBgGn26MWZnHyp296e15ZxxfLbkyZ457VX\nicMrsx2gsMAc0wHu+5/lHXrSNK0kexcADqKWNokuY13ls5yuKaQokZj8oJ6mgQ6JMIwzyaeoBXbx\nhf1qGaeLAEeTxyakjHVwCuBg57UDJYnR4+v3TzUsTjYWxnJ5AqtGyxBlAB3cmrIfy0UIQCw5HpQA\n4SqZcD5eOBUMm/zUT245qN0IflgT2YGpQUEgJbnOB6mgCVXG4oeT60sEQlulifhWYLn0qsy5n80H\n5DVjzllwsfykHOfelJaAjqfF+nTy6db26TbLJDiQKcM3pXMLAsCLGqqFA7V11vdvq/hWSW42ieE4\nkz0JHf8AGuUkXcpO75sfd9KmD0KnqQTgtFgDIzUO0KhBIx7dqlaQKuwc4qBU+Rh3JqyBikqCGAIq\nN0JAGMZ5xU8OBJh1yMfnUsto6ESMMA9BmgCg0DORhcY9a2tC8PR3ayXM7bUVT9T9Ky5gxYDOfUil\nh1C4srxdsxCBSpGeOaT20Gt9T0X4czxQ3N1ZAgfNuUZ5r0xTgcCvE/hzfE+LCk21d6HGO9e2jpUU\n92VLYWmuCcEdqbuw+DUlWtRDQ3rTgc0YFN2kGmhD6KKKYBRRRQAUUUUAFIaDSdTQAoFLRSUAFFFF\nIBaKKKAPlqCaPT7mRfKBXkRknpT3vHnTGNqjke9VZws0RBPzKMgmo4H3IA3zHPSpsGjLqBjKruDt\nRhnJ5rRtXWV2IGIw3T0FZ0kZ85eq7uMfyqa3m+zh4z7g/WkBpy3a+YFBG0HsKle7eRQCQCQMBetY\n8e6eYcgAn9KsvG4FxIhBSDAyO+aAIZm3yHrgkkjPU1FhAS/8RNNQhm68mnvFtxg0JWACONwGVBqM\nKUkO7Ab3p4LquAe/Aq15G61edsEqQM0wMsw7pmHGOv0qCW0+9tcNnkVcbBPJ5zUZXBOMcUrgZrnE\nu1hjHY96FgjkikdSVkT5toHBFWbm2DqrkHdnmqMn7mQbScMPm9xTvcCRCGBJXIA6jtV60tvlzySv\nIbNZocxSkAfL1B9asxXZiXhvl61nON0aQdmdhpcjeUB5TMTwcDqK6wW9yJ0uILVVgSNQSGyXHcYr\nmPD92GsHaIhkbnP901pwaxPCjxLJtZ/kJH8B9R9a4ZqUnaJ6UJRUbl6+cTwI9uPLikOT6596qWOn\noyu5JdVyzkjj6Ck02UyPLA5IRz8rHuav26lX+zKrBGc7t3Az60VI8ug4u6OUm057tDDETgv5kpHf\nFUGtkN+tkxdYlJK543HrXZRRQrPMEHyKNrY7muauLWQa35jfd5KEjp24qozsrETgm9CxBbWnnmRy\nyyiFgFjOORWjBocVs9nqF1Gke4b1VRnfzxmsS4umXVGtIiAsa4LgYy7V0aXMkunhGKMsKhQRzmto\ntmErXsc/rd1fedKsWIY0ZsIhzu54478VjsskFs99cODcythUI6Y4FdibZDbSz3DANGS7GMfMw6YG\nenFcZLcrfXwESOIoy2xGAyM9c++a6Iyvoc0o9Snc2rxRCWWQNv5JxyT6UgCpZIiKyyOxJPtU12Xm\nILDaFGMGoWdSE+9x1NaGZGcKxU5bH86eq/KQowOpNJmMNuAPPTNO5Y4U8mgQ9QvlAE4XqR3pqPEr\nIxz16U91NvJNFIPm6c0kCqSMkNtFAwaVWcnapOe9TOzsBuYheoUDrTkaM3CShR8pJ6VJO3mtJKJN\noAAxQBAFmmjWFEwAQck5x65q9cWrxqUM25YgCQB1NEEQRJnLORgY2/yqtIZWjdVjc7juZs0ATGSM\nrGI4yVA2k9ix7U2RCty4lbdKABgdFoSN0jVAMDdzntQ6iAs27Ld/9qkBPI7yhXkyQuAOKry7zMid\nWPzNuqzcRoYIizHzGP3VHAX1qssT75JXBBxxQMWRhHIqZ6d6If3md3yxgZz60BVTYzAe+e9K5eRy\npYBcflTECyjz8hl5POKlZmc/PgheoqvCsbysFyI0GTjqafvQ7mkbaf4VHU0ASysqR4ChTx0pEMjq\nwTcF/iY9KhY/Lw3U8g1Y88CPy2OQegFAD8JEeW3Ejgipo/nKxqcyucBetUDIFGMcsOKt6dMLadLh\no22pk5zg9KAJfKmhR3mUBFOCvqamktYPsbXauUSMgLGfvMT6VXu7l5IkkuMDJ+UZ/HNRNulkjlkP\n7tRx6UICVGNywRchFGTTspayLtOST0PamQyRw3LDeSrDJwKY5SQ78bj2FAG9FrVsmj3mlyztFPcM\nhh2j7xzyKrXEQjKxt8hzzzzVSBYAVup18yWMkxgdjTCzXc25mOep9qSAdFi5kaGFPmDfiaSQeVL5\nbHkHBpXZrUgRfJ/t9zUKtGqlgoeQnqaYi0kDDEgxjOMk1FNcSTKyg7gKcpk2KrMAKEdLRm2KGYDB\nBoAhtgJn2KRuHrVS+iO4qeufvCppVjitmmRj55b7oqJmeaMu3HHSgDS8ETpZ+L7IyN8pJFfQaMCo\n6jPNfNWmf6JqcF1k/u5Awr6N064F3ZxSgfeUGskrTLbvEndfnzUinPFMfqKReGzV3sw3RKKWkpas\nkQ9KB0paKACiiigAoopu4dKACndKQUUAFFFFABS0UUgCiiimB8pyRSR2olYfIagiD8lUzmpftLSW\n/wBnAGA2dxqSJW3AAkAZ6UrCEjnklKmQZC+tWY4Xdj69T6CoXjaGb5wQO3Faum3otUmBgSYMmCWP\n3fepC5R+ffsjOOMZqMzOC0SyMUPB561JcOhBCuN2STj0qmjfNgLSQyxE5RyGGQfSpgM55P41AjEM\nOnWp13MSRg0wNLSdMm1S6EMUZZiOlP1WwuNOmNq6Fdp3N6Guk+H0wtNR3yquGGxcmus1W1t9Ye+i\nijTehADkfnipk7MdtDx2aEpy3UjNQiItzjOa6LWdIktrvZ/CehNUo4ooo5VGCVHUnvVLYRSxFLGs\nYTY69Tn71UtV0zyQjhtwK5x6VecCJ9w4U9aqyM1zu+bAxwaVgMIhnGAMlfT0p8bhc9Nh45FMuS0M\nnYMO/rULsN2VPHpTDY3NKuxbNIFkcA9Ap4Nb9jNFtZZWZg7AE44HfrXEwsQy/NjJzXS2m2TbG8+y\nIqWJPqK5qkeWV0dVGo7WO0s1G5o4zlQwcZX5sVpXDLdQgk7Hz949awNIPmwqRKDIFwCT0FbNu/lw\nrIxBYHp7dhWMl3OmM0X/ADLWztZLUx+Y+N2/HBP/ANasTU41jtmuRgzKAV9v/r1escTTSu27GDwK\nxr28+yw3CXMDMpcYOOVHHT6E1nKWqSNI35WznL2O5i1CWeVYvmBkORj6DHrXT6K/9o2cksexTAiE\nR45Y9cGsrUWN/MsjEbPLVTt6n3NaemFkYG3C7RyCvHQd66k3JanIo2bZU1DxGH067t4YdzKCMnoO\ncfia4eCR1YmJSZWYnbnkCuo8XQCGQm3UpHOgkwvHQ1ytkVF3wWx/e+tbUlcwqaPUuBS8v3lKoMu3\nbNV5pQYVCHhu571K4BWWKVwNvCrn7xNRTx7ZEDL91elamRJOAWTBHIHA7D3pVLRjevAXoT1NRYwq\njkHv71pRWrusbEfKOTmgLFSZd5DNlnYZbPrQNhXaF+8QM0s5LO7KxCA4yBSwBcgdQBnNAEsjRW8W\nw8tkYA71I8ZmIB2oGGTjoBT7ezE7CVnUKoyQep5p10wVJTGBsZ8A0mMmmmia3RIV/dKMntnniq+9\n3gy7qEDcgdz6VLAhXMhG5QvB+g7VHPtDrGOh+c0AKR5Z2NkA8896BEGkaRx7daFtzIPNJYKOXYj8\nqljiVsylT5anuetAiKViGRc8sMfQUmJCp5LDOBmpDCWyzt7j1pDM0kUccYwFyWb1NMCLZuKjy8t0\nxmprixVUhVtm4/eGTnPYVIpCw5iA3Bup60skrI6fMA+c475oAbJDHasY4zkD7zj1qK2URuZgg4Bx\nvqRlbaRtZmJyfc0qQsZTECu8HByelAinIwL7iMFj3oClvm37c8DipJBul4O/HTIwKWOIupLPggel\nMZHFEXmQLyAMEtVqMiSTyNu7B5z0AqOOEjGMnvmpQyJauDxuOSc8mkAl8oW5hMqiST+FAeBVme2k\niiSVymCOFFUIZy1ysrodq8Ad6s3EjXJMSK5YZbYB2oASBT5Ty4yzUlmpMB4Vdx/GrxsGa3jjCbSx\n6c1a0/RmJJkcbAucAc/SgEUZJFQRQwplj94gck0pcRFREBvU5J96bOHt3wsRDSHAPpSJBcurOsTh\nc8k9KOoEc8plYbxuI5psTDknAI9ab5Ugn2Kwkbp8ppbpkCBCOnU45zTFcc8oSIMF8x2bjJogErzy\neaNoPzc8URwEmN0G4j5yOwoUSX2ooJpBtY5bBwAKQEc7NLIwhQYPQCpf7OYRDMm5cZbHap5riGK9\neKxjYRrwXI5IpFgdjJGGcKoyc9TQwKt5LFHBHFEoHctXu3hadZ/D1nKP4oxzXhs1mTaOxcJt7mvW\n/htP5/hSBNxcxsVyaymveTNI/CdjIflBHNNAJWnBhgj9KI+Rmre4kOQnHNPpoGKdVksKKQ0tABRR\nRQAU0jnNOopMBM9qKaRzSqc8GhMBaWkpaACiiimAUUUUAfKKxlVygOD1xVy3eNYAPLw27OT6Vas1\nSS124+bFUDGIZmVlIK9Bmp2EXDCHD4OSRxk4xUDSiBmQLktwQaelyhkXd27VDqcv2lhJDHtZe3rQ\nxChldXyq5YYOe1VsbJCo7Y6VEjvvH8O717VOiMZgoY4J5btQNF/T7L7XcxRtwpYZxXVt4btbe6WV\nHLwd0HJFYdjbO8mxD3wSO5rsIVeBEV1A+XHTvSGGnRw2JkkK5AztGOlPs9bt7WZUZsF/vDNZt9OI\nbaRnf5zyp7CuVMhaTzXOSTnmp3YXOq8QXa3s6JbgMQDnbzisC6tDGysp+rVt+E4Jbu6Y4wD0HrVr\nXtFe3bzliYoTtyOgNVogRx1wjMuT+NZzfKSvatu7s2ibbk88msidDu54NAGTqcJKCQDlf1qjARJl\nkXDd1rbZdyFTWJIn2eZpFPyg9qBj2Cq/IIyOgrqNFtVmiEjP5kRXBXuKxbRFumH8akc4HK1e095t\nMv41lz5UnGexrCs207G1LRpnT6cpWXybfGew71rKskjrE7FW6n3FZke21ZZ4gBghtxqxdanJPNG5\nZATyWiHDCuGMnJHouMUtDUlRimI32MhwWB9ap6lGBd7rghoQoc5b7xxg9KlsJobi3baHG4k7m7mo\npSkivbybvLAIJ781Ss2JrQhsrG2/sq7nuFwckpsIGVPNV9I1JJIpoYEEbg8FmAyB29qR4THbQ2kw\nkKc4Zz26/wBKz5I4ornMZZbQSr5j7eVz6Cumm9zlqXvdFnxHfsqQhYiZIx86OM4HcfSstfD1xeqb\ni3wIgvmYHQD0+tXf7Vin1YF4RNGCU2t0Zc8VHGdSeO5tYSY7WV8Bcfd9qpOS2Ikk9zE2Rpq/lzDK\nJkqV6/Wp7y18pTKn70YHIqBLW4sdQAmjXMbBiW5BFXYb0teKY8tE83IP909a3Ur6nO1ZkNjYtcTh\njkIo3O5HCitiz0q41O7dUJMavyV7D6V0qW9hfWBt7K3AaccuOg29jUC29x4XIuiWKzN5SoDyT607\nhY42RRb3N5b4jJUFRvHOc9hUCRh/kBwEHLetSsGe7kkIy7OWOR9aF/0ZwoZRvBXJHTPequIYxQAC\nNjxgEnvWs0/k6VBb+XGyB95LD7xP+FZdnEEMxcllxhMd60b6XNtBb/KGhUBifTvSeoF4z2UejHMK\nNdMeFHG1R3rMVI2YmUlSUJzjJLY4/DmiNMbtuQNo5YY37ulO2KxZCD8ucnPFC0QDgGRGjLZRlx16\n/hVkGOO1APz7mDZHQAVWgBYq+FG4E/QVI0oUY25HTOPWgCuPNeXaoJBJJz0qSOFCQhVowOMg8fWn\nNITHgegGaS2UGRpJZDsUZHOBQIv7LS3tAiq0j5z83ygDFU3uVAQrHGpx95FyafOonJnZiEI7HpUY\nhaOGSQAFQnyqaYAs8hRi77kB6Ed6Xy8HDqAzHJ29cn1qKLzLiNpCuFPJPqacXd55NrE7RkE+1MQn\nkIJCM42DFSM6DCxw/Jnk+pqKGMu5YyHaFLNgdTQ8ohkWLy2LHB2g8+9IC/bSpZzb9iM6jI38jNPE\ncccbSziPJG5UxnFQLKS3zRgdzupkD/abxi4B3kjnsBQNEOclptvX7igVagdI5o5JCzySjLKOwFQe\nZGZWjXJCckDvVu3YC5Mmw524UHsKdgLFxNctbM/neQqpkBRzmmafPPcQeTFIy7uXlfvRLB5s2xiQ\nnBfA5NWtNdIZZAWKIFJy68HPQCiwrld7YrdsWlVmQAZBySe1S6zLJHp8UDsV3DLADr+NRspjlZmX\nLdeOOaq3hnlmAMuIwOg5p2W4XK1o+GGxgGORk9hUV9IfMEMZ35PPFaqixt7VWSEtKRjd/WqAiMly\njhlIHOMYoQyJI7hLWZY5BCCmJGY9RnoKjsrdmR5gBgDua0DC1zdhWhztHanTT20dxHDIoHlqS2zk\ns3pSsIiW3uJd5iBCKQSfarUMsiSCX/WBuHGPSooZHEBCpIS7fd9fagXE1hvgcKJGH3cfdpNAWdZv\n7E6UtpaWzKxO+V25wfQV2Hwku0bTLu33DKyblA7g15zFafaLom4kDBm+6OMV3Xw/ls7bXJLS3xna\nQSDms59y1qj1QgZyKjLAHAqXAIqDZ+99qp23EidWyOadmoUARjyalBppg0BHFA4paMVQhaKOlJuB\noAWiikJwKAA0gHNG8GjNK4DqKTNLTAKKKKACiiigD5o0+R3+XbiMd6jvreTzyx6g8c9aZZyE4QMV\nA9utaDRecoba8hUcYNS2Sc+VIbL9c1NETPcxxq4GTjJpJopIpGDKTzUUTtHOJVUEpzgjvRce5Omm\nS3KzzxKWWLrlh1Jx09Ks21s7wYDAMpyQKo2rOxkd2K+mPWtWGFWjEiXQWRuWRqT0GaOkMX1KGOHd\nu9ewrr729WGxJbDSAbee/vXJQXVvplvJLAxkuuxB4Bqil3eXD5klYk8tk5pPUC7q0ryFFbhQOlUY\nImb5ypKDg8VbupTMh5BbrgDpxT7aST7JLGi/fGCfSmgOr8Cwx3EzyLIR9nxgAfezXd6hHarYt55C\nxnLNkd64nw/Zx6FaSXcspCSKu70xWZcajqUkc0012xiaRtsI9O1TNczKUrGleW1lfqyLt8wD5SO9\ncBqcPlzuhGHVsc1u22ohH2kkP64rJ12Rf7SWQnIdQSAe/ekroV7mDIAMfMT7Vk39tiM4J9cVs3AI\nlBUfIRVa4gWSDIbkDpWgGTp1wYGwHKkHtXcWsVvdWe2QK+RlDnBFcJNbSQtvGCOuRXW6DdRPboGi\nYSgAj0NceLWl0b0NXZnSQWLvatb8s+3jNVLU+QLm2dUE3BjGOfwq3a3ZiujGoY5wQD6VLrBWaMyI\nfLkHRh1zXnQnyT1W56CXNEZZh4pFkTLbeWXpSy3J89ZkYbg27BGRxWfpl+yNJBcSsnnLhWx1P+FX\nvMyJUBQYGCcc8eldW2pKu1Zj9UuZp3UG3TayB3G7OT7Vz7W7SsEeTcrE5XdwMdK2XSW5gN15uwx4\nHPcelUby4tYbne52kruC4yN34VtTluZ1IJWDS/JgjbMcXmbdqMw+6Ca1dLvUtoHdSkoYOrDdhhxg\nGufcSPFth2Mu3dx1BzmpZrnzYM4H2hXG7Zxle9PmMXG43U72PUIZbjyyjqQgweSB3rEtpfLkO0EK\nRxntitOawkO27hjk8tssyuOnNRNZ20NsssyyFlYnC8ZB71vCS2MZRZ1Hh25gdZruXeyQx58kdM+v\n0qK/8QvqckNq21QhYJt+Uc9/wqppc1omn6tIl3iUxBEi6Fgcc1gyO0N0ku0FH4bcc81aM2bs2nx2\n6m5WZBtOSf8ACsy5yt2Y02FPvb2XmtiW5t77QZLVP9eCpXiuZa7k3eXMMyD5WIGOnY1Qma9m9usg\neRWlcHpnAFP1C4jubhTDCiMBt5PANVIIiYonK4R22n5uuDnpVuK0N7e/Z4xiUsNjYoYJlJLu6lXy\nDIZI4yFT5enbrU8cirvcjeVBKoO56c1A9tPbM0c0bRyAYCsMHgmpNPMauskgZ1AO7PrT0sK5Jar5\nsigjaMY2jpU0o2vKrR9RjDUz5opFIyM8jHb2qSad5J2mnyXJHUYoHcV42a3WRUxCWGWz3FXEsvNA\nMSJ83Y84qrPbSRaXHKJQ8chJ2D69q6rREUXcbbQIo+ZF3dOwpN6Dsc9NG8e+3kCBYxh8dAaijKpA\n7SEtkDb6Vb1CLZfXUewMTOwCsefbNQskgYZXzGxjAp7kla2s5306VAm1VG4nPQUhtkt40SJ8k9d1\nXLJ5Iy0cu4oflKegoS1N5eeXChDEjG7vRsBVTMUZQHA28nFOtlczysqgleFYdam1Cy+z3JWTO4Hn\nBotYlikVnOUJ7Hg0CK8yu8jPsDN90Z6fWoWEidRz6Cr1zat5rYJ2EZU5poiHlruUlu2aYFeBPLUT\nMFOeWWrbxJuMsasgbGMtnNRraNkFmAVRk1OsSRlHQGR1HU0gLMAXyTI+7JblieTUslwXngiiAAiB\nOeucdj+NNiBKl8KD0wecHHNN81bSaNgpy3DZ5JoGSXl9KYHVfLQMMthckn0qjEA0RdwC4XA4/pU8\ndqbiZjgBMkjn9KvTxxI+IYiF2gKCcc+tCYmjFWePynRojtB+Ud+PWooLV5pTIEJIGeOwq7JCqYCq\nMtngnvUk9vKLZws5jkfHyJ3pgMgkeS/QzuohAwQnU1WlTzLt5Ej8qIHlm7j2qza2Qt1JlkZdpBIN\nNvGF/foIzmKMde2aAGyXTrOsltFsCfKuRk59aJbc28ltO7oJpi2QTuOPetMRWtlZtcSMz5UkDHQ9\nMVRtYvP8qVbVYdxwHYZNAFSWyDBNjnzOrHHFXfCTy2Xi6zVOkjHdxWhd3ENvasCAI0UqxI5Lewqh\n4Rt7i88TWjqyoBISA55xWdRe6VDc9xDAckjpSMSTkCmshMXJ+YU/P7ofShbDGEgmpU4FVWB3jPWr\nCkHgnmhAx59aUEUwttOKR/u1VxD2NRnhs0IS6c0BTnFJ6hsSjpSP900o4FBGRiqEVhk5xTDKQ2Ks\nKFAIqu0WWFQ9C1qWUO5QakpiLtUCnVa2IFooopgFFFFAHzG6CKcuH6AHA6VchutoPzEPnp6imKI2\nDtKNqqMCooJIZptm/YYx8pz1qBFq5t5JLbzfujOASOKyPsz5k5AwOgrX1G5YwxRL8yd0z1qhZzCF\nikqHIJHPUCgEZflSfdAyc9BRsdpACzZHocVtGJP3c8Y+QtkAjvVaZRLdkqoWhPuMIYmdAwLuiqPM\nA/gOa0AkabjERtIwM+tVTaS7nnj8w8fNjgEVbjQHY2RwM7elIb0FSMkEIpPHNdj4e0uGbSJpJBy7\nccVneHrL7QxmkO23GQzYroZtQW0055CqxxIMIPX3obEY2u6qFQ2MH+rXapY+1U7SQzsiKqeaTyz8\nADtWFdXzS3Eki4AY5zVRr+SN+ZG464HWhaagaV5ctHcSMgAIyCR069qztTmE8saD7yLyfeopLiec\niSRTsA57d6rSkbMDOCcmkBEZOiHn39KagQyfPnbj5jTTuxnt7VExbLAZ24596tAyOZjt3xrhT936\nVX0/UXtbtAzZTd0ParT4eERg4wMCsuZfLkD4HoRUygmhxk07o9GsZo2BbcS5wyc8j2ralgN3Z+bE\nA2DlwOtcFo9wzSI0bOpAw6Yzx7V2Ok3kwChWwQMc9x715VemenSqXiUFaF7yMTOTDHyOOntUtvNH\nPdssLthmOQR0xVqexguHYiMBuQwU9/WqSabNa20l8VZF3FMq3J9jSpVFL3WataXRbuYxI6xxyblY\nfdPofWsfUbUwypKsQBiXaSOQwrQDxOY2hwHIzjr+daywwXWnhGXNwpySv8YrZ6PRidrHHQTTW8bS\nCJXXoF/wph3zyeaFYbzkjOAuO1X720VbiQQ/u8A5XPP4VVHmeQqTbhbg4BUd/erU7mU171zZs9Si\njsTFbM7Fh86umStPuoll0+chDISvCbcHPtWXDLPA7TIG+UHj++Pary3tw8sFw8G0cLhj1oT6mc1d\nWMGEfZ43K53OmDn+HBp2oRjykEgBcYJx6Vs6nZRSSJPbnyWZTvjZeGPqKyI/KXSZBKGF6snDN0Zf\nT8K7IyTRxNW0ZWwjacxQMtwjZBD4BFZtteGVj5q5dju+p96twzpHI0cm09smoZIbVZXbBVwCUKtg\nE9qoVzWinE8bSxw7VHJXPfFbsV1YtJBJZufNjcOqMxK4wP1rkIpSqeYGPuAetdLo0WnHTHmecRzm\nUqd3dcdqTGiGfUJJZ8SnzXXLMjc/rRYW8eoOIIH2zkblQ9D9Kpa1FDFKskNwr+YOqnJHPf06VWtb\nySCRJFl2un3T6UxdTUminh87emNp4PWp47d5p7eFzt3HcXxnIqvFqrXaHzymWPOO5qKbUZVTyZFV\ndp4kA6elMEPTE0yxkLEkcvyjdyQT046citZLmXzHMUpzMBu2Hr/nFYIuUmdt5VJSQdw74q/Z3BDD\nnPH3sUDctTduJI7u5gmMmHjUCXcOCR0/MVBc3EVzJmGJgTkYHqaqw3O1zJKcjd909xVqaWK31NWi\nUmNvm49ccUCIU6tGyEOGwdx5NT27raOzRgbs5BPWooyJzIMt5pOd3WlZHSIkg5HamTsMl3XCM0zE\n/N17/SmLMkrlI4isacZ60CZXUjjKnBBpwUoAAdobk4pgPgm84FWQmLOAfSrDPCuwnfM/+zwBTIWj\nCY7rzkd6lgjSSUNI5VG44POPXFADnt4SxUiRcruAVsjn1qOCJi2RkqTySKJt8ErlJOFB6DrU0bXP\nkW7TyIQQQiKen1pDI/mWTbghAf4aJGBkCrHucNuVieVqYkmRXC7VAznpmkieS7nKoqiNRuZsc4pA\nXZYgixhSCxGcjoDUbBpYAZMHacADjjuauiFIYUnMg+YEBP6msmUNDNhlLjrx600DHqUSZX8sMVyT\nuHQGkSNUl3c7yeOeBSSkwwCRgQWOAO5pvm7ihZ+OmO9MkLk/aEZAMnrz1IosdMuGh8wFFC5ODUiC\nM3KuhJ/vY6mrbP5RIJwO2eaAImtRMu6STKAjgdOPSpVWOEh5Q3koCwUevYCkhDPGMLlFJJJqfy5J\nYtzMinPHsKAMa/jkubeN0BWV5CAjL696q6Uk1jrdm7Hc6zAMw4ArfuI0WGOIMXxkFl681gakxtFM\njoVWMg4B5NEleLQ1ue6xtvTjp7VIMBOe1Yvhq8+16TbS8/PGDzWxuLZXGKyhL3S2iNzv+YDpUTuQ\nQR1pyAoxyadGMudwoerAkSUHbnvUxAYYqvIvzfLUiNhcmqQmgAKHPalHPIoY5XNNUFee1MCQHPWn\nU0HPNOqhEUq8ZFVVdi3NXiMiohEA2amSuNMehyvNLkZpCpC1Gcg5phYnopqnIzTqYgooooA+dZrB\nrlsowB/jx2FYE6eVcu8e4gMQCO9dRpuoR2zl5ly7rg471k6lFJHASVBVjnI61GwhlgHmuNkSZGQf\nm7VXunlF9KXAzv5xxirVq0dvYrcRttm3447iq925kn3nIJHOfWh6BcjuLmUqiO2xUHCjvVrTYXuy\nBGGDEZGRWZgyyBByTW9aXRtI18oZK8LnsaQ+po3U8FjGkc0pZwoygHFY8d7BLcNI6SDc3L44H0FP\nkZZZHdiZJD1Y/wBKsPh7Botqo3XzO4oUbK4ORuwapBa6eEhjlcdGbbxnrzWPf6m958u52QDAWsiz\n1K5iZoJHzGeCM/e961NPhjic3Mhy6/6tByCcdzU2dxmbFbu8mQCAOuR3p7KoBymCD3711GnWyXge\nKUAdyR3NLqWhwrAxBwV7+lU2Bx8jNISTxVZhsU96uXMEluxDA4PT3qnKSRxTEViSO3Wmk/Oc9MdK\nc/TNRNnGAeOpNAFZ5Nsm4fdNVbhw3OM7qtXUeYiFI5qrbxM3mZ4VB1NMCTStQazu1DN8oPeuna/k\nTfcJcMMY+UdCK4m5R92D+FamiXBlb7NIwzjjPcVzVad/eOmhU+ydjba1cSyAiQFgM7cYrZu5c2AE\noLRSDOFPRq4d4mUYDbQTn3Bro9EnaZWhYlmYZUNzg1wVKaVpRPQpTt7rJLGCUyt5R2jpz1/CtMzL\nGzEhxuTHy9RUUD2iRhpC8coOCT0z7Urvv3NEC+eT71qldCdk9CKW1V7MtnMhPysB1rG890SeCRco\neGyM/lXW2rQwywIZfmZONxxg1zmtae9pdnDOxYclf5miN9glZrUz1lDadJbmRcBiybiQR7g1FHNd\nWSW8Vxllbpk85qcLA8A8x1Dk7fm4NWrXTWlE7TFGCYcZPP1Fa301OdxtLQtSvBPpiYmkWZBkq3fP\npXP61dG3t4YTFyxOHPFau+URNGVWQLwpPBx61DfwQzWcMtwzvDEcTAckGqpvlZlVhfU5FiUlDSAs\noPzZ54qaW5tijEEnn5Sv8qha5kia4W0jc25UFyyc4zVYPFKcqpAPf0rsVmcuxq20geFjvwVGQCKm\nt5pAVCFyCOhrJiZ12qOtX7WZ/OEUjAZ4Vh0osK7JnhJuducEnoe9IUQSsASR0NQXjzQTsrMDIh+8\nD/I1Wt7uUZ6HP3vUUDNKMhcJE43A9+tXbmPfZwvvHmq2yQe3UVjq8TzkEEbh196naVjBJDGdzYB3\nH2oEmSTH94ifKVj+UMvpWjDcbQoAOFXAwetZlvFLdIQIvukcqQOfx+hqzDC8ce4Esf4lJ6UXG0b1\nrNA6FJCMsPlz2pYrkNGuF4Dc9+lY0SsJPmBGBmpYIyJSecd8UxHQM/l58obd3IxUxnE0KRyNtcfx\nVnI5UKudy9vpTplMbhw+VPRTQgJzCUlI6Iw6+tPxvRVb5sHH1qOK8CqYnwc9Ku2Ful9MyxkBUGfm\nOMH1ouBGu5Uj3oPJbOHx39Km8vMiyueI12U1mc2osgCyJN5gPqMU+WdCWRlC55HpQFiRSY7gmQhg\nRwMcU9RkvkYyeAO1QbiA0hB5Hyiq01yIJrZp3OyQ7sA+hoQGq+5oPQKNqrjtUkEoiyghyzLgen1q\nO4KoTJH91vnVc5xmpLFAkM7FMuvILNzQIkSBIbd18wGVmywz0xUKDZH5iqdzE/M1M3leR/EQT70+\nd8psz8o5zQtB3uRhAwVpCGPIXNNkSEgYUlj6VYWNipcgHHcdhSNmIh3TJ7AUySO3si785RAOTRt8\n3EaMxVTjIHU1N5sksfy9A24oe9Q3TTXDgwRLCzMMsp4GKNgL/kTRKMLztzknpUC/aMiGJwfVietW\nPMLQuz5dyNpJPHtSC3BtQdpyGwCOOcdKAGRvInnefbhiVCqQemKxdVcygjy8sWzt9q2ZJJpyiLHt\nOPXrVeSwkkcrKyq5UjB5Ap9NAjoem6HALLQ7QEg4iGMe4rRU7iG6Vy/hS+uJbB7W5cM0DbBx2rpU\nPArCOhqSsgIBHWmupPI6ioGaQSf7NKk5D4PeruFiRMvn1p4AVSCaZD8zFh0ply5U8daQi0oBXFOx\nxiq9q25NxPPpVjOatO5JHyh9qeDmjANMwQ3FLYZJS4pByKWqEFIVBpaKAEAwKWiigAooooA+Yopw\ntwXPJXgCo7nUJLhPKPC96aEVNzA/d5qAjLFs5xUCETgBc/L2HrUzRM1o1wWOVOMZ7VASWZei9qa8\npj3xBuO/vTAS1kRbgMwODxmrzTkHyo+561RtzHhg4JLDj2q1aqglBwS23gUIPUvRFY1yTz3pJrks\nVC4we5qJ1GAzHaAckDvUDr58qoCEDMFye1DYkJexrJulH3uwFa9m0EtlCqTZwAxXvnvWNKXjmeNS\nGCkgP2OKht7uaK8AjXfkYwvFD2GjvbTUIbdy5AVMAcD0rUlkt7yJ49/zvgA5xn6VwKXMiTJNNCTG\nTwrHg+1bEk6/Z7aaNsSgZZfSo3KG6zbpYXflH5oXHAJyVrDurRokEicwk8H/ABq9f6jJevudDhRg\nH3qtDKyfK3KN1U9KoDNZHKMwHHfFVmIYYxgdxWveobFDBs3xONyk9jisg9Sc5pLUCrK3I6bFqOZt\n0APIOc49amKBlMY49TUUpC4PU9OfSqYIrSDz0VAANlW7S0WC4SSLJbuKgYLEQwbIJ5rT0yQx3aOu\nAp4NY1G+UumveNq605poI50L5YEMv8ql0VR5h+fa4Azg9PWrkN6XDR7F3BeoPXFVopoInZ3iCM4O\nCvrXnRlJ3iz01H7SNK6hxcqJOYmPJq9FeRpeOsG5ogAAxGe1ZsF2JYPLZNzKeGq1HqZjkRZIEAAK\n5XuT3qY3vY0dmrjLpWkUFpCS/O4gc5P6VzV9JqUmrulmA4iABRxncSM9ew7V1CmVoolljRlBPzY5\nOKYsVuJ2kjUDOFZvb3rojK2jMpq6ujlp9Rs2ljsru3a0vA4Gw56npz6e9akdxPpzy21wmA64Hzc4\n9a7S68O6VqNpa3Wp28UhdGEbD5XUZ4Oa8+ufAWovqDzWuqvIqM3khySRjt+VVGcZOzMm2jdZYIvK\nuZD56hcMhGMcdaptqVtFBJbmICCRSGOck88VWsdTt/KTTtSkNrqUIx+8OA/uD6VLfWqywoYkSQjq\n6Hg596aTi/IXOmrM5fU4VtpDJC7LGwKsgPUHoayd6AkRjj1Heui1GF5YC7oXUDYOMBcVkRafyryY\njgZsFuyema64SOOcbFeJysm45wO1WbefynJGADx8/OKSW3ktmaEhZFPKspzTbbURaMxMMcrGNkPm\nLnGR19jWhmtGEreXIULfKT1qSHbb3WZWIGM5AzUcKrNCWOScZyKRJEWJUZdxPXd1FA7lhyJEAkwA\nScMtVopDbygh+h7nrT42YRvGBkZ4zVWaI+Vnbg57UCOksEhuVDhsQs+Vk9GAPGaqwXEkbMcggtzz\nms1NQiW1hhhidHDfvPm+V/fFIDI98kUY4kbAGeM1NrFM6J74O+8gDPHFW7TzJpQ8ILsBkqO4rn1U\nxGYOdhiI3ozc56cVNDfeQM27yJKOB7g0xWN8yDzdyAIG64PHWp95SVw5V1B4I6Vg/aRtQxOefvBu\nxqyl3ldmCCDnPXNFwsbDCLKsrcnpT4meKUvG4JxgqKzBMJMYZVIpyyncxJGRxkd6BXsbkF1JGpUQ\n/P8Ap9adJJHKSHQoT0zWOl+yBQCQxOPm71dhv7a6lMUirE/Yk8Z+tK9ii19pb7Jszkr0/wBketSX\nl5ZOiC3hRpEjILOepNMihn0uVnkbdG/ykbxyDUq2jW8byJbCVZDjJH3QfpQ5AkFsceZBIAJGI2sG\nyB+Na1oqXlwsDPgY6jvWZJHHEuFbEi/L7Z9Pyqv9p+z3DNHMQrYI46HuKaYmat8iRN5atkgcn0Oa\nYTnaMglunvVJ5lfBLEbuoPc1LFMMHzDwi5FUiS6suyZo1OOBuJ6CrnyPGzltzDgZrGtrkyO7HHJ4\n9xVmCcRgllO0N+lFgLODzsHzdz2AqQYAyByDxio1YMmI+uSTj0p7L+7lAkxhhgetHqFhTliCFG0t\ngrnp71O0gWNlJ3bWyozxluM1BHIUEcoQMj5Xbj7tWImW3uw5O9W5DEdSOxH1oEMu4xAs5QfNkCMp\n1A70l2kl3aeYi7ZVGJCTgcdAPep3Qyv50rFZGBJDdDz2FLeBTY7QfkJDhh13elCtcZZ8DO0mq3iy\nSHLgPtJ6V3TNh9nSvNvCQls9f3SncWUqcelekTJujD+lYNJSZoiTeOARUM8fGV61Ku2SPcOopwwR\nzVoCG1mMZCsOtWpwpTOKpSnZIT2oWYleTxRewdSYOBjHAqyj5FVcKUBFSxyYG09aS0Bk565pCaA2\nTQy8Zq7kgjdjT+3FQsePenxklaaYNDlJ706kNN3c4piH0UgOaWgAooooA+W5yOxGT1qDGDtXpUxA\nYA0gUu2emKzWhJGVZCMkBuuPSq0pTIIBz/FnvVx180+Yv3VOOe9QTcvkjApopEHK/NkCp4ZnKk5C\n84OOuKrlS/XoKVFLOcjA7GmJlkztJIxXhaanmTzBVPzMQBSQx5k2dF60vK8pw2cZpAhLxZrSd7WQ\n5ZDhjniqkbSrIMMVyw5B96mk3N80h3E9/WmSR7h6DFAzbuo57UGzlZioYMjHuMdj6U+2n3YRj07m\nsJJppWWOR35IUFmyAKvYe3maCdgJFAwwOQQaVhmpIxVJMDd5nT2qrEofkfM69vWm29yY25yVHODW\nzpj2qwO7DDkgDA/Oi7QFOXdd2k8NxhSo3Ix6rXOsvlkjrg4rstXAFiBbwq0TLhiB82evNcjKDjdj\nAqY3uNkWwqMmoJYjPgRIc4xkDrV+C1a5dEVto6sfQV2OjpZQD7OYkZiRtbH3qqbaWgI8+s7ImRop\no2DYyuRV63h3v+7GMdc9hXb+JbSJ9N+1QRqJYSMjHOK5MtGgd3OA/NYc3NoaWtqTWxhBUhsEKVz7\n0sU6qF3unDYK4rMEnmTAq+B6CpnsZHcmMq46kCsXBR36nXCrdWOrsrJbuCT7LIBznaDyKkW3udOk\nO9RLFjqRy1YWjXc9pdqVYq47Eda1rq6urx/mkIIGVwMAGuZwcZ36HTBpxIxc75fLVWZF+Yr0471M\nuJWYL8sLHHJ7VB5LzFf3gEm3B4wDUsUTW0XlTABmPynv9K3i0yeW6Ogn12wnmtLVoLiOGFdocD73\ntS6ncrbSgrmON8lV24OD1rP0+WW3FxGskcTqhKiTqOh4rOvLqWV8b2YKoJ3tmjlSdzFJ3MLXNNj8\nQzJG0fkvESolAyTXOyWeu+F5StncrPGXAwOT9MV3hJjth5agTvkZz0z7VHb3VpA80hJmcrsT5ej+\ntXCbTt0JqU+ZXOPi8YQSK0OqW7wyoccDIz7jtUk99pFzG6Wd5iOTB8t/WsTVG+33s9xdR/vXkJOB\n3p9v4ahuYQ6yOXI5UcbT2rqskrnJzN+6aESw4JJMmONy9AKT7FBJMWRWEZ7mkuvDWraIIry1k8+2\nZwpHbPoaq3kHiYv586fc+ZQAAPpxT5kLkLbW3kzgQsVibiq91blZDn5uOorOXWry2b/TLdtn0x19\nKvRarbXQ2iUR+obgmrTIsK0TRRrKAWBpoLkEMOvrWgimeHyVxuP3T2qN4XtyVlQeYp5Ht6ii4WMs\nQebcqWcRqTjPbNSsghmYZB8s4DIeCexq0sSk7VG4t07YNRSWxD+U6eUwHUUMCDzJZW80q+TjLY71\nOS4XeSQ470PcTywxR+ZuCfKE29qlSWSEgSKAzDID9SKQxI58IZHUkgZGDUsNyc7snPfHQ1fg8pLh\nJ/s6KsisoY8gcYzisRo5Y3YxncinBIFAGwbiIswyATjg0sU7Rk8+3WqEMm6MHjJPIParTxumdsbE\nYznHamJouQzLvd5JG3KvyDHU0iTlrhW5J7Y7VnKZgTyBU8EuLhQ2cdM0PVBc76TUDPpdt50UcpRB\nG8Cr82M/ez/npTLS1ub+QrpxnVX6xp0J9vb/AOvXN2y3Z3iOYtLx5aFsb/YH+ldl4E8TSaKpOoiI\nwSvtLDAcH1+lZPyNLi3nhPWIrZrieOXYBnPHbpmufk8+Mt5q5UDsM5717Nca3p5tpGW/gaJl4Bbp\nn2rzPUdOinma7ikEkCnEjQtkj3xQpCaOZ+3GRxHl/LU4+YdPSriXUsoLR46bTk9aTUIYDeybYXUb\nAUlxgtx6Vm7QGMeQWODkHFaXJsbEbT2vlxyABWG4HuB71diuTKGU5BBwQaxLm9hm1MPvkEnlBSpX\nbhgMfiKtRXEsCb5jvjzx64ppsTSNq2lG0FMhiMNk9a1CIfsqSq43MpyuejCuYd1Tc8b5jyCFzzVh\nblopVZfubcnPftRbURqxXjIr2rkYRPMTjk1ZspRDbh5CHLAnHdTWJPdpPEjsSJlwA57r6U+bUWjn\nQtt2gYI6bqom1jfklBtFuA4cphGGeQD3okuIZFEUcq/d+YehrGW8ge4hYykQzL823t7GnECSWWZJ\nMgNtTjsB3oWgy/pUxTxDaFOQxKvk/rXpxkJg2g59SK8btLxIdVhmkn2JGwDMv1r1yzmBiBRt0bjI\nJrGbXOaR1RcibbEcDNMYkCpIuC2elKwUcnkGqQiIL5kZzVZTglMdKsEFSdv3ahlwnzDv1oZSHpKV\n4p5Y789jUEbhmq4AhiyalDaAPtI5q2pyKpBAy5B+btU0G/o3FUmQ0TFQaXG0cUdR70vUVZIgbNRS\nHBpWO3mlGJF96TGgjYmpajVdh5qSmgYHgU0MDSmosEP7UPQR8uhjhO4PT6VIrbYmVR94d+wqpFFL\nPeiBJI0k25RX4Eh/ug9uP51Ziy25GDRtG5WRGHzAjsRWanG7prddA5JWUraMFO9QACFU1BInm8dB\n71NI5iYqoOKaE3Ekghcce9UhEKRb329u1XxZ4jyMHHaoHQoAx6+lAuSTjJUigCvKHRxgcA9adIxO\nABnvVqOATLvLZzximXMRiXYATxkmgYs0EEOnQSl8zMx3J6CqhVS3zfpQrZGWyD2rYihhXw/cXGwl\n2cBWbrQw2MSXajYUZA6ZqW2u4IJUa5tjcRL1UMQfrmq8pGPftimW8u2UK5AUt37UdBmjJeWcp22r\nMIC2QrnnPvVhp2RdseQF6/Ws+SBAnnRFC247gvcetXIzHLYbwWMqn5lHTHrSAu2Y1DUs29rJIzDJ\nfn5QMVBeaLd284jaMt3JAyK9L8I6JANBgbBWZ8MxzjOTXQ3tpGtnJDFbB7hh8pbA+vPsKiUmnoVa\n55JpcJtNNupmRVlkxEq9+Tz+grVt4BEhVJBl1DDC5YH2NV9Y0u8tg5yrL5hBMZyM4zisY6utvbur\ngq5O3Ksc1aakrkmzL50fnwXEcqMyMGMgrjJpfMkWJVyOg44Naep668WkIDkTOhX5myzE9Sa5uKY4\nXc3IORUKK3LT0JEkWCVkI+bvmtfTLlSTlsJWHK4kiyP9av3hSQ3Dkqn3VXqB3pThzKw4z5Xc7J4R\ncW5kgkJI6Adqt6XeiWIRzjbIDjdjINY+gS20Vx++vvLJHccZ9DWo0TW96XjIJzux2I9a86r7j5Zf\neelSnGWqNF4xK/l7tjHnPam3hjeRI3jaRY8E89PWkEm0hlG/dwVI7+1Nmt5NRMi2mI5ol3Ou7lh7\nVMJxsW07kPmPI8kyNtC/LnjLA+9SJH5h3KhLOM4PbFUrd3a5AITKDAUnP4mtLzyE52jIwwrp0aMp\ntplWIGQSMGDStlBxwAKyosQ2gGwbzLjGe9adhAkD7Nx83/WMP7lV79pBdWwWPKuCw46nuc0RtcTd\nomVq/h9lukuUjPkyrlgRjBqzbRmwjPK7kYbcjIBHrW1c3W+ESTI0iW6gqi85Hr+dUoyk0pjMUnmM\nGMceMckcH3rZybRzKCuaN/q+/wAKGFGiYq4k6Y/AVgXF79tjit4yIyp3Z67uM/hSS6biOOaVzJLt\nyUzgZ6fTirNvpaRwu8kGZgREQD97PTmnHUU3ZWRl39s91YtEY0dNwdz95l9R/Wubn8MRvfSwwORs\nYqCec9wa7nUYf9GltooxEVB3oDj5+QOe/NYuq3DQXKXkJCyMgVlAwAR1reL6GDXU5cnU9DmaNGE8\nakFsruAqy/iqORk86zCMBhijHB/A1amuy6pGoKBhufPcmoWs4Z0aR4lyO/c1diL2LMF/aXPzI6FT\n/DnDCrBhycxuJP8Ae6j61z1zokfm4t5SO/NM8nVbBBKjsy4xnOaVhm41qC5YvhuCFA/lU0Mzxktc\nQRyqp3AP1rDg8QSJ8lzERjjKituK7sb1VMcrMT94ORuBPpQMam+4lcxglACURO3NT6rI8Vw6rtii\nKriMdTx/jmnQWqCSRrdWckYQbtpGe9VLhH8zbNk+zdfzpCZCh3Y28Y9e9WTesImUKRxjcD19qiRJ\nYJBvQFSMDPelhRnztXIzkKe1MAjaVXBC7h1xVqG2NxJiDPmZzsNVd3zBlO3HUVMZWSMFcq+cq6nB\nBoANpUhW3blbHU8VeOp3YsooEKeVGTk4yce9Uommdig+d2OTVi2NjHHcm5jkEm3bHtPAalYpE63w\nnyYx5PY46GpLPU7nTHmnt5mWSQbCD0xWW8bIAYyeepWmA7k+ZixJ4XtS0Fd3Nu28Sana3W9bhZBu\n5SRAy4rTlvNI1QNJLvsZm6lBuQn+lccQ5PlnPzY7dPer8VtcSRyJGonVBksD0o0QzorXRpLpZIo2\nhmjALi5jBLA46Edqrw3T+S8FzIV8oYVXHNZdhf3FkDArMI94LBeHGPQ1ci8SNJI0d1bpOhzt8wfM\nM+45o1EXyynOGz8o+YDvU9lcOkyTJGsgjH3GOd1MgNrIm6F1UuMeXL0z7EUrm1SDBinhcA/NG+5S\naLgkRS3ccz8psG7O3HA9qnE/lqGVi3GD349qzZEuonRpUyjcqe/40RXQt52YYI6EH1p3JaNKKZFJ\nUM7ISeNnINaJvzJZ7WX5QmNwGGFZFvNLMwkJ45Uunb0NPt5JIrgrM27epG/bmi47FWdpIU3Lt2M3\n517jobP/AGVaMR8hjH8q8FvxLb/MpPl9sHOa918IzM/hnT3fPMY61nU1kio6I6JNj9KaynkdaRCE\nkyfuntVg46itOgipG20kOKhkGTwKuSKGfmo8KCyn8KLDTM9sxHPepI3LDrVkpHJGc9apqNjkA96h\nlrUvQj5ferQPy+9VIXGQKsggtiqWpDQ9X7Gn1Hs5zSh8HBqybDmGVqFFKtxU9JjmgRHNnAxSo5xz\nUhGRioGVl6dKTGT9aMZqsshBqdXDUXEfLkM2lP50tzYpaNaoJ4xExHmAHhWz1OccikiSadBNd3kF\nrPeuXjSVCzSEngnH3R2//VTGsoGv5p54DHZ2sXnvF5hkLHOAMn1yMgelMhkF1M+r3yjyEYYQH5Xk\nH3EX/ZHBP/168uzg24v79/Rb+rO+/Mve/wCB6v8AQekoSKaG4jPnI5VhnhSDzz3pyzySNtwNnVaq\nTQmcahdvcklG3BlON8zkYH0A/wA9Kng/1obcflHp7c130pNrl7af13OKcVF3XUn3N96T+LjAq09n\nAIlZm8sZyT7VAJVYHcOnQVDPduwKevFakAl2EkJB/dqcDPpVlHlvAxtv9Ywz+FZMgZyE6BjzVtTL\nbttjkIBXaSPSkNDYIwLgiXlAMkelXLi/WSBYkB2K3ToKqzNH5YSPIBHzE9zVbBwSFzk0Axj/AHyc\nfSoNoMhUfMx61ZEJdTz9Ce1QwxeWzEHJpbiJbYyQzoIVJwfzrotLjtbi6V5EaPafmi757fhWGySL\nGsFtt86bGX5+QfWr1lfTr5SQwt5qks8pJPA7cdAKGylqepeGtTnuZW2QOsCr8pYYAPaunEzbIhKQ\nxXOXHHWuXXUVXRUhtMtdeUpd1XBB78d+9a2ml7jSFWfEZU5cHk7aylrqWip4vDHQ5PJWJCG3qT3X\nHIrw+e58yVnZdpU8Z617tqiSy6VNpq+W0sqOIsjk+leeeLfBz2WnWt/BDskFuPtKnn5hwTSg7PUG\nrnmtzK00pZiWPqahGZHUE7Rjirn2UzRTPnBTBIJxVJAHkCrniuhGY+2kcFnJ4Hc09pBC+7H3+fpR\nKNoCA455GKmMZmVlGOnekUtio000hzu688VZh1bUIygMzbVPB71GyLu6bdo5qN48p8pOc8ihxi1q\ngjKUXoej6LqH2+zSRyFKnBOOQfWpbqB5H3IT5nVXQ81yOh3ctsxCy5B5K9K6uK5+1FWhLRSoM8V5\nFSnyTbS0PXpz54a7kBhltw0UykZ/jxzVuHMqbWAjReWz1OKnU/2kxXbliDhves8k/aQjs3yjlD/M\nVtSmnoRUTJreGYajMcDyZYwCD1LVFOkrxxzhuY4mCqCBsOeB696th1w2xeg6g84xUNt9lVm85CzD\nPHqD3P6Vez0M3sRR2t9cx3EYl2rEuGWLDY7n6UsU/wBj1CO4uATPt3koMLx05oEwRrtY3yrZRipw\npPB4FYi3FzPII3XYNxzIT1Gent/9atXotDFXuaH2p7hJHeDBcnIz054NOi1CKxleLyC88xXcC3oe\ntOt7C5lljLRbwrbTzhQOxz0qpZwXT38lzLF/q8jO3nrj9KqLsialpPlQ6yuGlF1NdA+SZSWIHOR/\nkU2W0tb2zVBuTCt1HGT/ABGpXxPaT28kgjhXccjgMadpdnFZad5t1IHcsPLYcjb/APWq4vqZSj0O\nTOlXMk0nmjy0VgvmHgA067K20skEcgliBwrgYzUmsanLeXkrIXETNlB1BxwKpojCZF2fJ2De9axv\nbUzla+g9Uyu/2/OrSsXtFU8DqvvVcI24RHnsMVZIAY7T8qjApiIDZWtzE4njXJ4z6VU1LwxbrIG0\n28Mg2g4ZSvPfFXwflxxz2qQyfvgyn7g60Ac6U1iwwcO6KewyBVmHxMXIivoc7T9/HNdCjs7Bsj6V\nTvNPtrgESxqT0LAYNFgJYtR07UI8xMA2MMgPf1ANOW1nALQYdD1x1ArBn8NlQHtZsY5wari41jTi\nN+8qOhPP61NgN8O6gRsgK57jk0x8IpORnqB71StfFUbkRXtuCO7jqK0Y7nTr4usMyleozwRS2HuV\n1ldfnViMelPcOq/vI8b8Yz/Op3tOobDBRnjuKQzGRkjlJKqu1N/YelO47aESFoDuBz34Nan22xew\njDxYboSo5U+tZclvhu4I6DPWmiN47ZnJ6EDB4xRdDS0Lkc8sG6WAhxjB4z/Otvw/rlrYRSwTQBTM\nCjvjP41zUbMSQOo7HvTzIDEWC7cnnFKyYJm3eae0t7M0N0j8B48DGV9/esq5iDRx7Ytkik75M/eq\nWz1SbT3JVUmjI2/MOlXdJg+33UiQqHYqzMv+FGwXuZiTTWeQpyGOc9av2GpzQToVZeGDbWGQfYir\nV3pCvNIY5ECKmQzN0PpWKsaqzMc7uwPRhSvdE2sdXqdzbX0KMsCQ3HrE5x9MGsgKWlwwDhOpQ4b8\nqoSSOP7wQinx/L++VgDu456mmloO5oKZQWki3DggqRjP4UyK+nUA4LMpwQeoqB9WmZv3u2TjG3GD\nxTxJazA7VKscfeOMH60XA1YZ7O4sWeaRVfoiEfxGvbPDFzbXug2wgdP3KhWCnIBr55u7d4rcZjdI\nyThsZUn616T8F7sut/ZPITtIZRWc7J3KV7Hq7gADPSpkYMgUdaZMgVBTIWIYLWzIQ6RDyfSoXfAz\nip2l2MVcZzVaf73HQ1NyhhZgMgcGo8ZcECniTjYelMJw+0HipY0i3EmWFWSmMEVDCemalLHNUtiW\nP3cYqOQYp2Aw96E+bINNgLE/HNS9qiVMH2pxOOKaEOBoIzSLTqoRAyYfOKd0qUjNMKUmhny/K8yX\nIeAozgFGEgyrKeSrDuKqyyPctuuPKijgQ+XDEu1F9SB6mrVxA0dvE6nLA5JqnKgmVHB2sCMn0I6V\nj7OF+dK7QlN25ehJJGYEtrAlgyD7RPgf8tX6A/7q/ln1zToY7LZapc273E9yFcsk23yg7FV2gdTx\nz+HXmooIr24lu5S8clwDvaNVw0uRksg6YHp6Y6802dJY/wCzYlSQXS20flqv3g5ZtvH0PX0rmlG9\nqd9dW2nbW17+iOq7+O17/wBWSJQ6RzvHJMGeOYxgZA3YYjNOuIAs5Gc59KWJbe1u4NMS0hv5ZHC3\nM0gJZnJ5CEdMev0qszlLmeNH8yJJXRGJ+8o6fWtaFbndmvO/dd+6+fcitScW7Nf5eRZFntgS4Lph\n2IwDkirCxQQxHa5aQ9Kq2qGYGM8DOanVNjFRnitzAguGklnAIXbjGFGMCkbLMsUYwoHJFWxbO67k\nBbIx8taNrp8VpEzTKRIo5B7Ur9A1Mi7i8mJI0BZjzjFVbVFUh5chc8gelWS73d6CrFgTx7CnXcfk\nKq5G49vQU0BOL97bzY4I8RODtLgEgV1nhmSNL+PCBTLFlh6A+o/WuKEitiV1zt/h9a1vD2oj+2o3\nKYyTkdfpRa40zoLnTxZeJVhujcW1k+BHIrkj8T2zzXWefBbwxzNM4tmwrOP61vLHBqViLSaHBZA4\nYqDg+2e9cjrk6WUzaY0ibuHMZ7qOlYqV3ZltaXNAo15qMV/bt/o6L5eXOGOcHAHvWd4zvoLjRrux\njlZHjj38ckY7fnVOx8UR2rut5GCmMBQckc+lR+JJY1sptRLhopcIAFA+U9RVcruTc8nMgCTDaS7g\nDOcCqojMIzgZx+dXJLd0mZSh5yVwOq+tPuraI6aJTOzXG8KqY6L61qmJmb8xIYjLCpHdFAZQd/Q0\npUAEZyMUwKSCFGDTYELMxk2k/L0zUyTAbUYA7Tk+9Oa1zb7sZwOSfWqyRFAW2kjoTS0egM2F1S3l\nto4Vt40mi+7IDgkehrVstVMKGUALjHzDkVxsmMH19a3fD9xIJRG6qySrtwelctalpzHTQrNOx2Fj\nqyTXSTwOm8Zyo71NrKCZBcxKFdQcr3rLgsIxcZUeXKT1XvVkPcRzNFOhYg8N61xxa6He9SGzd2iU\nlgvc57VbaATHaGGG5z/Sqpck42JGz9AO9SZAYRxsCy85z1rog01YxnFplueUWVlalkVxE5dowOSP\nr71l3cxi1QrGgNuxz8oDYHXGasyRP5iPs3xhMpg5+bPcVF9jdZwsjKJJGy4B4zn/AOvmrvZGVy0l\nxOkEsVvKqowJAY9OOOtVdImHlvZSZR3Qy+ax6n2+tTbLePS5JFC3Fwr+WArcpgnJIqldW01v5Znm\nBZY12gDJHoBVp2VmTKF9UV/JLzfZ2zsdgrK39Kv30UjWxsbaKMbGwSzc4A7VHHbgXiMV8ycxjAxy\nr96uXAMNuyrEss5HAPGGPTmqUtTDle7OJki82+toYm3uuc7RwKe4KsQ/BzjGeeKtXVo9hPA1zlGb\nORGc/NnrUV1F8rSpIr7mPOOc1um7mfLYjgJHz47ce9MYl8Kp+pqZ22x7lxt+6Paoim1wM9s0xC/K\nHB7DgCnxgP8ALnG41GxyF2jlqmA2OOd2MYIFMCwAI2Krz2zUWNz8nFSuQI2Xblsg7s01uTk/WkA5\n9nlDBO7ofSk2qyEH5setDKBntkUi425xjNAFOfSrO4U7ogCe61mT+GmA3WkxGeMH0rownOCPrTos\nCQ7ucdKAOVS41zRDjDPHjuNwqzF4gt5hm6jaOXPJAytdME3lQw3Bjiqd7odpePzEVbONyUrDuV1u\nbe8jUrcIwHfOCParNwAYPKCLMoHyk9R/jWJceEZ42Jhn57Bhg1U3axpUg3IzIOeeRScRqRvzJFsj\n2I4DKM57GmGEtgBh9fWsyLxGkjBZQ6AHPqK6DTU0+/hlkW+ihnXBRXPDD+lLYdtSP7BMud5Xbzjt\nmmw74Zg9vlHAwWVscd6vSXE+5YLopPEvRl/oabdRWZbNrLIy4GRKoBz3+tDYikJJ5MJyS/ylc5Br\nVttMtVkMMkjrJuGQV6A+9ZsUKs6q7x8n1xgVpxErEI5Wk3CXqvPy+9IdirPavbSMJRuiXgle1VoY\nkuJjFGx3k8A9MV0mpX2+Oe3gjQqRuDKBuJGOCO9Yi21zFa/aVjXltpRQQ2PWmIBZXNtcqZIlOCOC\nfet2ddKhHm/2e5RhsOG4HvWXJFc3MiQTo6XAG7n+Je1SxaikbJHIq3EQ4ZcFTUvUCOByPMjhuSEV\nsiOUfK3t6ZrvPhEpbXNScQLD8g3BDlT9K4i5ktruTECm33cFCc11nw91ubSddSxlt18ucbA46+1T\nNbFRfQ9qk2ldrd6okNFIDnIBq7KC0eR1FUWbrjnsa1b0Ij5liQeaQB3GRVd9yfK1LC7iQLnp0zRP\nkvuIqSkRmPdH706KPeMEc0xnMZyOlOimywxSY9y1sMQ6cU8MNuaekgf5WoaIKDirVhD4wNuaCApz\nVYSsDt7VIzErjFFxWJwPSmuuRx1pIwcdaeKpMQyPOOalpBS0xBQaKY+c8UAfMcsrQKzSH58cDHGK\nzUl3OA5CqW+atW6a3kUg7t4UbcnqRWZKwdyUTjvgVBO6JZzuO2Msrod6uDgg9iKWylnn1C7lknD6\njNb7LeVyFy3AIGeM7c/54qGRhHHtI5bk88mrVq1mYZkezEpdMIWHQ1lVoqpG73/r+mjanVcHf+n/\nAMEi8s6XGY+RqEylcK+7yIz1yR/E3P4fWqgQR7QoxjoDViJDAjKiIuTnIGTn60N+8zuA5Oc0U4NX\nct3u/ToiZSUrRjokLA7JKNnXuK14ljuXUthVB7VhrlXyDg5rVs9kssRUkEH5q0eupNz0Xw1DFEyw\nrBHIsmMjFX9f0q0MLA+VCXYgY7545qt4enjgRjsAGPvdzUyxC/1G4W4Al3D92qn7uDxz9KzkikcJ\nqmlwaTcGNTgAcFTkH8a5y4UtOXdvlr0LX1+1xSQWtpHIIEMbgdEOePyFcdJYyIhiaNSkR+dhyM04\nu5LMjzEZtrEhSeSK1NJ8s36MiEDNZxtSxJUjHoeprYtfJtNp35CAFj71oI9G07xDJa3AAdZIgBuH\nUrVP4gKxsLLXFVGK4Urt6Kema5Tw7eGK/uGlQSRytncT3rU1rWbjUbaTTvLLLIx2wgZwuOD+lZzj\nd3LUu5yk032u4F0sARccjdkL9anbV3vY/s04QwLxjsD61io8nmFWZoyR8w6ZqZnkuF8iCE7sbsAc\ngfXvWnSxJduo5JYV8tolWGNlTccZB6ism4jxCqKvQdQOtOeWVWbzd2CQSrd6I5YizpIMA8rt6A0m\ngRnFBtxUlvavczrFH1Pf0pJNm/agIGeRW/osCpbNKVyd20EcGh3KTOn0nw1pX2XyLhGlfAyzDGPp\nXFeJtL/sa+ktUkzC/wA6ZHauo+3z6eUxLvkxwCeBWZ4wMN9aWl2pXfkoQKi1ncq6tY4PaCSjEAg9\nanhu2tp4zjcinP0pbmOMMjBOq8/WqzKWU4BPpVtcysTdxdz0vSZItQtg6EB09T1qeVjJk/xCvP8A\nQtSazkCOzeWx55+7XcQS74w6klG6HrivJrUXSlc9bD1FUiUZxyUk4kXODjtTRcDCbQoAHJxzmr14\nglQc/MD3rO+zLtYMSqdQw6iqhJ2uauK2Zch/0mLEjbYXOCVPIPt6VPdoYblJ/LKkgkndwcDiqsEC\nojbJAQ3UHtWvLKl7ZwpMoxAu0N/Wt1M5HBXZhrCZbqLY6guhaUg8DJyRU8k/2rTASdxhmVcgYP1N\nKbNGYqkhY5J47+1Q3VosgJwy5+XC98CqckJx00LOz98BHIFIGQwPODVK9mmfUhao23AG4jqRWjDZ\nYtmaM4TbgHGTxVeGIXniG2fcwKqFkJGN1FO17oznFpWJvENpaCxgktZfPnhUFYyOp7/zrk7a1kGn\nu8mQwYnC1p63eT2uqSQ27Y+bbwMn86qwOYvlLsccHPpXXHU5WVViYJloyMjjNNaNzH5rDAJ25Jq3\ncyCU7Qf3Y5LexFVmH2iNYhwF+bNWQRxKHVmz06Cp4u45BIx9KZJtcgou0KOP8aFchCSNxz2oAl2j\nbgZyB3oLKQDzk0igysqpwMc/WkAMMoUMGAPIoAeecAA9KmSIHAc4JHA9aY37zJBxUmFfbjO5Rzmg\nBWJKrtB44zSqB5ZwBkjGT603dk4J4HalRBsJIxzkUwJkUgJkkHoaV8qUXogOcg9aQEmR8gEAZAzQ\nSXYKQMAUgJHYEKxJx0GaeI18sO67lY4wahwWIPZamXfIyqOAD60AZs3h2xvxJiPy3XunGRWBceFb\nuE5tpN344Ndu6GKYoNoZfmOT1FOnj27JYyQj8gntSA87S51fTP8AWxyMg5wwyK0bXxLbsx+0xGM9\niORmu2VYRZtDLGGbdurI1Dw5YXjKyxBN/wDEOKGrjTsQwX8FwmIVglz1BXkVJHdXNtLtjIjGc7Ty\nCKw7rwfdW5L2cwcg4xnBAqh9q1rTWVZw7qOAHG6lYLnSzxSS/wCkLM5O7HHY00X99ExSSd8e/ftW\nTbeKYwCkkLRlvvbTkH8K2La7ju7fdFKsqZ6A8rmkBag1JHVmvI5ZbkAbJVcjAFSRIl2WCRsHPOS/\nWqux41Ckgkjp6fWrVuoS2WSdQFDYyhwRUt2Hew+1Mnnlpv8AVxHBJHJHpXVfD60m1vxEZoohHFZf\nMzP3z0xXIz4SNow4aNj1xmut+HPifS9AubyG6JVrgqqv1qJq8UVGx7pGMxjPP1qpcRDzARwKmspV\nltUkVgwYZBHcU64iLr8vWtdHHQnZlXBBBFH8XzdDTEJB2nrTpFLKcGkhsiKhnKnv0qMr5TYqSNjt\n5GStJIRJyOtNhcEnZX5q75hkXcKzDCwG7tVmzlOdp6VOw9y1ENxyRVkAMMYpqALz60FsGtEkQx/C\n8Ubhmq+WLn5qkVecmi9wsTUUgpaoQUUUUAfJbSyM27A55zmtOG6t7XTnTyt80o2h8/drJSRfIWPk\nnkgYp8kgkQbuFQcYrMQRhpJFRm+bH5VuWlrGkYHzGQmsW2UeapBBOcn2ro1EaKTz060xFdrdFHPJ\n9KpzQBQwT/8AVVwneykZwfzpwQNHtP8AEep9KQ1oZBRm5Iya1LK2ZYuDj1NVRC0s4UkIBWxCluLY\nhZSG3dMdRQhnS6RmWyKhjuz09q6n7V/Z9pbsIg5chSR0HrXAWF5IsmBtWPbjn2rZvb6VfLcMpVYw\nQuc1MldjuaN5fQFJ5IwkEb8MO59TXOWrwX168UGFh6/j706O6GsQS/uwjjnHrVfSNIuIUNxJujDs\nSqg9fahWQmN1HRLWz8uUZdnBAZeRmslrZC626OWIOSfb1rX+3ohnadXKRnaqe5rH32kUsjRyEmXq\nBzg+lUhEjXCaeC46MMKM9aWPWU/tO0v88MvlSKTwAKybtiHbeVY4wnoBVDOVdR068U9xlnXCs983\n2cnbk4Oc55qxA9lbQIYZHabZgljjYfaqtq2ybzQFbnbz0HvTL+KGKf8AdSiRGHNGwE+pRTF1mkZG\nDr/D61ksTxjj3qb7bLFhVjzDwcZ4qB5hIScY9PemAn35OTxWrY3EiARqfkQ7sGqHlnAQAb1GeO9N\njzJKVVsMPfFJMZrXWqeZ5jOo4AA56VQvLwz28ERIBQFjtqCWNYx5m4M2eeelU/MJOegzQA58OoHX\nbVZ9sbMVbg9AKszuDGiqgXbklgeWJqJId3Rdx9qNg3EgtywfeeeorrNEju1hKk5QDOCe3tWLbwne\nEbHJzjvW3aGW2ZS7DZjBX0rgxMrqx24aLTLcrurFSvTkc9aUyLJEQvykjPtmnfZmmxJEcL3U1WZd\nr5H8PUYrBNWstzva6imRS6FwAoHJFbei3kUWoQrKYjDJlHJHQVh27xmUq4wp6GtK2hRJUWToRW0H\npYxqRuro2NT0r+x7hA0geN1Dxsi9j61SuYYROEc7Y2+dEY85/wAmtca5df2cLKaESwxDh3GWArM8\nQWeXjZR5ZdQRnpjGafs1fmRiptKzKf25rZESFDsPyklehqg86ieBw/zHhsDGCDT7tZI7cOp/dBtx\nKjJrOnO1TN83zOSCvTHaqjaMtCZO6LOs2qJcRynJ3DcD6A9KyVUWlrOzAmVgVjDDr71Y1GWaRIHe\nRnOwJgnoR6VmfaZTcK3TGQC3Oa64HJNFiKJZ7OQueeBjuf8ACki+S35Ug7dhqaRUtrWPOXQMTxwD\n6VXSTMOzdwy5P1qkzMaxXdjAAHT3pkZAQnFDtmQAYIHSpFXZkjGT2qwEjcgITwc54p6Izs0mOB3p\npG3ORwO9PhkdYjgADOcetACk4BxgHPP0qy7B3OCAyru4/iFMEYaNWYgEnJpioSGfjC8c9ce1IB2d\n7AjqRyaeqM+SAdvc1CmSxiHQnPvViJHLbQcL2zQA0KS2ABycDmrbW0kMQkYHY4wOO9RQzC3+ZArS\nKcjIqyNQn2schgTuCt0zSAqoG8sgA++antiOGboOc1AZjJlmOC3UCpEZljKL0bhqYD2UmXcD6YzV\njzCbcADcqHDKKrxEN82Oh5qe3YCF9wwjHgetAFqOATWsjKwx3HfFQRsJFWLbyO/rUsDeUT6A5I9R\nSSqI9xVvkccY7UAJhfLIPXqDTI40cKWUF155GaRUZY40VsryW96erBpCgyRtBBFBJm3fhzTLoAvA\noYtjenFc7e+ELy1lLadMWUnDDODXeMuIkiABYHORQWAffjAJx1osM81e512wbN1HIwA2/OM9PetC\n28TWksardI0bj+Iciu+EEZBWZA4YE4bpWPdeFdO1Jt3keSx/iSpauVcSP7HqDRyW90hBHUevuKz7\nq3Nrcq3B54IHFZ9x4GvoGeTTrneFbI52kVU8/V9McJfRSOgI5cZH51LjoNbn1d4auBPotkwHBiX+\nVbRrhvhvfm+8J2sgTaoyq13I6damhtZjmtShOpSQOBx3qRdpfPappUDDB6GmBAqYNVbUSZTnAWTe\ng4p8duZE3imuu3PORRHPj5QcLTAnKgR7cVEqAdBzTwxZuDxVhYwFotcYQ9ADUjgEUgXK+9RCTBIb\ntTWhIixktT95jYA1F9pUP1qZisoBFNWB3JsjGaM96QD5QDVW4lKZ9KoRb3A0ZqhFcDHJ5PSraOGH\nvSTHY+UIyZ7hXIXyUwvP8NaosoGVreFgRwA4HU1kxwFpEyDtxnAq+hkRfmLYJ6dMVm9SSBIjbzB2\nT7vT/aNalvNHINzBjk5xmsq7laR0GMKgwKtpIIlQhcOcADtVIDVVYhGXJ+bsvpSGGSa0JJVSOh9K\ngSMyMpJC5OMDrVlsIzwRseeSfSkxFZDHJH5X/LbODJ2pzDsuNw4zVWSKSNiM7SaQB2kVQTjGeO1I\ndzSEuy225BPUn2qdtQItVJ7ACswSBYjFn5zwKjmyBHCpLH+tCA6jSWjtRmEfvgwyP7oqS5uLnVZ2\ntYuIkUBmUY2n1+tV7K5t4rqPdCSrKPNwe2KuXV1poEsNuXihVSxK9S3bNTJ62KS0ucjqBdrprVH+\nWPKjcfvHuazPNZYckgAkjPfNW7yaJg02TvPygepPeo4Eha2mj2r9oAJ+bvVokzzKSgySafDzKUC5\nJFQKHPbpUsrmM5X8aYyw4aGAxkDccE47VRcFgz8YHApVkZ2ZFBYtxUcZPIbg5xzTSAI4xLJsZiEz\nzinLDJDII5h8pGQSKeSYQrqT97Oe9T3N079WMjcYBGNopMCJl8qMygglWxioSFWXJbbnrVkAyRkl\nhllrPkOM55A4xQA13yxGeKhY84PanKdynAwaYygHHX3phce0gIHy/jUluHW5QZABOVqOaNUAI+bI\nrV8PWjXt9GGTeE5X0/H2rOpKyLirs6Ky0JmgubiQ+XJHAZUB5LCm2yxXCx5Pl8DI65qHUtaaW8MM\nU/mKcqSBgfT9Kr2ayYaRmxxwa82cZWbZ3QnZ2R0kQCQYjXcnYe9Z1xHI0bSJlWBwc9aRLqXapXkq\neMdxU00yzQsWkww+8K4/evd7ncmmtDJVHEinPfJx1q/FceXK2JCyPjBbqKzpd5OIic9vUUsLE3Co\n5dtw6j1rsV3uYyklozqVu02xBeCOuOpqxqU6XVuPMnfdsB57EdawrOSPzQzEjZ2z1NWJFyrKSST8\nwPY+1axbWhnKKvdFOW9kjs9iyq6Z6H2NZ2oXUogJXAx0Xtj2pJ8mVwvCEfdHrUTgXEDI56HINUlZ\nil8JFLdLcWsIAAy2SB2rYuNGjbSftEQ27XBXJ7msVkjaOJdgBBIJA4IPeuoiE9rZC2KpNCyBkKtW\njnyvQ5lT5tzlHd/sk0THbtfdj36cGqkk2IowCQeSffNbd5YvNd7AhijkAVI2H3T6Vz06TRs8T8Oj\nbTW8JJmE4tFnftVWbn6elWAdzYRSRjis+N90ZVhtwe9XbSV083OPlXgVqZA8mPlAyAf1p6t5Y2kc\nk4qFCrkMT3zir1tD9pLpn94OQPWgdhQU8vYeW7U1kO0+o6VXJZJGwcD3q3IwEQXB3DrQAqSiA7kU\nA4B+tOk8yZlaFc5XkrznJqPClB71IJSscaIeR2HFAEY+U4Iyen0pw+UMh5NMJzKxLDIJyaeuMq+C\nRQA4HyghIBBNWFCsDyBnriq6kdWHXpVhAEjbKnLHg0wFjICMpwRnj3qRmyFGQpAwFFNjjUklmwFG\namjlAV0VAVYYyeopAMaZh0OCOKuWhkuIPIeRFXJYHuTVNYVjkLAZHHX1qe3domYjDDqBjpSAfIPL\nOwkBe59D7U07UgSUADL4IHUilkfzkL7eAeRSxqA2xs7TyKYi1GzcoVypBZCRj8KbAFe6IIDbeevW\nmmYiPbjO05DHtTrdFUBiVyy9enNJjRYeBt28FiM4AoROWwR+7HzY7VJJLKbcKH+cHG33qO3XIEoA\ndgCHQfxGgGJudd68Mj8Ae9SSRQyRgXEW5FbBVucikjMOcxKwiPJJ6r7VHf743LRt1GfXFJq407M9\nR8FpFbeHYbeOIooJZR2x7V1kRDKMVzfhG+ivfDdo8R+6NvTpW9ASvU1nS0RUiyRkVFImUPrUoOaU\njNbbkFDyt/HeqEkLwTZYZUmtrywpyKjeISAhvwqWikyug4DCrUbB1qEIRGfamxyhRkHmkgZYLCI8\n9KpXL/vAynipHnWQYzzVUxyeZkj5TSkxpEc6lRvHSrmnyeYMZ6UeUDHg1HGPs8uVFLrcd7qxpk4q\nCaNZBz0pDcBgMU0TK5wOlac1yShKjI+TUsFxggE4PrVp4w4waoywmM+1SM+d1iWO3aSUcsMAD+VU\njcBZMNnnvUt3ceY/HCj7o9vWqQBlf/ZFKKMyQ7/MLk/KenFTxBpWErEkg9KgZWYKqZzV+1j23CsT\nnaOQaLiNGIlFEjgBv4VqeJlVpACQzjOcVHZJ9ruHZgcAZPoBUs8oG9EZTjHI7Ck2UZWoq0V4NxyW\nUc5pLd3kEgBAQck96iv5TLLtQ7v9qlD/AOjiKNfmY4OKFsIs2qmWZmI6D5T60ke6G4aaUhQBxmnB\nXhxhh0wMdqp3k4LBXyzHjFHMBZj1ueI7iFKDgjHWn2txHPdCFyzK6liM8L7msZWUyH5uAcBat2eo\nR2+nXFuI18yU7fMxkhepp26jv0GzqkXlkPu2ZwSO1Zq3BWbdk5PJNT3Mh8tTu3EjJFUTknIJHaqQ\njVETBpLjpHs3qR0yeAKpSZAIHNX7e4lg0prXYp848M3UCs85jkbJI28dKBiSLJbyJgEMV3UbovIe\nSRh5ueAPWp7OFrxpWm3lFBVOeSev5VSe3UEEHdzj60AS7zJEDhgOxNIMuSM5z3NPICRYwM+lVs7Z\nFznGeaYFq5uVARlQJt+X5aqyurtvHUippJAoJH3T2qpvKttK5z39KAEB2qFA4z1olUY3e/T1pGcY\nBHIU808csrSE4HOBSYIf9nkl09GbgtJwB1ArZtLttLTZbYD7DvzWbZSsGkmxlcYCntV7RbRdR1JU\nkYiMEMxz+lZtX0LWmpoaRoHnZlu2MZYfKoq5qNuunMF42FfkAqWRpIIHmZwqhvl56is/VJlura1n\nPBRivPesKlJyNadSw8OBb7mfYSalt5IpoWj3KM/xHvWfFH55LOSfRRWjY6a8swLL5cYBINcVVRj6\nnXSlLmIJVNrMswQuqnaBms65umkJYZTjgKa6ieMSwZkOB06frWZd6XC8QZMkr3XuKVLExtaSOidJ\nvVEFtcqYkxne3XHer8dwxXaBlQe/asFZXtkaLqDyPUVqafcZDbgD3ye9dMm+W6MopbMdfRxi7JOQ\nDycVnR7UJbD7N3HHFbl1AstsZI+S/wB3msEIy3AiMuwHqD0NEW2tdxTTRJ5TG2acldhfCgHkVbsr\nt0UwjlF5X2qizqAUPyjP8NOhkMLsADJGQcA8VUkRHXU3dQvmne3uwEVPutzyW9ao6jpwuLc3Fuu7\ncCXAHKtWKk+yX52YqeVB/hNXbbWPsM6SB3ww+YY4NVG5nUSMYgxbgwHFRwyhXO7qRXS3mqaPdhPt\nVoFVs5kQ4K1x90Y1uh5MuUB4J7iuuDucbWpejlwSB1NaNrdGFxKv+s6VipIhV2BIYdB61bhcvsI9\n6skt7/NkLO3U5qbzQTucHniqRJRgQPl6ZqQO3Tjnp70AW1IRcJksPWpeQdxA3MPyqoJPn+U9Bgg1\nP5y4yw6CgB6rk7cAnHJNShYyyq0hX5cqAM1Dk4UgdehqeKRC5XB3EDOO+KAJRFlssPlX1qSRMv8A\nu2LJ6elVFyHIAO09jVhWwRhsIBxQBYhMQUZB3k46dBUW5llKDBHTNRK7BhliQTnFSSnBx0JoC4yF\n2muBG8nlRrzuxnNXtgUuwGY+x9ahjQCMxueCORT3Z0gMQO7HTPYGgRI0YVcgfNwRmkmLLEGC85wa\nlhjjZY5NwYnIYehp+5BJG5GAx249KBoZFGzbGc546EZzVi6mNzckR2whBXjb0GKSREa43xMYwvJN\nAZ3hEf38NkEdcUkDHqGktihPzjv3NS25KOij7w7etMiIZNyt81ETHzUc5yCRj0psRd2RwvJDtAHX\niql/bODKqyFSqg//AFqmlZllWSVRt4AYd/rT7xkkmjRmBYDOT39jSXcGjQ+H+pz2niFrFpCYJUys\nfQBq9aDBj6V4No7yW3iazuMFQZcN7CvdgcxK4781ltKxfQtL0FSVDE+RU1axJCmOuRx1p9FUBUdy\nqspHJFZkj4Q4yGBrXmjJINU5YBnPrWcrlxKUQLnk1owSqV2N2qmqDzNozgUE7ZSoqCjRKKwyDUNy\nEAyOtRW8jKxBqz5QZS3c9Kvcm1ikXGNo696RWKn2qOWFonzQr7v8Kh6DL8UoYYNSMoYYNZ4YryOl\nWops8GqTuSfLLsXbaOp61KqhF2imxptGT941ZgiLtnHHanJ9DMltoTnJ61ZWL96SoJz1pwXYAo6m\nrSp5abB99vvH0rJspIeJNkP2a3OBnLse/tUDR/Z1ZDIEycsR2p888diEJQvK/RB2HrSW8f2m5WSR\nSSTuWHPC/U/WkjRQS3GQ6YJrcyRBtobHzDmraaUtvaea6lpGbIxxgV0NpAHjy3LY5UdqqF5GeUSo\nUhXKgsMUXadhJJ6nL3dxFHEwRTnoDWRdcZLOrMAOM1p61arEA8RJUnmufuZfNYMkYjBG3AOa0jqQ\nS27BVyrc+/rQlyUlwFDYGORVZSVGD9OKCQGz09KtCLMjhl3jr0we9MdVVgVOeM/jTfldzng4pVRy\nhY/dHSgZNdHcIgJCV25PsaYGbKjIOfXt7mqzuQwHYVPLiK0iQA7pPmb3oGTJeYDB9uxRtG3+dCSo\njuBCDH/D7GqbhBJtwQpI4PenZMcnl5b5TwO1AmSP87eg6UGHK7z90dfegsEUHGWPNQvIxGCePSmF\nyGTJJJYgdhUSyO+IU5JFPdsgk5pLAzG7AhGZH6cdBSb0H1Jvs7wrskXk84oUbjhhj+ldHe6ZJOsL\nySfMRxxxXPsreYyKuXBxiohK6KnHl1QjvsiATgfzqex1NrGUyQ4V26t3H0qG7t/K8rc4Z2TcQvQU\nkCADlcknAzVLQW5pXGpzX0KW7c7RwQeKg89lgSGfDEHPXpUCvGJQOFA6npUEocylgc56Z9KmTbBa\nHRae6+cAg3Ackg1vQPuMcrq3JIAzwVFclpr+WpRW6nk+tb63KQwtkncBhfT3ry8VTfNY9CjNI1ZU\nV/lQMGxx6VVSZbceVICG9T0pbO5SNVM0y7mHX0qa6SG6i5wVPAYHpXnSjKDs9j0YVFJGFq9oPMWX\njB5yOoqglw0B2hsqOlbckXHkysHU9KxruzktbsOnzZHAr0sPPTlkctaOpq2cvSRiG4xjPSoteuER\nFtoLSSa63grtThkxnOahsCvIJGQeQa2mlkngEki7AigL6itrpMzabRyqy+bcJbupWfdtCN13e9XY\nwVmXJG4HaQB0qC90lLy68ws0c5ICyHjaR3qgs9xYThLrdKN3yTrnDe59K0a5loZRk4uzNNwC0scy\n4jI3IVXIVh/Ssq7kn2PIkfQ/pWpFfiRCwYbWbBXqKhZMLM0boWAztPBx7URvF6oHFSWjMaFmuISj\n25bn5pFPIFVZ4Cj8DAPI3dxVt52gLDO0t1xVe7neZFZ1AZehHpXTF9jjkrbkaEIRg7s9cCrHmGNy\ncFeOhFUYLlopxJFlSOKuPcPcEF2GT61oZl63lhaNyzuHzlBnjNL5oOR079aoq4RMEY+lWlvfPgS2\n+zoWXOJB97FAE0EyGYmRuBxV+J0Tcr4ZGHBrOa4CgARBgIwpYDqfenRkOCUOMfw0DNFHWIcEkD+V\nPVXjdmBzxkVnibLDI9jVgyFMDG4MOD6UxF6N96sH4PalDlUDDBjzzVMNhQB1qdXYgAgYPUUCJzw4\nJOO61K7o5zxu7iqwkDbRggg4qRmAnAKjNAFgsdoxgjtVmFGNq8zqSmdo9zWeMqMK20npmrYmuDaR\nrIMRDJHNADo90ZLKcrnFWwUmHynhh90+tVwpKBSMFuR705sQ4EZ5/iHofagCQFoiA/AHY+lPSRxd\nrbqASeQ1QvI28nO4MPSpFy+wBiDnqBQBMQkUW0MS5flTU0GV3AqWX360zbtODgnPBNOEilC4IPpS\nYF1JYhG8ko3Y+6DRpLWt480+3KnIIJ+4fSq0yfLbFPmO7Mg9qjsWSxv54SwME/OR2NKwyC4m8jU7\neYZAEvIr3LTpVmsY2DZyoNeCa1buPkZz8pGwj6969j8L3JbSLUE5bYAc1nO6krFJHQxuAcVZU8VX\ndR99R1qSFt2c9RWiYmTUUlLViCo3jVxg1JSFgDigChLEEyBVYxYcOR9a0p4fMwQcEUxI/wCEisnF\n3LTK4RXQsoxikhujv2ycGrDJ5fygfLUE0AkGR1ot2FcsOiyrWbPbtE2RU0M7RNsk6etXSFlWnuO5\nko+eDwakBxyKdcWpQ5WoVfnB4NQ1YZ83ohdsdu9aUSCNM457VHbwhVyeg/WrUSeYxZuEXrSbMkh8\nKbF81hkn7oqUultC1xNyB0H94+lKi+YxZvlUDP8Auis+WX7bcBwpMEZ2xIP4jUG0VbUfaW897d5J\n/wBIl5Lf8809a6mOGCztSI4wAF2qT1PvUWnWIs4CJDmVuZW/9lHsKju7je3HAHSmtCJyEj1SeFsx\n4VjxzUk2tw3cLQTSAMowCRjJrAvbvy1aQHBAwvuapxSK8e9QOvzE9Sa1SUlqQpEusyslnLHIhDcB\nSB1rlhtBAPJHFdncj7bpMyyNwi5BbsR6Vxef4h9KIqzGTSFmVVGAi9MCmbixX5RjoKazbU27vlzn\n3p9uC8gIUk9hinYRNFAu4g5JxyfeiWTcoQDGOK6B/DV02lC6idSSfu5wScZ4rncCJJd6nf0x6UJp\njK5wTtPOTiruoqY7mHj5FQY/Co7aGKa2mlncjaVVVXv61Hdy+bdFlGB0QUD2B1NzOxLDAG4n0qWE\nR7PmYEjoe9VpCVUbRz3NQggYYtxnmmSWj8zEhuBUWUZ+T8vUihpFcHygcAVHyoP0oAZK6DOzIHvX\nQ+HrNniVoWUu2S5I5AHasOK2NzIkS8s5wPauoRG061azif59nzSj9QKiTfQ1pq4T6iXKh3CiJc5H\nTHrWDBLHdzSSEiPcc/UUa3dKoS2hkBBQFyP5VmK5SHzN2wggD1ohGyHUkm9C/PbrDMwjO5c5Umnx\nGNYJX3YYL8ufWqqXskj73XchGCfWlcoVIGNvYVVkZu4qkG2AxlgeTUchLORgjjnmpsgQkDjjpULL\nv4B+fvRYS1L9hh3UZI21Nqd3IjDYenArNS6EMOxVwR1NWoIzdowJBcr39axlFN3lsbRn0RW+0yNK\n4dn6Zbmr1nrb6cpD4khfjaTytSzaHfpbGaSMKqj5vX61jMvJiblQOtS4U6keW2hXtJRd0dYtzBeW\nzOkh4wV9qseYJITFL97oD61ytlfQ2k6bFfyWG2RWOcH1rceVVKhZQ6tyrCuKdLkdjvhVVRXe4bUi\nl+YsCDwVq8LzzQcq54yTnvVNImkZ9rhmznB4qSNHZMbsMxweetaQlpqFrk8xEsikg+WeAc9DRPE0\neneSMMs4xhl6YqZFYxgSoMjlafk38EbLuQDII6YxVKVnoRZbM5aXRZ4VU2N0I4zk7HI6gfpmokv5\nrZSmoWuyPGPNRSR9a6EW5meVFjDEjJJHTHcVJcO0VpGsxUl1JAK9MdiK6OfmMOVxehzdxBDcw+dF\ntYtyGByMVlSW8tsQ0gLo3BIroLvQFkmU28j2szjLqvTPpj9az47O4tpXjubkSLHIVKgZyO5qou2x\nnNX3MeaALIzW/KgZ680xJgTtcnirFzqdtcT/AOrMOPlDAcMKjZHjLP8AK/Ymtkc7VmO88EYzx70R\nsyvuQn1BpkNu0uWDDKn7uOtNdJIWJQ7gOopjsXop2RWHXeetXbWWDzcS7gpHUetU7WG3vRtjYwyq\nmTvPBNNeG5DhNmSpxlelArF/ersdhJxzTo5SOMnHaqccssA3SRlQxKgkdx71MJd+0jGD6UxWLolB\nAqRJD2PI5wT1rOdmOABwO9SkFV+Uhv8AaoCxqPPlt6pgDnAqyF8yIzxsGB688isuJ2dwqtg4496W\nGfyy2AQD1waANJXAbaR05B96smRWUq7DkE8DoazVcMFwSMnAPoakExDYbGSMZpMRoRytlWYFlU9M\n9KkEm8BgMd8eoqlHNsYuh/OpzKCcZwSODTAu71O2RVI8v5jjvUqv5km5TkNyFqmm5cZG6NuM+lTI\nrwtuH3OitQBckclwwwGHJGe1SSBYbgKgGxuQoqsW+Vlxnj5mBqVCcrICCAMgd6QFuMtbSeYOeKhN\nsbqMmEAA/MAepPpVhHj/AHc28ksDtXr81KJMeZdIQhUgFR05/lTQmUdfkNxHA6xBDGNr+9eieD5J\nP7IgBJPyjk15/eSrcRbWyoGTXYeD9RWfS49o27Ds+tZ1U9GXFnosLB48ZyRU6gBsjisy0b5gQeDW\ngdwGRzjtRFjZOKWmKwZcinVqSLSEAilooAaue9GM804jIpoJB5pAIQCMGq7oUOR0q11ppHY0NCKU\nsSyr71Xjle3fa3K1eeMqcr0qJ0WVcEVDRSZKrLKnrVO5tcfMopgL2r+q1ejlWVPWjcq584qpdgi1\nZC5xGgyo/U01E8pNo++3U+gpLmf7HAAgzPJwg9B61g2EYkV9Nvb7FEw2jmZ/6VqaLYYC3TKRxiBS\nOg7saoaVp3nyEPkxRnMzf32/u/410k0ghj7BiOg7ChDlKxFdTBV8tTwOtYt1PgEZx6mp7mfAz3PS\nud1G73ExKcj+I+p9KqKuYN3K91cNczYX7v8AD/jUaB5Jo4Ic5Ldv1NMY+Wh/vHritG0tGjHlcieU\nZkP9xfT61t8KKihtzdPHY3USgmN3wr+vrWC7rkYHA7+ta2tXinFhbnCR/fYdz6VU0yC1DST3shKR\nLuWMdXbtUruNldIwyeYzYBNTwtGjKyuQA2DjqRVRiXlLYA3H7vYVMylWHPA5wKaEemeGb6zvI7dZ\nzl0bIXOCpxjP0rjvGYFnrt1CbRYXLblMR+VlPQ1mQ6hLaEmFmQsuG+lU9Qu3u5AZJmcqMAk9vSly\nrm5kVzEulKrXBMmWGOEPc9qkvrVtPSOSTHmvklPQVStJGjulboM8+uKt65cxT3iCIkqF+Zm7mhpq\nQ9HEqQ3BjicYDeYMHPaoAxLAgA47EcZqxEUMYjIA96ayJlynQdasgIy8ZY9AwI9OtJlVKleo5JNM\nllZlXBBwO1NVlC5YMR7UAWILx7a489Rk1HJdyuztvKlumT0FV92FBycY5FIWLHcBkelA9bWECbm+\nY4I5JqVYw7NwSvGBSDrnHBHQVatwsYMx+UJyPrS2QIl3RwR+TLBvTB3Luwc9jTIUH3AQ6t93FVGL\nSyZf5mJyfpVy2yjBQpYnnjtUWsO99BDA6nBXn3qPHU9xV9WSRWU53AfNmqUp2gqo79apMVrFaQFp\nsr0rf0OFolExXLE5APpWCqnf1rorV/KtR+9UDbjB70SSa1GnY159SF1by2rFvmQ/SuH5EhTPzEYJ\nFak93DCVKk7wOQTmsdFd3LrnJP4UoxUVoVe7J4MOVwQR34rQg2RgRuChz8p6iqUMexjv2gg9qupJ\nEVVQ2Seuaxqam9N8pNHOfMyWbeOorTiuIJsHIRh39azzCk1u0kWfNQcgd6jgkdFB2gkjOGFY8ql6\nnSp2OkBbygqYDcdev4U6F2d2JZRCq/MO+awraeaeRYlYlhyAPSr8ThRsIO4nnNHIDmrlm1mCoIxt\nKyOdxxyAailtWa7cOrMQoBUnse9QwHBPHyh+QOtaV3dRq6yPGN5jwpB6/WtIroDV9UZCZn1ZpAD5\nkbLtUn8OfwqXVbe0bVJURSPNQ7Qh+UtjFORlhFxMgwWXaTjv7VUfekqSKcn5duR2PXilFu/MRNJq\nxy99pAtTJuIJV8ADtVEW7liInZOMtnpXaajbiS2kkdGMgYM4x1ArAkSKGF1jwwc7ge4HYV1QldHF\nUjyszI7u5s5Czxgj1WriXEd8mEUE98daQJvhBIG8nIqGS1RzuUlG6HbxV2ITsSyQJFC5dTn+Fv8A\nGk07U5bGQv8AfXGNh6VEv2uEEHE8WehPNMzDO52nyn/uPxSHcmmvZWEqjJjdt23sD7UkE7g+9QFn\njUxsPlFPUHKlWGTTA0YpNxIJGOx9DTVneGVt3CHoRyKpF2QncSDT4HBfLgtx0pAaMV5tA3qrDnDd\nCKkSRQCTlgfX0rPRxKWTYVbOFGOBVwWtzt3hCwj6j0obAn+1hFPOVxnBp8Ny0i7+MZyBjmqyShCW\nwvTBzUKzzRlQwG3tjtTuJo2vNAQkE5I5FT+dzGoGVJxtI61lJcfIGOOe4qdbgbVV/u9QAcEH60Cs\nb1tIWcQBipU/dfoR6U6V33OsblTnlD0rJju5hGu855wGPX8aljutxyxG7vmjcZqxSTLvkMRKMMHF\nSJcIpA3FCOB9KpW96QpiRyAeqmpDIeUZcnHcc0rAawmie5YJyv8ADj1qUyMFuABhXABB9qw45Ruj\naNjkHkCr89wpuCyMSjDOD2NNCH30rNDkhdj8ZHatnwfdCJLiEEYVgcVgvKjwjzASjAjK9Qa1PCIR\n55VUk8YJIqaj0HE9X0u58yFAO1bMcmSRXJaPMck84U4rqIWBGe5rOLs7Fssxgoxz0NSAg81GjB1K\n9xTN+3g8YrS9iCxnmlqMEGng+tUmAtIRmlooAbyppetL1pvK0AFQyRfxLU+c0mMUNXEU2UOMMKqF\nXtn3LytaUkWeV61CQCNrVDVykz//2Q==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview one of the augmented data produced\n",
    "\n",
    "# from IPython.display import Image\n",
    "# Image('preview/rust_0_1980.jpeg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in the Xception model to use as the base net for transfer learning\n",
    "# The Xception model is selected as it is high accuracy for its small size.\n",
    "\n",
    "model = applications.Xception(weights = \"imagenet\", include_top=False, input_shape = (img_width, img_height, 3))\n",
    "\n",
    "# model = applications.MobileNet(weights = \"imagenet\", include_top=False, input_shape = (img_width, img_height, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 111, 111, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_bn (BatchNormaliza (None, 111, 111, 32) 128         block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_act (Activation)   (None, 111, 111, 32) 0           block1_conv1_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 109, 109, 64) 18432       block1_conv1_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_bn (BatchNormaliza (None, 109, 109, 64) 256         block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_act (Activation)   (None, 109, 109, 64) 0           block1_conv2_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv1 (SeparableConv2 (None, 109, 109, 128 8768        block1_conv2_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv1_bn (BatchNormal (None, 109, 109, 128 512         block2_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2_act (Activation (None, 109, 109, 128 0           block2_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2 (SeparableConv2 (None, 109, 109, 128 17536       block2_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2_bn (BatchNormal (None, 109, 109, 128 512         block2_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 55, 55, 128)  8192        block1_conv2_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 55, 55, 128)  0           block2_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 55, 55, 128)  512         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 55, 55, 128)  0           block2_pool[0][0]                \n",
      "                                                                 batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1_act (Activation (None, 55, 55, 128)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1 (SeparableConv2 (None, 55, 55, 256)  33920       block3_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1_bn (BatchNormal (None, 55, 55, 256)  1024        block3_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2_act (Activation (None, 55, 55, 256)  0           block3_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2 (SeparableConv2 (None, 55, 55, 256)  67840       block3_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2_bn (BatchNormal (None, 55, 55, 256)  1024        block3_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 28, 28, 256)  32768       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 28, 28, 256)  0           block3_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 28, 28, 256)  1024        conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 28, 28, 256)  0           block3_pool[0][0]                \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1_act (Activation (None, 28, 28, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1 (SeparableConv2 (None, 28, 28, 728)  188672      block4_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1_bn (BatchNormal (None, 28, 28, 728)  2912        block4_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2_act (Activation (None, 28, 28, 728)  0           block4_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2 (SeparableConv2 (None, 28, 28, 728)  536536      block4_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2_bn (BatchNormal (None, 28, 28, 728)  2912        block4_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 14, 14, 728)  186368      add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, 14, 14, 728)  0           block4_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 14, 14, 728)  2912        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 14, 14, 728)  0           block4_pool[0][0]                \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1_act (Activation (None, 14, 14, 728)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1 (SeparableConv2 (None, 14, 14, 728)  536536      block5_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1_bn (BatchNormal (None, 14, 14, 728)  2912        block5_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2_act (Activation (None, 14, 14, 728)  0           block5_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2 (SeparableConv2 (None, 14, 14, 728)  536536      block5_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2_bn (BatchNormal (None, 14, 14, 728)  2912        block5_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3_act (Activation (None, 14, 14, 728)  0           block5_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3 (SeparableConv2 (None, 14, 14, 728)  536536      block5_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3_bn (BatchNormal (None, 14, 14, 728)  2912        block5_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 14, 14, 728)  0           block5_sepconv3_bn[0][0]         \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1_act (Activation (None, 14, 14, 728)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1 (SeparableConv2 (None, 14, 14, 728)  536536      block6_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1_bn (BatchNormal (None, 14, 14, 728)  2912        block6_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2_act (Activation (None, 14, 14, 728)  0           block6_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2 (SeparableConv2 (None, 14, 14, 728)  536536      block6_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2_bn (BatchNormal (None, 14, 14, 728)  2912        block6_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3_act (Activation (None, 14, 14, 728)  0           block6_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3 (SeparableConv2 (None, 14, 14, 728)  536536      block6_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3_bn (BatchNormal (None, 14, 14, 728)  2912        block6_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 14, 14, 728)  0           block6_sepconv3_bn[0][0]         \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1_act (Activation (None, 14, 14, 728)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1 (SeparableConv2 (None, 14, 14, 728)  536536      block7_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1_bn (BatchNormal (None, 14, 14, 728)  2912        block7_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2_act (Activation (None, 14, 14, 728)  0           block7_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2 (SeparableConv2 (None, 14, 14, 728)  536536      block7_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2_bn (BatchNormal (None, 14, 14, 728)  2912        block7_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3_act (Activation (None, 14, 14, 728)  0           block7_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3 (SeparableConv2 (None, 14, 14, 728)  536536      block7_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3_bn (BatchNormal (None, 14, 14, 728)  2912        block7_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 14, 14, 728)  0           block7_sepconv3_bn[0][0]         \n",
      "                                                                 add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1_act (Activation (None, 14, 14, 728)  0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1 (SeparableConv2 (None, 14, 14, 728)  536536      block8_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1_bn (BatchNormal (None, 14, 14, 728)  2912        block8_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2_act (Activation (None, 14, 14, 728)  0           block8_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2 (SeparableConv2 (None, 14, 14, 728)  536536      block8_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2_bn (BatchNormal (None, 14, 14, 728)  2912        block8_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3_act (Activation (None, 14, 14, 728)  0           block8_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3 (SeparableConv2 (None, 14, 14, 728)  536536      block8_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3_bn (BatchNormal (None, 14, 14, 728)  2912        block8_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 14, 14, 728)  0           block8_sepconv3_bn[0][0]         \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1_act (Activation (None, 14, 14, 728)  0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1 (SeparableConv2 (None, 14, 14, 728)  536536      block9_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1_bn (BatchNormal (None, 14, 14, 728)  2912        block9_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2_act (Activation (None, 14, 14, 728)  0           block9_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2 (SeparableConv2 (None, 14, 14, 728)  536536      block9_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2_bn (BatchNormal (None, 14, 14, 728)  2912        block9_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3_act (Activation (None, 14, 14, 728)  0           block9_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3 (SeparableConv2 (None, 14, 14, 728)  536536      block9_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3_bn (BatchNormal (None, 14, 14, 728)  2912        block9_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 14, 14, 728)  0           block9_sepconv3_bn[0][0]         \n",
      "                                                                 add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1_act (Activatio (None, 14, 14, 728)  0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1 (SeparableConv (None, 14, 14, 728)  536536      block10_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1_bn (BatchNorma (None, 14, 14, 728)  2912        block10_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2_act (Activatio (None, 14, 14, 728)  0           block10_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2 (SeparableConv (None, 14, 14, 728)  536536      block10_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2_bn (BatchNorma (None, 14, 14, 728)  2912        block10_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3_act (Activatio (None, 14, 14, 728)  0           block10_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3 (SeparableConv (None, 14, 14, 728)  536536      block10_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3_bn (BatchNorma (None, 14, 14, 728)  2912        block10_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 14, 14, 728)  0           block10_sepconv3_bn[0][0]        \n",
      "                                                                 add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1_act (Activatio (None, 14, 14, 728)  0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1 (SeparableConv (None, 14, 14, 728)  536536      block11_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1_bn (BatchNorma (None, 14, 14, 728)  2912        block11_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2_act (Activatio (None, 14, 14, 728)  0           block11_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2 (SeparableConv (None, 14, 14, 728)  536536      block11_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2_bn (BatchNorma (None, 14, 14, 728)  2912        block11_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3_act (Activatio (None, 14, 14, 728)  0           block11_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3 (SeparableConv (None, 14, 14, 728)  536536      block11_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3_bn (BatchNorma (None, 14, 14, 728)  2912        block11_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 14, 14, 728)  0           block11_sepconv3_bn[0][0]        \n",
      "                                                                 add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1_act (Activatio (None, 14, 14, 728)  0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1 (SeparableConv (None, 14, 14, 728)  536536      block12_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1_bn (BatchNorma (None, 14, 14, 728)  2912        block12_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2_act (Activatio (None, 14, 14, 728)  0           block12_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2 (SeparableConv (None, 14, 14, 728)  536536      block12_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2_bn (BatchNorma (None, 14, 14, 728)  2912        block12_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3_act (Activatio (None, 14, 14, 728)  0           block12_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3 (SeparableConv (None, 14, 14, 728)  536536      block12_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3_bn (BatchNorma (None, 14, 14, 728)  2912        block12_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 14, 14, 728)  0           block12_sepconv3_bn[0][0]        \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1_act (Activatio (None, 14, 14, 728)  0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1 (SeparableConv (None, 14, 14, 728)  536536      block13_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1_bn (BatchNorma (None, 14, 14, 728)  2912        block13_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2_act (Activatio (None, 14, 14, 728)  0           block13_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2 (SeparableConv (None, 14, 14, 1024) 752024      block13_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2_bn (BatchNorma (None, 14, 14, 1024) 4096        block13_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 7, 7, 1024)   745472      add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_pool (MaxPooling2D)     (None, 7, 7, 1024)   0           block13_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 7, 7, 1024)   4096        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 7, 7, 1024)   0           block13_pool[0][0]               \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1 (SeparableConv (None, 7, 7, 1536)   1582080     add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1_bn (BatchNorma (None, 7, 7, 1536)   6144        block14_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1_act (Activatio (None, 7, 7, 1536)   0           block14_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2 (SeparableConv (None, 7, 7, 2048)   3159552     block14_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2_bn (BatchNorma (None, 7, 7, 2048)   8192        block14_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2_act (Activatio (None, 7, 7, 2048)   0           block14_sepconv2_bn[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 20,861,480\n",
      "Trainable params: 20,806,952\n",
      "Non-trainable params: 54,528\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# View the model summary to see the layers of the network\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 0  Name: input_1\n",
      "layer: 1  Name: block1_conv1\n",
      "layer: 2  Name: block1_conv1_bn\n",
      "layer: 3  Name: block1_conv1_act\n",
      "layer: 4  Name: block1_conv2\n",
      "layer: 5  Name: block1_conv2_bn\n",
      "layer: 6  Name: block1_conv2_act\n",
      "layer: 7  Name: block2_sepconv1\n",
      "layer: 8  Name: block2_sepconv1_bn\n",
      "layer: 9  Name: block2_sepconv2_act\n",
      "layer: 10  Name: block2_sepconv2\n",
      "layer: 11  Name: block2_sepconv2_bn\n",
      "layer: 12  Name: conv2d_1\n",
      "layer: 13  Name: block2_pool\n",
      "layer: 14  Name: batch_normalization_1\n",
      "layer: 15  Name: add_1\n",
      "layer: 16  Name: block3_sepconv1_act\n",
      "layer: 17  Name: block3_sepconv1\n",
      "layer: 18  Name: block3_sepconv1_bn\n",
      "layer: 19  Name: block3_sepconv2_act\n",
      "layer: 20  Name: block3_sepconv2\n",
      "layer: 21  Name: block3_sepconv2_bn\n",
      "layer: 22  Name: conv2d_2\n",
      "layer: 23  Name: block3_pool\n",
      "layer: 24  Name: batch_normalization_2\n",
      "layer: 25  Name: add_2\n",
      "layer: 26  Name: block4_sepconv1_act\n",
      "layer: 27  Name: block4_sepconv1\n",
      "layer: 28  Name: block4_sepconv1_bn\n",
      "layer: 29  Name: block4_sepconv2_act\n",
      "layer: 30  Name: block4_sepconv2\n",
      "layer: 31  Name: block4_sepconv2_bn\n",
      "layer: 32  Name: conv2d_3\n",
      "layer: 33  Name: block4_pool\n",
      "layer: 34  Name: batch_normalization_3\n",
      "layer: 35  Name: add_3\n",
      "layer: 36  Name: block5_sepconv1_act\n",
      "layer: 37  Name: block5_sepconv1\n",
      "layer: 38  Name: block5_sepconv1_bn\n",
      "layer: 39  Name: block5_sepconv2_act\n",
      "layer: 40  Name: block5_sepconv2\n",
      "layer: 41  Name: block5_sepconv2_bn\n",
      "layer: 42  Name: block5_sepconv3_act\n",
      "layer: 43  Name: block5_sepconv3\n",
      "layer: 44  Name: block5_sepconv3_bn\n",
      "layer: 45  Name: add_4\n",
      "layer: 46  Name: block6_sepconv1_act\n",
      "layer: 47  Name: block6_sepconv1\n",
      "layer: 48  Name: block6_sepconv1_bn\n",
      "layer: 49  Name: block6_sepconv2_act\n",
      "layer: 50  Name: block6_sepconv2\n",
      "layer: 51  Name: block6_sepconv2_bn\n",
      "layer: 52  Name: block6_sepconv3_act\n",
      "layer: 53  Name: block6_sepconv3\n",
      "layer: 54  Name: block6_sepconv3_bn\n",
      "layer: 55  Name: add_5\n",
      "layer: 56  Name: block7_sepconv1_act\n",
      "layer: 57  Name: block7_sepconv1\n",
      "layer: 58  Name: block7_sepconv1_bn\n",
      "layer: 59  Name: block7_sepconv2_act\n",
      "layer: 60  Name: block7_sepconv2\n",
      "layer: 61  Name: block7_sepconv2_bn\n",
      "layer: 62  Name: block7_sepconv3_act\n",
      "layer: 63  Name: block7_sepconv3\n",
      "layer: 64  Name: block7_sepconv3_bn\n",
      "layer: 65  Name: add_6\n",
      "layer: 66  Name: block8_sepconv1_act\n",
      "layer: 67  Name: block8_sepconv1\n",
      "layer: 68  Name: block8_sepconv1_bn\n",
      "layer: 69  Name: block8_sepconv2_act\n",
      "layer: 70  Name: block8_sepconv2\n",
      "layer: 71  Name: block8_sepconv2_bn\n",
      "layer: 72  Name: block8_sepconv3_act\n",
      "layer: 73  Name: block8_sepconv3\n",
      "layer: 74  Name: block8_sepconv3_bn\n",
      "layer: 75  Name: add_7\n",
      "layer: 76  Name: block9_sepconv1_act\n",
      "layer: 77  Name: block9_sepconv1\n",
      "layer: 78  Name: block9_sepconv1_bn\n",
      "layer: 79  Name: block9_sepconv2_act\n",
      "layer: 80  Name: block9_sepconv2\n",
      "layer: 81  Name: block9_sepconv2_bn\n",
      "layer: 82  Name: block9_sepconv3_act\n",
      "layer: 83  Name: block9_sepconv3\n",
      "layer: 84  Name: block9_sepconv3_bn\n",
      "layer: 85  Name: add_8\n",
      "layer: 86  Name: block10_sepconv1_act\n",
      "layer: 87  Name: block10_sepconv1\n",
      "layer: 88  Name: block10_sepconv1_bn\n",
      "layer: 89  Name: block10_sepconv2_act\n",
      "layer: 90  Name: block10_sepconv2\n",
      "layer: 91  Name: block10_sepconv2_bn\n",
      "layer: 92  Name: block10_sepconv3_act\n",
      "layer: 93  Name: block10_sepconv3\n",
      "layer: 94  Name: block10_sepconv3_bn\n",
      "layer: 95  Name: add_9\n",
      "layer: 96  Name: block11_sepconv1_act\n",
      "layer: 97  Name: block11_sepconv1\n",
      "layer: 98  Name: block11_sepconv1_bn\n",
      "layer: 99  Name: block11_sepconv2_act\n",
      "layer: 100  Name: block11_sepconv2\n",
      "layer: 101  Name: block11_sepconv2_bn\n",
      "layer: 102  Name: block11_sepconv3_act\n",
      "layer: 103  Name: block11_sepconv3\n",
      "layer: 104  Name: block11_sepconv3_bn\n",
      "layer: 105  Name: add_10\n",
      "layer: 106  Name: block12_sepconv1_act\n",
      "layer: 107  Name: block12_sepconv1\n",
      "layer: 108  Name: block12_sepconv1_bn\n",
      "layer: 109  Name: block12_sepconv2_act\n",
      "layer: 110  Name: block12_sepconv2\n",
      "layer: 111  Name: block12_sepconv2_bn\n",
      "layer: 112  Name: block12_sepconv3_act\n",
      "layer: 113  Name: block12_sepconv3\n",
      "layer: 114  Name: block12_sepconv3_bn\n",
      "layer: 115  Name: add_11\n",
      "layer: 116  Name: block13_sepconv1_act\n",
      "layer: 117  Name: block13_sepconv1\n",
      "layer: 118  Name: block13_sepconv1_bn\n",
      "layer: 119  Name: block13_sepconv2_act\n",
      "layer: 120  Name: block13_sepconv2\n",
      "layer: 121  Name: block13_sepconv2_bn\n",
      "layer: 122  Name: conv2d_4\n",
      "layer: 123  Name: block13_pool\n",
      "layer: 124  Name: batch_normalization_4\n",
      "layer: 125  Name: add_12\n",
      "layer: 126  Name: block14_sepconv1\n",
      "layer: 127  Name: block14_sepconv1_bn\n",
      "layer: 128  Name: block14_sepconv1_act\n",
      "layer: 129  Name: block14_sepconv2\n",
      "layer: 130  Name: block14_sepconv2_bn\n",
      "layer: 131  Name: block14_sepconv2_act\n"
     ]
    }
   ],
   "source": [
    "# Check the layer names to decide which layers to train and freeze for later in the process\n",
    "\n",
    "for i, layer in enumerate(model.layers):\n",
    "    print('layer:', i, ' Name:', layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1538 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# There are 2778 no-rust photos\n",
    "# There are 2762 rust photos\n",
    "\n",
    "generator = datagen.flow_from_directory(\n",
    "        'data/train',\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,  # this means our generator will only yield batches of data, no labels\n",
    "        shuffle=False)  # our data will be in order, so all first 2778 images will be no rust, then 2762 rust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-fc1fe422cbcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# the predict_generator method returns the output of a model, given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# a generator that yields batches of numpy data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbottleneck_features_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5540\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# save the output as a Numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bottleneck_features_train.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbottleneck_features_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict_generator\u001b[0;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m   1528\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1530\u001b[0;31m             verbose=verbose)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mpredict_generator\u001b[0;34m(model, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m    433\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m             \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1279\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_predict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1281\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# the predict_generator method returns the output of a model, given\n",
    "# a generator that yields batches of numpy data\n",
    "bottleneck_features_train = model.predict_generator(generator, 5540)\n",
    "# save the output as a Numpy array\n",
    "np.save(open('bottleneck_features_train.npy', 'wb'), bottleneck_features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marsilea\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5540 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Run the python file created to get bottleneck features\n",
    "\n",
    "# %run main_run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly we will freeze all the layers in the network as not trainable and train a classifier on top of the network\n",
    "for layer in model.layers[:117]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "for layer in model.layers[117:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding custom Layers \n",
    "\n",
    "x = model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation=\"relu\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(512, activation=\"relu\")(x)\n",
    "predictions = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_final = Model(model.input, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 111, 111, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_bn (BatchNormaliza (None, 111, 111, 32) 128         block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_act (Activation)   (None, 111, 111, 32) 0           block1_conv1_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 109, 109, 64) 18432       block1_conv1_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_bn (BatchNormaliza (None, 109, 109, 64) 256         block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_act (Activation)   (None, 109, 109, 64) 0           block1_conv2_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv1 (SeparableConv2 (None, 109, 109, 128 8768        block1_conv2_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv1_bn (BatchNormal (None, 109, 109, 128 512         block2_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2_act (Activation (None, 109, 109, 128 0           block2_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2 (SeparableConv2 (None, 109, 109, 128 17536       block2_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2_bn (BatchNormal (None, 109, 109, 128 512         block2_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 55, 55, 128)  8192        block1_conv2_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 55, 55, 128)  0           block2_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 55, 55, 128)  512         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 55, 55, 128)  0           block2_pool[0][0]                \n",
      "                                                                 batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1_act (Activation (None, 55, 55, 128)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1 (SeparableConv2 (None, 55, 55, 256)  33920       block3_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1_bn (BatchNormal (None, 55, 55, 256)  1024        block3_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2_act (Activation (None, 55, 55, 256)  0           block3_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2 (SeparableConv2 (None, 55, 55, 256)  67840       block3_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2_bn (BatchNormal (None, 55, 55, 256)  1024        block3_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 28, 28, 256)  32768       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 28, 28, 256)  0           block3_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 28, 28, 256)  1024        conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 28, 28, 256)  0           block3_pool[0][0]                \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1_act (Activation (None, 28, 28, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1 (SeparableConv2 (None, 28, 28, 728)  188672      block4_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1_bn (BatchNormal (None, 28, 28, 728)  2912        block4_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2_act (Activation (None, 28, 28, 728)  0           block4_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2 (SeparableConv2 (None, 28, 28, 728)  536536      block4_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2_bn (BatchNormal (None, 28, 28, 728)  2912        block4_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 14, 14, 728)  186368      add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, 14, 14, 728)  0           block4_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 14, 14, 728)  2912        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 14, 14, 728)  0           block4_pool[0][0]                \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1_act (Activation (None, 14, 14, 728)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1 (SeparableConv2 (None, 14, 14, 728)  536536      block5_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1_bn (BatchNormal (None, 14, 14, 728)  2912        block5_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2_act (Activation (None, 14, 14, 728)  0           block5_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2 (SeparableConv2 (None, 14, 14, 728)  536536      block5_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2_bn (BatchNormal (None, 14, 14, 728)  2912        block5_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3_act (Activation (None, 14, 14, 728)  0           block5_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3 (SeparableConv2 (None, 14, 14, 728)  536536      block5_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3_bn (BatchNormal (None, 14, 14, 728)  2912        block5_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 14, 14, 728)  0           block5_sepconv3_bn[0][0]         \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1_act (Activation (None, 14, 14, 728)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1 (SeparableConv2 (None, 14, 14, 728)  536536      block6_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1_bn (BatchNormal (None, 14, 14, 728)  2912        block6_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2_act (Activation (None, 14, 14, 728)  0           block6_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2 (SeparableConv2 (None, 14, 14, 728)  536536      block6_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2_bn (BatchNormal (None, 14, 14, 728)  2912        block6_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3_act (Activation (None, 14, 14, 728)  0           block6_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3 (SeparableConv2 (None, 14, 14, 728)  536536      block6_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3_bn (BatchNormal (None, 14, 14, 728)  2912        block6_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 14, 14, 728)  0           block6_sepconv3_bn[0][0]         \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1_act (Activation (None, 14, 14, 728)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1 (SeparableConv2 (None, 14, 14, 728)  536536      block7_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1_bn (BatchNormal (None, 14, 14, 728)  2912        block7_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2_act (Activation (None, 14, 14, 728)  0           block7_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2 (SeparableConv2 (None, 14, 14, 728)  536536      block7_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2_bn (BatchNormal (None, 14, 14, 728)  2912        block7_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3_act (Activation (None, 14, 14, 728)  0           block7_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3 (SeparableConv2 (None, 14, 14, 728)  536536      block7_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3_bn (BatchNormal (None, 14, 14, 728)  2912        block7_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 14, 14, 728)  0           block7_sepconv3_bn[0][0]         \n",
      "                                                                 add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1_act (Activation (None, 14, 14, 728)  0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1 (SeparableConv2 (None, 14, 14, 728)  536536      block8_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1_bn (BatchNormal (None, 14, 14, 728)  2912        block8_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2_act (Activation (None, 14, 14, 728)  0           block8_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2 (SeparableConv2 (None, 14, 14, 728)  536536      block8_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2_bn (BatchNormal (None, 14, 14, 728)  2912        block8_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3_act (Activation (None, 14, 14, 728)  0           block8_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3 (SeparableConv2 (None, 14, 14, 728)  536536      block8_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3_bn (BatchNormal (None, 14, 14, 728)  2912        block8_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 14, 14, 728)  0           block8_sepconv3_bn[0][0]         \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1_act (Activation (None, 14, 14, 728)  0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1 (SeparableConv2 (None, 14, 14, 728)  536536      block9_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1_bn (BatchNormal (None, 14, 14, 728)  2912        block9_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2_act (Activation (None, 14, 14, 728)  0           block9_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2 (SeparableConv2 (None, 14, 14, 728)  536536      block9_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2_bn (BatchNormal (None, 14, 14, 728)  2912        block9_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3_act (Activation (None, 14, 14, 728)  0           block9_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3 (SeparableConv2 (None, 14, 14, 728)  536536      block9_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3_bn (BatchNormal (None, 14, 14, 728)  2912        block9_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 14, 14, 728)  0           block9_sepconv3_bn[0][0]         \n",
      "                                                                 add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1_act (Activatio (None, 14, 14, 728)  0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1 (SeparableConv (None, 14, 14, 728)  536536      block10_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1_bn (BatchNorma (None, 14, 14, 728)  2912        block10_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2_act (Activatio (None, 14, 14, 728)  0           block10_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2 (SeparableConv (None, 14, 14, 728)  536536      block10_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2_bn (BatchNorma (None, 14, 14, 728)  2912        block10_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3_act (Activatio (None, 14, 14, 728)  0           block10_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3 (SeparableConv (None, 14, 14, 728)  536536      block10_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3_bn (BatchNorma (None, 14, 14, 728)  2912        block10_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 14, 14, 728)  0           block10_sepconv3_bn[0][0]        \n",
      "                                                                 add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1_act (Activatio (None, 14, 14, 728)  0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1 (SeparableConv (None, 14, 14, 728)  536536      block11_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1_bn (BatchNorma (None, 14, 14, 728)  2912        block11_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2_act (Activatio (None, 14, 14, 728)  0           block11_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2 (SeparableConv (None, 14, 14, 728)  536536      block11_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2_bn (BatchNorma (None, 14, 14, 728)  2912        block11_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3_act (Activatio (None, 14, 14, 728)  0           block11_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3 (SeparableConv (None, 14, 14, 728)  536536      block11_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3_bn (BatchNorma (None, 14, 14, 728)  2912        block11_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 14, 14, 728)  0           block11_sepconv3_bn[0][0]        \n",
      "                                                                 add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1_act (Activatio (None, 14, 14, 728)  0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1 (SeparableConv (None, 14, 14, 728)  536536      block12_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1_bn (BatchNorma (None, 14, 14, 728)  2912        block12_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2_act (Activatio (None, 14, 14, 728)  0           block12_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2 (SeparableConv (None, 14, 14, 728)  536536      block12_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2_bn (BatchNorma (None, 14, 14, 728)  2912        block12_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3_act (Activatio (None, 14, 14, 728)  0           block12_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3 (SeparableConv (None, 14, 14, 728)  536536      block12_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3_bn (BatchNorma (None, 14, 14, 728)  2912        block12_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 14, 14, 728)  0           block12_sepconv3_bn[0][0]        \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1_act (Activatio (None, 14, 14, 728)  0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1 (SeparableConv (None, 14, 14, 728)  536536      block13_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1_bn (BatchNorma (None, 14, 14, 728)  2912        block13_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2_act (Activatio (None, 14, 14, 728)  0           block13_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2 (SeparableConv (None, 14, 14, 1024) 752024      block13_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2_bn (BatchNorma (None, 14, 14, 1024) 4096        block13_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 7, 7, 1024)   745472      add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_pool (MaxPooling2D)     (None, 7, 7, 1024)   0           block13_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 7, 7, 1024)   4096        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 7, 7, 1024)   0           block13_pool[0][0]               \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1 (SeparableConv (None, 7, 7, 1536)   1582080     add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1_bn (BatchNorma (None, 7, 7, 1536)   6144        block14_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1_act (Activatio (None, 7, 7, 1536)   0           block14_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2 (SeparableConv (None, 7, 7, 2048)   3159552     block14_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2_bn (BatchNorma (None, 7, 7, 2048)   8192        block14_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2_act (Activatio (None, 7, 7, 2048)   0           block14_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 100352)       0           block14_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          25690368    flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 256)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 512)          131584      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            513         dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 46,683,945\n",
      "Trainable params: 32,610,849\n",
      "Non-trainable params: 14,073,096\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Check the new combined network with the Xception network and classifier network on top\n",
    "\n",
    "model_final.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.convolutional.SeparableConv2D object at 0x7f19ee37cd30>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f19ee2e7358>\n",
      "<keras.layers.core.Activation object at 0x7f19ee2c4550>\n",
      "<keras.layers.convolutional.SeparableConv2D object at 0x7f19ee27ae80>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f19ee252320>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f19ee49e828>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f19ee1b60f0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f19ee406a90>\n",
      "<keras.layers.merge.Add object at 0x7f19ee1daa20>\n",
      "<keras.layers.convolutional.SeparableConv2D object at 0x7f19ee134198>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f19ee0acf98>\n",
      "<keras.layers.core.Activation object at 0x7f19ee0b1358>\n",
      "<keras.layers.convolutional.SeparableConv2D object at 0x7f19ee0c6b38>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f19ee0a0e80>\n",
      "<keras.layers.core.Activation object at 0x7f19edfbbc88>\n",
      "<keras.layers.core.Flatten object at 0x7f19d42722e8>\n",
      "<keras.layers.core.Dense object at 0x7f19d4272e48>\n",
      "<keras.layers.core.Dropout object at 0x7f19d4272ef0>\n",
      "<keras.layers.core.Dense object at 0x7f19d4272828>\n",
      "<keras.layers.core.Dense object at 0x7f19d4272320>\n"
     ]
    }
   ],
   "source": [
    "# Check only the classifier layers are trainable initially\n",
    "for layer in model_final.layers:\n",
    "    if layer.trainable:\n",
    "        print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model \n",
    "model_final.compile(optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the train and test generators with data Augumentation \n",
    "train_datagen = ImageDataGenerator(\n",
    "rescale = 1./255,\n",
    "horizontal_flip = True,\n",
    "fill_mode = \"nearest\",\n",
    "zoom_range = 0.3,\n",
    "width_shift_range = 0.3,\n",
    "height_shift_range=0.3,\n",
    "rotation_range=30)\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "rescale = 1./255,\n",
    "horizontal_flip = True,\n",
    "fill_mode = \"nearest\",\n",
    "zoom_range = 0.3,\n",
    "width_shift_range = 0.3,\n",
    "height_shift_range=0.3,\n",
    "rotation_range=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1538 images belonging to 2 classes.\n",
      "Found 384 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "train_data_dir,\n",
    "target_size = (img_height, img_width),\n",
    "batch_size = batch_size, \n",
    "class_mode = \"binary\")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "validation_data_dir,\n",
    "target_size = (img_height, img_width),\n",
    "class_mode = \"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model according to the conditions  \n",
    "checkpoint = ModelCheckpoint(\"xception_1.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "193/192 [==============================] - 66s 344ms/step - loss: 0.6731 - acc: 0.5842 - val_loss: 0.5965 - val_acc: 0.6914\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.69141, saving model to xception_1.h5\n",
      "Epoch 2/50\n",
      "193/192 [==============================] - 64s 332ms/step - loss: 0.6011 - acc: 0.7001 - val_loss: 0.5788 - val_acc: 0.6764\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.69141\n",
      "Epoch 3/50\n",
      "193/192 [==============================] - 64s 332ms/step - loss: 0.5551 - acc: 0.7247 - val_loss: 0.5867 - val_acc: 0.6823\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.69141\n",
      "Epoch 4/50\n",
      "193/192 [==============================] - 64s 331ms/step - loss: 0.5397 - acc: 0.7260 - val_loss: 0.4940 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.69141 to 0.76237, saving model to xception_1.h5\n",
      "Epoch 5/50\n",
      "193/192 [==============================] - 64s 333ms/step - loss: 0.5109 - acc: 0.7552 - val_loss: 0.6339 - val_acc: 0.6803\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.76237\n",
      "Epoch 6/50\n",
      "193/192 [==============================] - 64s 330ms/step - loss: 0.4880 - acc: 0.7707 - val_loss: 0.5187 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.76237\n",
      "Epoch 7/50\n",
      "193/192 [==============================] - 64s 332ms/step - loss: 0.4573 - acc: 0.7863 - val_loss: 0.5784 - val_acc: 0.7272\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.76237\n",
      "Epoch 8/50\n",
      "193/192 [==============================] - 64s 332ms/step - loss: 0.4625 - acc: 0.7895 - val_loss: 0.5373 - val_acc: 0.7604\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.76237\n",
      "Epoch 9/50\n",
      "193/192 [==============================] - 64s 333ms/step - loss: 0.4458 - acc: 0.7914 - val_loss: 0.5669 - val_acc: 0.7591\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.76237\n",
      "Epoch 10/50\n",
      "193/192 [==============================] - 64s 331ms/step - loss: 0.4370 - acc: 0.8038 - val_loss: 0.5109 - val_acc: 0.7754\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.76237 to 0.77539, saving model to xception_1.h5\n",
      "Epoch 11/50\n",
      "193/192 [==============================] - 64s 332ms/step - loss: 0.4183 - acc: 0.8193 - val_loss: 0.4920 - val_acc: 0.7891\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.77539 to 0.78906, saving model to xception_1.h5\n",
      "Epoch 12/50\n",
      "193/192 [==============================] - 64s 333ms/step - loss: 0.4378 - acc: 0.8070 - val_loss: 0.5621 - val_acc: 0.7487\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.78906\n",
      "Epoch 13/50\n",
      "193/192 [==============================] - 64s 331ms/step - loss: 0.4084 - acc: 0.8245 - val_loss: 0.5906 - val_acc: 0.7448\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.78906\n",
      "Epoch 14/50\n",
      "193/192 [==============================] - 64s 333ms/step - loss: 0.4194 - acc: 0.8115 - val_loss: 0.5174 - val_acc: 0.7747\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.78906\n",
      "Epoch 15/50\n",
      "193/192 [==============================] - 64s 330ms/step - loss: 0.3944 - acc: 0.8232 - val_loss: 0.6165 - val_acc: 0.7578\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.78906\n",
      "Epoch 16/50\n",
      "193/192 [==============================] - 64s 331ms/step - loss: 0.4048 - acc: 0.8154 - val_loss: 0.5382 - val_acc: 0.7728\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.78906\n",
      "Epoch 17/50\n",
      "193/192 [==============================] - 65s 335ms/step - loss: 0.3756 - acc: 0.8342 - val_loss: 0.5334 - val_acc: 0.7806\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.78906\n",
      "Epoch 18/50\n",
      "193/192 [==============================] - 64s 332ms/step - loss: 0.4064 - acc: 0.8148 - val_loss: 0.4570 - val_acc: 0.7943\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.78906 to 0.79427, saving model to xception_1.h5\n",
      "Epoch 19/50\n",
      "193/192 [==============================] - 64s 334ms/step - loss: 0.3936 - acc: 0.8174 - val_loss: 0.5928 - val_acc: 0.7617\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.79427\n",
      "Epoch 20/50\n",
      "193/192 [==============================] - 65s 334ms/step - loss: 0.3684 - acc: 0.8387 - val_loss: 0.5680 - val_acc: 0.7715\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.79427\n",
      "Epoch 21/50\n",
      "193/192 [==============================] - 64s 332ms/step - loss: 0.3846 - acc: 0.8219 - val_loss: 0.5232 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.79427\n",
      "Epoch 22/50\n",
      "193/192 [==============================] - 64s 331ms/step - loss: 0.3795 - acc: 0.8303 - val_loss: 0.5137 - val_acc: 0.7975\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.79427 to 0.79753, saving model to xception_1.h5\n",
      "Epoch 23/50\n",
      "193/192 [==============================] - 64s 334ms/step - loss: 0.3854 - acc: 0.8219 - val_loss: 0.5371 - val_acc: 0.7891\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.79753\n",
      "Epoch 24/50\n",
      "193/192 [==============================] - 64s 333ms/step - loss: 0.3412 - acc: 0.8484 - val_loss: 0.6095 - val_acc: 0.7721\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.79753\n",
      "Epoch 25/50\n",
      "193/192 [==============================] - 64s 331ms/step - loss: 0.3749 - acc: 0.8290 - val_loss: 0.5328 - val_acc: 0.7767\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.79753\n",
      "Epoch 26/50\n",
      "193/192 [==============================] - 64s 331ms/step - loss: 0.3676 - acc: 0.8420 - val_loss: 0.5268 - val_acc: 0.7767\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.79753\n",
      "Epoch 27/50\n",
      "193/192 [==============================] - 64s 331ms/step - loss: 0.3642 - acc: 0.8452 - val_loss: 0.5267 - val_acc: 0.7949\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.79753\n",
      "Epoch 28/50\n",
      "193/192 [==============================] - 64s 331ms/step - loss: 0.3641 - acc: 0.8400 - val_loss: 0.5837 - val_acc: 0.7832\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.79753\n",
      "Epoch 29/50\n",
      "193/192 [==============================] - 64s 331ms/step - loss: 0.3413 - acc: 0.8523 - val_loss: 0.4994 - val_acc: 0.8034\n",
      "\n",
      "Epoch 00029: val_acc improved from 0.79753 to 0.80339, saving model to xception_1.h5\n",
      "Epoch 30/50\n",
      "193/192 [==============================] - 64s 331ms/step - loss: 0.3422 - acc: 0.8562 - val_loss: 0.5582 - val_acc: 0.7858\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.80339\n",
      "Epoch 31/50\n",
      "193/192 [==============================] - 64s 333ms/step - loss: 0.3700 - acc: 0.8277 - val_loss: 0.4778 - val_acc: 0.8014\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.80339\n",
      "Epoch 32/50\n",
      "193/192 [==============================] - 64s 332ms/step - loss: 0.3621 - acc: 0.8446 - val_loss: 0.5199 - val_acc: 0.7910\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.80339\n",
      "Epoch 33/50\n",
      "193/192 [==============================] - 64s 332ms/step - loss: 0.3323 - acc: 0.8556 - val_loss: 0.4701 - val_acc: 0.8138\n",
      "\n",
      "Epoch 00033: val_acc improved from 0.80339 to 0.81380, saving model to xception_1.h5\n",
      "Epoch 34/50\n",
      "193/192 [==============================] - 64s 333ms/step - loss: 0.3268 - acc: 0.8672 - val_loss: 0.5952 - val_acc: 0.7839\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.81380\n",
      "Epoch 35/50\n",
      "193/192 [==============================] - 64s 334ms/step - loss: 0.3514 - acc: 0.8530 - val_loss: 0.5658 - val_acc: 0.7904\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.81380\n",
      "Epoch 36/50\n",
      "193/192 [==============================] - 64s 333ms/step - loss: 0.3304 - acc: 0.8446 - val_loss: 0.5042 - val_acc: 0.8073\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.81380\n",
      "Epoch 37/50\n",
      "193/192 [==============================] - 64s 333ms/step - loss: 0.3152 - acc: 0.8653 - val_loss: 0.5173 - val_acc: 0.8079\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.81380\n",
      "Epoch 38/50\n",
      "193/192 [==============================] - 64s 331ms/step - loss: 0.3344 - acc: 0.8517 - val_loss: 0.5283 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.81380\n",
      "Epoch 39/50\n",
      "193/192 [==============================] - 64s 331ms/step - loss: 0.3350 - acc: 0.8562 - val_loss: 0.5671 - val_acc: 0.7904\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.81380\n",
      "Epoch 40/50\n",
      "193/192 [==============================] - 64s 332ms/step - loss: 0.3455 - acc: 0.8543 - val_loss: 0.5179 - val_acc: 0.7949\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.81380\n",
      "Epoch 41/50\n",
      "193/192 [==============================] - 64s 332ms/step - loss: 0.3326 - acc: 0.8484 - val_loss: 0.5456 - val_acc: 0.7949\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.81380\n",
      "Epoch 42/50\n",
      "193/192 [==============================] - 64s 332ms/step - loss: 0.2970 - acc: 0.8666 - val_loss: 0.5503 - val_acc: 0.7969\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.81380\n",
      "Epoch 43/50\n",
      "193/192 [==============================] - 64s 332ms/step - loss: 0.3388 - acc: 0.8582 - val_loss: 0.5155 - val_acc: 0.8001\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.81380\n",
      "Epoch 00043: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f19d42102e8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model \n",
    "model_final.fit_generator(\n",
    "train_generator,\n",
    "steps_per_epoch = nb_train_samples / batch_size,\n",
    "epochs = epochs,\n",
    "validation_data = validation_generator,\n",
    "validation_steps = nb_validation_samples / batch_size,\n",
    "callbacks = [checkpoint, early])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### To do\n",
    "\n",
    "Now that the top most layers have been trained to a good accuracy. Set some of the mid and lower layers to trainable, set a low learning rate and continue training with fine tuning the mid and lower layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained model\n",
    "model_final.load_weights(\"xception_1.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.convolutional.Conv2D object at 0x7f09e91a5588>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f09e91a5390>\n",
      "<keras.layers.core.Activation object at 0x7f09e91a5dd8>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f09e91a5748>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f09e91a5e10>\n",
      "<keras.layers.core.Activation object at 0x7f09e90992e8>\n",
      "<keras.layers.convolutional.SeparableConv2D object at 0x7f09d97be198>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f09d97a7cc0>\n",
      "<keras.layers.core.Activation object at 0x7f09d97dc2e8>\n",
      "<keras.layers.convolutional.SeparableConv2D object at 0x7f09d970e320>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f09d969c6d8>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f09e90cd8d0>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f09d93dda20>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f09e90b0e80>\n",
      "<keras.layers.merge.Add object at 0x7f09d93e6828>\n",
      "<keras.layers.core.Activation object at 0x7f09d92c3828>\n",
      "<keras.layers.convolutional.SeparableConv2D object at 0x7f09d933b7f0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f09d9295710>\n",
      "<keras.layers.core.Activation object at 0x7f09d927bcc0>\n",
      "<keras.layers.convolutional.SeparableConv2D object at 0x7f09d9213fd0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f09d91f1fd0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f09d93a8e10>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f09d9213f98>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f09d9342588>\n",
      "<keras.layers.merge.Add object at 0x7f09d9168668>\n",
      "<keras.layers.core.Activation object at 0x7f09d9056320>\n",
      "<keras.layers.convolutional.SeparableConv2D object at 0x7f09d90562b0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f09d9024f28>\n",
      "<keras.layers.core.Activation object at 0x7f09d8fef748>\n",
      "<keras.layers.convolutional.SeparableConv2D object at 0x7f09d8f988d0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f09d8f09dd8>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f09d90d2828>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f09d8e84470>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f09e918b780>\n",
      "<keras.layers.merge.Add object at 0x7f09d8e91fd0>\n",
      "<keras.layers.core.Activation object at 0x7f09d8e6d6a0>\n",
      "<keras.layers.convolutional.SeparableConv2D object at 0x7f09d8e6d6d8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f09d8d877f0>\n",
      "<keras.layers.core.Activation object at 0x7f09d8e36780>\n",
      "<keras.layers.convolutional.SeparableConv2D object at 0x7f09d8de85c0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f09d8d644a8>\n",
      "<keras.layers.core.Activation object at 0x7f09d8ceae48>\n",
      "<keras.layers.convolutional.SeparableConv2D object at 0x7f09d8de8588>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f09d8c44898>\n",
      "<keras.layers.merge.Add object at 0x7f09d8c290b8>\n",
      "<keras.layers.core.Activation object at 0x7f09d8e6d208>\n",
      "<keras.layers.convolutional.SeparableConv2D object at 0x7f09d8bcd9e8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f09d8ba5a58>\n",
      "<keras.layers.core.Activation object at 0x7f09d8b1b5f8>\n",
      "<keras.layers.convolutional.SeparableConv2D object at 0x7f09d8b29da0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f09d8a97390>\n",
      "<keras.layers.core.Activation object at 0x7f09d8a1ad68>\n",
      "<keras.layers.convolutional.SeparableConv2D object at 0x7f09d8b29358>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f09d89f27b8>\n",
      "<keras.layers.merge.Add object at 0x7f09d8959048>\n",
      "<keras.layers.core.Activation object at 0x7f09d8bcda90>\n",
      "<keras.layers.convolutional.SeparableConv2D object at 0x7f09d897e908>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f09d88d6978>\n",
      "<keras.layers.core.Activation object at 0x7f09d884d518>\n",
      "<keras.layers.convolutional.SeparableConv2D object at 0x7f09d8859cc0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f09d8837e80>\n",
      "<keras.layers.core.Activation object at 0x7f09d8859278>\n",
      "<keras.layers.convolutional.SeparableConv2D object at 0x7f09d87a8908>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f09d87236d8>\n",
      "<keras.layers.merge.Add object at 0x7f09d86a2ac8>\n",
      "<keras.layers.core.Activation object at 0x7f09d897e9b0>\n",
      "<keras.layers.convolutional.SeparableConv2D object at 0x7f09d86ae828>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f09d863ef60>\n",
      "<keras.layers.core.Activation object at 0x7f09d85fe438>\n",
      "<keras.layers.convolutional.SeparableConv2D object at 0x7f09d858bf98>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f09d8566f28>\n",
      "<keras.layers.core.Activation object at 0x7f09d858bf60>\n",
      "<keras.layers.convolutional.SeparableConv2D object at 0x7f09d84d9828>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f09d84545f8>\n",
      "<keras.layers.merge.Add object at 0x7f09d8436c50>\n",
      "<keras.layers.core.Activation object at 0x7f09d86ae8d0>\n",
      "<keras.layers.convolutional.SeparableConv2D object at 0x7f09d83dc4a8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f09d83b47b8>\n",
      "<keras.layers.core.Activation object at 0x7f09d8331358>\n",
      "<keras.layers.convolutional.SeparableConv2D object at 0x7f09d833eeb8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f09d8298f60>\n",
      "<keras.layers.core.Activation object at 0x7f09d833ee80>\n",
      "<keras.layers.convolutional.SeparableConv2D object at 0x7f09d820b748>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f09d818a518>\n",
      "<keras.layers.merge.Add object at 0x7f09d810fba8>\n",
      "<keras.layers.core.Activation object at 0x7f09d83dc7f0>\n",
      "<keras.layers.convolutional.SeparableConv2D object at 0x7f09d8168b00>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f09d80e76d8>\n",
      "<keras.layers.core.Activation object at 0x7f09d8060240>\n",
      "<keras.layers.convolutional.SeparableConv2D object at 0x7f09d8060278>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f09a6f69f28>\n",
      "<keras.layers.core.Activation object at 0x7f09d806bda0>\n",
      "<keras.layers.convolutional.SeparableConv2D object at 0x7f09a6f60668>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f09a6edc438>\n",
      "<keras.layers.merge.Add object at 0x7f09a6e5fe10>\n",
      "<keras.layers.core.Activation object at 0x7f09d81682e8>\n",
      "<keras.layers.convolutional.SeparableConv2D object at 0x7f09a6e3ea20>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f09a6dba5f8>\n",
      "<keras.layers.core.Activation object at 0x7f09a6d9df98>\n",
      "<keras.layers.convolutional.SeparableConv2D object at 0x7f09a6d34198>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f09a6d17e10>\n",
      "<keras.layers.core.Activation object at 0x7f09a6c92588>\n",
      "<keras.layers.convolutional.SeparableConv2D object at 0x7f09a6ca2d68>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f09a6c0f358>\n",
      "<keras.layers.merge.Add object at 0x7f09a6b92d30>\n",
      "<keras.layers.core.Activation object at 0x7f09a6e3e438>\n",
      "<keras.layers.convolutional.SeparableConv2D object at 0x7f09a6b70940>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f09a6b229e8>\n",
      "<keras.layers.core.Activation object at 0x7f09a6acefd0>\n",
      "<keras.layers.convolutional.SeparableConv2D object at 0x7f09a6a6a0b8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f09a6a49e10>\n",
      "<keras.layers.core.Activation object at 0x7f09a69c14a8>\n",
      "<keras.layers.convolutional.SeparableConv2D object at 0x7f09a69d0240>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f09a692bf28>\n",
      "<keras.layers.merge.Add object at 0x7f09a69d0cc0>\n",
      "<keras.layers.core.Activation object at 0x7f09a689e518>\n",
      "<keras.layers.convolutional.SeparableConv2D object at 0x7f09a67ffdd8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f09a67689b0>\n",
      "<keras.layers.core.Activation object at 0x7f09a67462e8>\n",
      "<keras.layers.convolutional.SeparableConv2D object at 0x7f09a6762940>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f09a66d3978>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f09a6921278>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f09a66360f0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f09a688ab38>\n",
      "<keras.layers.merge.Add object at 0x7f09a665cb70>\n",
      "<keras.layers.convolutional.SeparableConv2D object at 0x7f09a65b4278>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f09a652ef98>\n",
      "<keras.layers.core.Activation object at 0x7f09a6535400>\n",
      "<keras.layers.convolutional.SeparableConv2D object at 0x7f09a6547f60>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f09a6522ef0>\n",
      "<keras.layers.core.Activation object at 0x7f09a643ff28>\n",
      "<keras.layers.core.Flatten object at 0x7f0989061ac8>\n",
      "<keras.layers.core.Dense object at 0x7f0988b84ac8>\n",
      "<keras.layers.core.Dropout object at 0x7f098947def0>\n",
      "<keras.layers.core.Dense object at 0x7f0988b165c0>\n",
      "<keras.layers.core.Dense object at 0x7f0988b16588>\n"
     ]
    }
   ],
   "source": [
    "# Check only the classifier layers are trainable initially\n",
    "for layer in model_final.layers:\n",
    "    if layer.trainable:\n",
    "        print(layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set more layers to trainable\n",
    "for layer in model.layers[:77]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "for layer in model.layers[77:]:\n",
    "    layer.trainable = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1538 images belonging to 2 classes.\n",
      "Found 384 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# compile the model \n",
    "model_final.compile(optimizer=optimizers.SGD(lr=1e-5, momentum=0.9),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Initiate the train and test generators with data Augumentation \n",
    "train_datagen = ImageDataGenerator(\n",
    "rescale = 1./255,\n",
    "horizontal_flip = True,\n",
    "fill_mode = \"nearest\",\n",
    "zoom_range = 0.3,\n",
    "width_shift_range = 0.3,\n",
    "height_shift_range=0.3,\n",
    "rotation_range=30)\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "rescale = 1./255,\n",
    "horizontal_flip = True,\n",
    "fill_mode = \"nearest\",\n",
    "zoom_range = 0.3,\n",
    "width_shift_range = 0.3,\n",
    "height_shift_range=0.3,\n",
    "rotation_range=30)\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "train_data_dir,\n",
    "target_size = (img_height, img_width),\n",
    "batch_size = batch_size, \n",
    "class_mode = \"binary\")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "validation_data_dir,\n",
    "target_size = (img_height, img_width),\n",
    "class_mode = \"binary\")\n",
    "\n",
    "# Save the model according to the conditions  \n",
    "checkpoint = ModelCheckpoint(\"xception_2.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=15, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "193/192 [==============================] - 76s 394ms/step - loss: 0.3172 - acc: 0.8607 - val_loss: 0.4808 - val_acc: 0.7930\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.79297, saving model to xception_2.h5\n",
      "Epoch 2/50\n",
      "193/192 [==============================] - 69s 355ms/step - loss: 0.3441 - acc: 0.8472 - val_loss: 0.4962 - val_acc: 0.7858\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.79297\n",
      "Epoch 3/50\n",
      "193/192 [==============================] - 69s 356ms/step - loss: 0.3422 - acc: 0.8575 - val_loss: 0.4700 - val_acc: 0.8145\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.79297 to 0.81445, saving model to xception_2.h5\n",
      "Epoch 4/50\n",
      "193/192 [==============================] - 69s 356ms/step - loss: 0.3301 - acc: 0.8620 - val_loss: 0.4645 - val_acc: 0.8145\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.81445\n",
      "Epoch 5/50\n",
      "193/192 [==============================] - 69s 356ms/step - loss: 0.3325 - acc: 0.8523 - val_loss: 0.4725 - val_acc: 0.8079\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.81445\n",
      "Epoch 6/50\n",
      "193/192 [==============================] - 68s 354ms/step - loss: 0.3279 - acc: 0.8601 - val_loss: 0.4528 - val_acc: 0.8138\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.81445\n",
      "Epoch 7/50\n",
      "193/192 [==============================] - 69s 355ms/step - loss: 0.3638 - acc: 0.8361 - val_loss: 0.4701 - val_acc: 0.8060\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.81445\n",
      "Epoch 8/50\n",
      "193/192 [==============================] - 69s 356ms/step - loss: 0.3463 - acc: 0.8549 - val_loss: 0.4542 - val_acc: 0.8079\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.81445\n",
      "Epoch 9/50\n",
      "193/192 [==============================] - 69s 356ms/step - loss: 0.3328 - acc: 0.8556 - val_loss: 0.4583 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.81445\n",
      "Epoch 10/50\n",
      "193/192 [==============================] - 70s 361ms/step - loss: 0.3409 - acc: 0.8536 - val_loss: 0.4540 - val_acc: 0.8099\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.81445\n",
      "Epoch 11/50\n",
      "193/192 [==============================] - 69s 356ms/step - loss: 0.3222 - acc: 0.8698 - val_loss: 0.4810 - val_acc: 0.8105\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.81445\n",
      "Epoch 12/50\n",
      "193/192 [==============================] - 69s 356ms/step - loss: 0.3219 - acc: 0.8640 - val_loss: 0.4697 - val_acc: 0.8105\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.81445\n",
      "Epoch 13/50\n",
      "193/192 [==============================] - 69s 356ms/step - loss: 0.3285 - acc: 0.8659 - val_loss: 0.4831 - val_acc: 0.7975\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.81445\n",
      "Epoch 14/50\n",
      "193/192 [==============================] - 69s 355ms/step - loss: 0.3143 - acc: 0.8653 - val_loss: 0.4728 - val_acc: 0.8092\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.81445\n",
      "Epoch 15/50\n",
      "193/192 [==============================] - 69s 356ms/step - loss: 0.3147 - acc: 0.8601 - val_loss: 0.4537 - val_acc: 0.8210\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.81445 to 0.82096, saving model to xception_2.h5\n",
      "Epoch 16/50\n",
      "193/192 [==============================] - 69s 356ms/step - loss: 0.3182 - acc: 0.8653 - val_loss: 0.4572 - val_acc: 0.8060\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.82096\n",
      "Epoch 17/50\n",
      "193/192 [==============================] - 69s 357ms/step - loss: 0.3434 - acc: 0.8510 - val_loss: 0.4965 - val_acc: 0.8021\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.82096\n",
      "Epoch 18/50\n",
      "193/192 [==============================] - 69s 356ms/step - loss: 0.3307 - acc: 0.8620 - val_loss: 0.4512 - val_acc: 0.8138\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.82096\n",
      "Epoch 19/50\n",
      "193/192 [==============================] - 69s 356ms/step - loss: 0.3378 - acc: 0.8536 - val_loss: 0.4670 - val_acc: 0.8027\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.82096\n",
      "Epoch 20/50\n",
      "193/192 [==============================] - 69s 356ms/step - loss: 0.3416 - acc: 0.8465 - val_loss: 0.4563 - val_acc: 0.8105\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.82096\n",
      "Epoch 21/50\n",
      "193/192 [==============================] - 69s 356ms/step - loss: 0.3167 - acc: 0.8718 - val_loss: 0.4352 - val_acc: 0.8229\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.82096 to 0.82292, saving model to xception_2.h5\n",
      "Epoch 22/50\n",
      "193/192 [==============================] - 69s 357ms/step - loss: 0.3482 - acc: 0.8452 - val_loss: 0.4744 - val_acc: 0.8066\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.82292\n",
      "Epoch 23/50\n",
      "193/192 [==============================] - 69s 355ms/step - loss: 0.3312 - acc: 0.8562 - val_loss: 0.4557 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.82292\n",
      "Epoch 24/50\n",
      "193/192 [==============================] - 69s 356ms/step - loss: 0.3307 - acc: 0.8575 - val_loss: 0.4864 - val_acc: 0.8040\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.82292\n",
      "Epoch 25/50\n",
      "193/192 [==============================] - 69s 356ms/step - loss: 0.3314 - acc: 0.8666 - val_loss: 0.4338 - val_acc: 0.8236\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.82292 to 0.82357, saving model to xception_2.h5\n",
      "Epoch 26/50\n",
      "193/192 [==============================] - 69s 359ms/step - loss: 0.3049 - acc: 0.8782 - val_loss: 0.4591 - val_acc: 0.8151\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.82357\n",
      "Epoch 27/50\n",
      "193/192 [==============================] - 69s 356ms/step - loss: 0.3399 - acc: 0.8588 - val_loss: 0.4778 - val_acc: 0.8053\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.82357\n",
      "Epoch 28/50\n",
      "193/192 [==============================] - 69s 356ms/step - loss: 0.3294 - acc: 0.8562 - val_loss: 0.4478 - val_acc: 0.8197\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.82357\n",
      "Epoch 29/50\n",
      "193/192 [==============================] - 69s 359ms/step - loss: 0.3058 - acc: 0.8705 - val_loss: 0.4619 - val_acc: 0.8099\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.82357\n",
      "Epoch 30/50\n",
      "193/192 [==============================] - 69s 356ms/step - loss: 0.3091 - acc: 0.8621 - val_loss: 0.4749 - val_acc: 0.7988\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.82357\n",
      "Epoch 31/50\n",
      "193/192 [==============================] - 68s 355ms/step - loss: 0.3156 - acc: 0.8595 - val_loss: 0.4615 - val_acc: 0.8164\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.82357\n",
      "Epoch 32/50\n",
      "193/192 [==============================] - 70s 362ms/step - loss: 0.3321 - acc: 0.8705 - val_loss: 0.4633 - val_acc: 0.8099\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.82357\n",
      "Epoch 33/50\n",
      "193/192 [==============================] - 68s 355ms/step - loss: 0.3175 - acc: 0.8620 - val_loss: 0.4788 - val_acc: 0.7982\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.82357\n",
      "Epoch 34/50\n",
      "193/192 [==============================] - 69s 356ms/step - loss: 0.3254 - acc: 0.8523 - val_loss: 0.4699 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.82357\n",
      "Epoch 35/50\n",
      "193/192 [==============================] - 69s 356ms/step - loss: 0.3307 - acc: 0.8569 - val_loss: 0.4574 - val_acc: 0.8092\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.82357\n",
      "Epoch 36/50\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.3389 - acc: 0.8548"
     ]
    }
   ],
   "source": [
    "# Train the model \n",
    "model_final.fit_generator(\n",
    "train_generator,\n",
    "steps_per_epoch = nb_train_samples / batch_size,\n",
    "epochs = epochs,\n",
    "validation_data = validation_generator,\n",
    "validation_steps = nb_validation_samples / batch_size,\n",
    "callbacks = [checkpoint, early])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try a mobilenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_224_tf_no_top.h5\n",
      "17227776/17225924 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Bring in the Xception model to use as the base net for transfer learning\n",
    "# The Xception model is selected as it is high accuracy for its small size.\n",
    "\n",
    "# model = applications.Xception(weights = \"imagenet\", include_top=False, input_shape = (img_width, img_height, 3))\n",
    "\n",
    "model = applications.MobileNet(weights = \"imagenet\", include_top=False, input_shape = (img_width, img_height, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding custom Layers \n",
    "\n",
    "x = model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(16, activation=\"relu\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(16, activation=\"relu\")(x)\n",
    "predictions = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_final = Model(model.input, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model \n",
    "model_final.compile(optimizer=optimizers.Adam(),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7fbc142a4470>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fbc142a42e8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fbc142a49e8>\n",
      "<keras.layers.core.Activation object at 0x7fbc142a4e80>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7fbc14221fd0>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fbc142a4b70>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fbc1423ddd8>\n",
      "<keras.layers.core.Activation object at 0x7fbc10222a90>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fbc101e0cf8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fbc101f8668>\n",
      "<keras.layers.core.Activation object at 0x7fbc1015ee48>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7fbc10112208>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fbc10174fd0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fbc10045da0>\n",
      "<keras.layers.core.Activation object at 0x7fbc100cbd68>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fbbf07dba20>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fbbf07f3358>\n",
      "<keras.layers.core.Activation object at 0x7fbbf0771d30>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7fbbf07a9860>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fbbf0755e48>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fbbf06c3a90>\n",
      "<keras.layers.core.Activation object at 0x7fbbf063fe80>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fbbf01de748>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fbbf01f4080>\n",
      "<keras.layers.core.Activation object at 0x7fbbf016e7f0>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7fbbf01ac588>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fbbf0156b70>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fbbf00c27b8>\n",
      "<keras.layers.core.Activation object at 0x7fbbf0046dd8>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fbbf0046d68>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fbbf00171d0>\n",
      "<keras.layers.core.Activation object at 0x7fbbcef455f8>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7fbc14310940>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fbbf002b588>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fbbcef1c6a0>\n",
      "<keras.layers.core.Activation object at 0x7fbbcee9cf60>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fbbcee9ccf8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fbbcedf08d0>\n",
      "<keras.layers.core.Activation object at 0x7fbbced7e630>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7fbbcedc63c8>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fbbced6a9b0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fbbced536d8>\n",
      "<keras.layers.core.Activation object at 0x7fbbcecd6fd0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fbbcecb4f98>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fbbceca2908>\n",
      "<keras.layers.core.Activation object at 0x7fbbcebb6668>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7fbbcec01400>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fbbcec209e8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fbbceb8a710>\n",
      "<keras.layers.core.Activation object at 0x7fbbceb0ddd8>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fbbceb0dd68>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fbbceae0940>\n",
      "<keras.layers.core.Activation object at 0x7fbbce9ec6a0>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7fbbcea35438>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fbbcea55a20>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fbbce9c1748>\n",
      "<keras.layers.core.Activation object at 0x7fbbce945e10>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fbbce945da0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fbbce916978>\n",
      "<keras.layers.core.Activation object at 0x7fbbce8246d8>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7fbbce86d470>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fbbce88ea58>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fbbce7fa780>\n",
      "<keras.layers.core.Activation object at 0x7fbbce77be48>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fbbce77bdd8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fbbce7509b0>\n",
      "<keras.layers.core.Activation object at 0x7fbbce6dc710>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7fbbce6a74a8>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fbbce6c6a90>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fbbce6317b8>\n",
      "<keras.layers.core.Activation object at 0x7fbbce5b4e80>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fbbce5b4e10>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fbbce586ef0>\n",
      "<keras.layers.core.Activation object at 0x7fbbce4f2748>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7fbbce55c4e0>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fbbce4fdac8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fbbce46a860>\n",
      "<keras.layers.core.Activation object at 0x7fbbce3eeeb8>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fbbce3eee48>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fbbce3bef28>\n",
      "<keras.layers.core.Activation object at 0x7fbbce34e780>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7fbbce397518>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fbbce336b00>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fbbce322828>\n",
      "<keras.layers.core.Activation object at 0x7fbbce227ef0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fbbce227e80>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fbbce1f6fd0>\n",
      "<keras.layers.core.Activation object at 0x7fbbce1877b8>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7fbbce1c5550>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fbbce170b38>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fbbce0d8d68>\n",
      "<keras.layers.core.Activation object at 0x7fbbce15a860>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fbbce02cac8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fbbce042438>\n",
      "<keras.layers.core.Activation object at 0x7fbbcdfab2b0>\n",
      "<keras.layers.core.Flatten object at 0x7fbbccd78eb8>\n",
      "<keras.layers.core.Dense object at 0x7fbbccf0a7b8>\n",
      "<keras.layers.core.Dropout object at 0x7fbbcd1a7f28>\n",
      "<keras.layers.core.Dense object at 0x7fbbccb75f98>\n",
      "<keras.layers.core.Dense object at 0x7fbbcd841a58>\n"
     ]
    }
   ],
   "source": [
    "# Check only the classifier layers are trainable initially\n",
    "for layer in model_final.layers:\n",
    "    if layer.trainable:\n",
    "        print(layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1538 images belonging to 2 classes.\n",
      "Found 384 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# compile the model \n",
    "model_final.compile(optimizer=optimizers.SGD(lr=1e-5, momentum=0.9),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Initiate the train and test generators with data Augumentation \n",
    "train_datagen = ImageDataGenerator(\n",
    "rescale = 1./255,\n",
    "horizontal_flip = True,\n",
    "fill_mode = \"nearest\",\n",
    "zoom_range = 0.3,\n",
    "width_shift_range = 0.3,\n",
    "height_shift_range=0.3,\n",
    "rotation_range=30)\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "rescale = 1./255,\n",
    "horizontal_flip = True,\n",
    "fill_mode = \"nearest\",\n",
    "zoom_range = 0.3,\n",
    "width_shift_range = 0.3,\n",
    "height_shift_range=0.3,\n",
    "rotation_range=30)\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "train_data_dir,\n",
    "target_size = (img_height, img_width),\n",
    "batch_size = batch_size, \n",
    "class_mode = \"binary\")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "validation_data_dir,\n",
    "target_size = (img_height, img_width),\n",
    "class_mode = \"binary\")\n",
    "\n",
    "# Save the model according to the conditions  \n",
    "checkpoint = ModelCheckpoint(\"mobilenet_1.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=15, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "193/192 [==============================] - 76s 394ms/step - loss: 0.8078 - acc: 0.5194 - val_loss: 0.6828 - val_acc: 0.5553\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.55534, saving model to mobilenet_1.h5\n",
      "Epoch 2/50\n",
      "193/192 [==============================] - 65s 339ms/step - loss: 0.6958 - acc: 0.5350 - val_loss: 0.6744 - val_acc: 0.5677\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.55534 to 0.56771, saving model to mobilenet_1.h5\n",
      "Epoch 3/50\n",
      "193/192 [==============================] - 67s 348ms/step - loss: 0.6963 - acc: 0.5246 - val_loss: 0.6752 - val_acc: 0.5638\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.56771\n",
      "Epoch 4/50\n",
      "193/192 [==============================] - 65s 339ms/step - loss: 0.6976 - acc: 0.5207 - val_loss: 0.6775 - val_acc: 0.5768\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.56771 to 0.57682, saving model to mobilenet_1.h5\n",
      "Epoch 5/50\n",
      "193/192 [==============================] - 65s 337ms/step - loss: 0.6858 - acc: 0.5272 - val_loss: 0.6736 - val_acc: 0.5716\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.57682\n",
      "Epoch 6/50\n",
      "193/192 [==============================] - 66s 342ms/step - loss: 0.6854 - acc: 0.5091 - val_loss: 0.6653 - val_acc: 0.5898\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.57682 to 0.58984, saving model to mobilenet_1.h5\n",
      "Epoch 7/50\n",
      "193/192 [==============================] - 66s 343ms/step - loss: 0.6748 - acc: 0.5699 - val_loss: 0.6520 - val_acc: 0.6270\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.58984 to 0.62695, saving model to mobilenet_1.h5\n",
      "Epoch 8/50\n",
      "193/192 [==============================] - 66s 341ms/step - loss: 0.6769 - acc: 0.5687 - val_loss: 0.6511 - val_acc: 0.6198\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.62695\n",
      "Epoch 9/50\n",
      "193/192 [==============================] - 66s 340ms/step - loss: 0.6711 - acc: 0.5829 - val_loss: 0.6466 - val_acc: 0.6413\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.62695 to 0.64128, saving model to mobilenet_1.h5\n",
      "Epoch 10/50\n",
      "193/192 [==============================] - 66s 341ms/step - loss: 0.6635 - acc: 0.5823 - val_loss: 0.6296 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.64128 to 0.66667, saving model to mobilenet_1.h5\n",
      "Epoch 11/50\n",
      "193/192 [==============================] - 66s 343ms/step - loss: 0.6553 - acc: 0.6179 - val_loss: 0.6120 - val_acc: 0.6999\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.66667 to 0.69987, saving model to mobilenet_1.h5\n",
      "Epoch 12/50\n",
      "193/192 [==============================] - 65s 338ms/step - loss: 0.6495 - acc: 0.6399 - val_loss: 0.5954 - val_acc: 0.7025\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.69987 to 0.70247, saving model to mobilenet_1.h5\n",
      "Epoch 13/50\n",
      "193/192 [==============================] - 66s 343ms/step - loss: 0.6439 - acc: 0.6308 - val_loss: 0.5926 - val_acc: 0.7188\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.70247 to 0.71875, saving model to mobilenet_1.h5\n",
      "Epoch 14/50\n",
      "193/192 [==============================] - 66s 343ms/step - loss: 0.6485 - acc: 0.6205 - val_loss: 0.5930 - val_acc: 0.7357\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.71875 to 0.73568, saving model to mobilenet_1.h5\n",
      "Epoch 15/50\n",
      "193/192 [==============================] - 65s 338ms/step - loss: 0.6425 - acc: 0.6431 - val_loss: 0.5790 - val_acc: 0.7402\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.73568 to 0.74023, saving model to mobilenet_1.h5\n",
      "Epoch 16/50\n",
      "193/192 [==============================] - 65s 339ms/step - loss: 0.6221 - acc: 0.6541 - val_loss: 0.5627 - val_acc: 0.7526\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.74023 to 0.75260, saving model to mobilenet_1.h5\n",
      "Epoch 17/50\n",
      "193/192 [==============================] - 66s 344ms/step - loss: 0.6169 - acc: 0.6736 - val_loss: 0.5628 - val_acc: 0.7428\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.75260\n",
      "Epoch 18/50\n",
      "193/192 [==============================] - 66s 343ms/step - loss: 0.6001 - acc: 0.6690 - val_loss: 0.5295 - val_acc: 0.7832\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.75260 to 0.78320, saving model to mobilenet_1.h5\n",
      "Epoch 19/50\n",
      "193/192 [==============================] - 65s 339ms/step - loss: 0.5981 - acc: 0.6768 - val_loss: 0.5328 - val_acc: 0.7682\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.78320\n",
      "Epoch 20/50\n",
      "193/192 [==============================] - 66s 339ms/step - loss: 0.5993 - acc: 0.6885 - val_loss: 0.5165 - val_acc: 0.7728\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.78320\n",
      "Epoch 21/50\n",
      "193/192 [==============================] - 65s 338ms/step - loss: 0.5827 - acc: 0.6943 - val_loss: 0.5183 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.78320 to 0.78711, saving model to mobilenet_1.h5\n",
      "Epoch 22/50\n",
      "193/192 [==============================] - 66s 340ms/step - loss: 0.5784 - acc: 0.6924 - val_loss: 0.5024 - val_acc: 0.7819\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.78711\n",
      "Epoch 23/50\n",
      "193/192 [==============================] - 66s 340ms/step - loss: 0.5947 - acc: 0.6768 - val_loss: 0.5004 - val_acc: 0.7949\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.78711 to 0.79492, saving model to mobilenet_1.h5\n",
      "Epoch 24/50\n",
      "193/192 [==============================] - 66s 341ms/step - loss: 0.5649 - acc: 0.7047 - val_loss: 0.4948 - val_acc: 0.7936\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.79492\n",
      "Epoch 25/50\n",
      "193/192 [==============================] - 66s 341ms/step - loss: 0.5642 - acc: 0.7053 - val_loss: 0.4889 - val_acc: 0.8060\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.79492 to 0.80599, saving model to mobilenet_1.h5\n",
      "Epoch 26/50\n",
      "193/192 [==============================] - 66s 340ms/step - loss: 0.5595 - acc: 0.7189 - val_loss: 0.4780 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.80599\n",
      "Epoch 27/50\n",
      "193/192 [==============================] - 66s 344ms/step - loss: 0.5607 - acc: 0.7144 - val_loss: 0.4787 - val_acc: 0.7975\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.80599\n",
      "Epoch 28/50\n",
      "193/192 [==============================] - 66s 342ms/step - loss: 0.5772 - acc: 0.7092 - val_loss: 0.4796 - val_acc: 0.8034\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.80599\n",
      "Epoch 29/50\n",
      "193/192 [==============================] - 66s 344ms/step - loss: 0.5435 - acc: 0.7299 - val_loss: 0.4685 - val_acc: 0.8145\n",
      "\n",
      "Epoch 00029: val_acc improved from 0.80599 to 0.81445, saving model to mobilenet_1.h5\n",
      "Epoch 30/50\n",
      "193/192 [==============================] - 66s 342ms/step - loss: 0.5621 - acc: 0.7111 - val_loss: 0.4723 - val_acc: 0.8184\n",
      "\n",
      "Epoch 00030: val_acc improved from 0.81445 to 0.81836, saving model to mobilenet_1.h5\n",
      "Epoch 31/50\n",
      "193/192 [==============================] - 66s 343ms/step - loss: 0.5461 - acc: 0.7202 - val_loss: 0.4579 - val_acc: 0.8151\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.81836\n",
      "Epoch 32/50\n",
      "193/192 [==============================] - 66s 342ms/step - loss: 0.5459 - acc: 0.7222 - val_loss: 0.4605 - val_acc: 0.8164\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.81836\n",
      "Epoch 33/50\n",
      "193/192 [==============================] - 66s 344ms/step - loss: 0.5363 - acc: 0.7267 - val_loss: 0.4523 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.81836\n",
      "Epoch 34/50\n",
      "193/192 [==============================] - 66s 342ms/step - loss: 0.5350 - acc: 0.7202 - val_loss: 0.4390 - val_acc: 0.8216\n",
      "\n",
      "Epoch 00034: val_acc improved from 0.81836 to 0.82161, saving model to mobilenet_1.h5\n",
      "Epoch 35/50\n",
      "193/192 [==============================] - 66s 341ms/step - loss: 0.5381 - acc: 0.7338 - val_loss: 0.4482 - val_acc: 0.8177\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.82161\n",
      "Epoch 36/50\n",
      "193/192 [==============================] - 67s 347ms/step - loss: 0.5298 - acc: 0.7409 - val_loss: 0.4419 - val_acc: 0.8301\n",
      "\n",
      "Epoch 00036: val_acc improved from 0.82161 to 0.83008, saving model to mobilenet_1.h5\n",
      "Epoch 37/50\n",
      "193/192 [==============================] - 66s 341ms/step - loss: 0.5277 - acc: 0.7448 - val_loss: 0.4426 - val_acc: 0.8151\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.83008\n",
      "Epoch 38/50\n",
      "193/192 [==============================] - 66s 343ms/step - loss: 0.5205 - acc: 0.7319 - val_loss: 0.4438 - val_acc: 0.8184\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.83008\n",
      "Epoch 39/50\n",
      "193/192 [==============================] - 66s 343ms/step - loss: 0.5013 - acc: 0.7565 - val_loss: 0.4259 - val_acc: 0.8210\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.83008\n",
      "Epoch 40/50\n",
      "193/192 [==============================] - 66s 341ms/step - loss: 0.4900 - acc: 0.7649 - val_loss: 0.4164 - val_acc: 0.8320\n",
      "\n",
      "Epoch 00040: val_acc improved from 0.83008 to 0.83203, saving model to mobilenet_1.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50\n",
      "193/192 [==============================] - 66s 343ms/step - loss: 0.4978 - acc: 0.7429 - val_loss: 0.4254 - val_acc: 0.8294\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.83203\n",
      "Epoch 42/50\n",
      "193/192 [==============================] - 67s 349ms/step - loss: 0.5121 - acc: 0.7545 - val_loss: 0.4259 - val_acc: 0.8197\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.83203\n",
      "Epoch 43/50\n",
      "193/192 [==============================] - 66s 339ms/step - loss: 0.5002 - acc: 0.7675 - val_loss: 0.4176 - val_acc: 0.8171\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.83203\n",
      "Epoch 44/50\n",
      "193/192 [==============================] - 66s 342ms/step - loss: 0.4860 - acc: 0.7552 - val_loss: 0.4056 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00044: val_acc improved from 0.83203 to 0.83333, saving model to mobilenet_1.h5\n",
      "Epoch 45/50\n",
      "193/192 [==============================] - 66s 342ms/step - loss: 0.4854 - acc: 0.7668 - val_loss: 0.3991 - val_acc: 0.8424\n",
      "\n",
      "Epoch 00045: val_acc improved from 0.83333 to 0.84245, saving model to mobilenet_1.h5\n",
      "Epoch 46/50\n",
      "193/192 [==============================] - 66s 342ms/step - loss: 0.5050 - acc: 0.7487 - val_loss: 0.4202 - val_acc: 0.8203\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.84245\n",
      "Epoch 47/50\n",
      "193/192 [==============================] - 66s 344ms/step - loss: 0.5137 - acc: 0.7565 - val_loss: 0.4283 - val_acc: 0.8255\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.84245\n",
      "Epoch 48/50\n",
      "193/192 [==============================] - 66s 340ms/step - loss: 0.5078 - acc: 0.7584 - val_loss: 0.4111 - val_acc: 0.8424\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.84245\n",
      "Epoch 49/50\n",
      "193/192 [==============================] - 66s 342ms/step - loss: 0.4939 - acc: 0.7727 - val_loss: 0.4013 - val_acc: 0.8392\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.84245\n",
      "Epoch 50/50\n",
      "193/192 [==============================] - 66s 342ms/step - loss: 0.4753 - acc: 0.7850 - val_loss: 0.3953 - val_acc: 0.8340\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.84245\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbbcd7cb6d8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model \n",
    "model_final.fit_generator(\n",
    "train_generator,\n",
    "steps_per_epoch = nb_train_samples / batch_size,\n",
    "epochs = epochs,\n",
    "validation_data = validation_generator,\n",
    "validation_steps = nb_validation_samples / batch_size,\n",
    "callbacks = [checkpoint, early])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the Xception model again using Adam optimiser as a change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in the Xception model to use as the base net for transfer learning\n",
    "# The Xception model is selected as it is high accuracy for its small size.\n",
    "\n",
    "model = applications.Xception(weights = \"imagenet\", include_top=False, input_shape = (img_width, img_height, 3))\n",
    "\n",
    "# model = applications.MobileNet(weights = \"imagenet\", include_top=False, input_shape = (img_width, img_height, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding custom Layers \n",
    "\n",
    "x = model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(16, activation=\"relu\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(16, activation=\"relu\")(x)\n",
    "predictions = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_final = Model(model.input, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set more layers to trainable\n",
    "for layer in model_final.layers[:132]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "for layer in model_final.layers[132:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.core.Flatten object at 0x7fbb6d956cc0>\n",
      "<keras.layers.core.Dense object at 0x7fbb6d3065c0>\n",
      "<keras.layers.core.Dropout object at 0x7fbb6d66c2e8>\n",
      "<keras.layers.core.Dense object at 0x7fbb6d978b70>\n",
      "<keras.layers.core.Dense object at 0x7fbb6c9d1a90>\n"
     ]
    }
   ],
   "source": [
    "# Check only the classifier layers are trainable initially\n",
    "for layer in model_final.layers:\n",
    "    if layer.trainable ==True:\n",
    "        print(layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1538 images belonging to 2 classes.\n",
      "Found 384 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# compile the model \n",
    "model_final.compile(optimizer=optimizers.Adam(),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Initiate the train and test generators with data Augumentation \n",
    "train_datagen = ImageDataGenerator(\n",
    "rescale = 1./255,\n",
    "horizontal_flip = True,\n",
    "fill_mode = \"nearest\",\n",
    "zoom_range = 0.3,\n",
    "width_shift_range = 0.3,\n",
    "height_shift_range=0.3,\n",
    "rotation_range=30)\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "rescale = 1./255,\n",
    "horizontal_flip = True,\n",
    "fill_mode = \"nearest\",\n",
    "zoom_range = 0.3,\n",
    "width_shift_range = 0.3,\n",
    "height_shift_range=0.3,\n",
    "rotation_range=30)\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "train_data_dir,\n",
    "target_size = (img_height, img_width),\n",
    "batch_size = batch_size, \n",
    "class_mode = \"binary\")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "validation_data_dir,\n",
    "target_size = (img_height, img_width),\n",
    "class_mode = \"binary\")\n",
    "\n",
    "# Save the model according to the conditions  \n",
    "checkpoint = ModelCheckpoint(\"xception_3.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=15, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "193/192 [==============================] - 67s 345ms/step - loss: 0.7428 - acc: 0.5006 - val_loss: 0.6820 - val_acc: 0.6986\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.69857, saving model to xception_3.h5\n",
      "Epoch 2/50\n",
      "193/192 [==============================] - 64s 330ms/step - loss: 0.6813 - acc: 0.5810 - val_loss: 0.6134 - val_acc: 0.7474\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.69857 to 0.74740, saving model to xception_3.h5\n",
      "Epoch 3/50\n",
      "193/192 [==============================] - 64s 329ms/step - loss: 0.6503 - acc: 0.5991 - val_loss: 0.5838 - val_acc: 0.7702\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.74740 to 0.77018, saving model to xception_3.h5\n",
      "Epoch 4/50\n",
      "193/192 [==============================] - 64s 330ms/step - loss: 0.6488 - acc: 0.6218 - val_loss: 0.5526 - val_acc: 0.7630\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.77018\n",
      "Epoch 5/50\n",
      "193/192 [==============================] - 64s 330ms/step - loss: 0.6471 - acc: 0.6146 - val_loss: 0.6020 - val_acc: 0.7220\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.77018\n",
      "Epoch 6/50\n",
      " 16/192 [=>............................] - ETA: 19s - loss: 0.6288 - acc: 0.6250"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-60b03057aa2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb_validation_samples\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m callbacks = [checkpoint, early])\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1424\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    190\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1218\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model \n",
    "model_final.fit_generator(\n",
    "train_generator,\n",
    "steps_per_epoch = nb_train_samples / batch_size,\n",
    "epochs = epochs,\n",
    "validation_data = validation_generator,\n",
    "validation_steps = nb_validation_samples / batch_size,\n",
    "callbacks = [checkpoint, early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set more layers to trainable\n",
    "for layer in model_final.layers[:77]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "for layer in model_final.layers[77:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/engine/training.py:478: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "193/192 [==============================] - 66s 341ms/step - loss: 0.6266 - acc: 0.6490 - val_loss: 0.5721 - val_acc: 0.7539\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.77018\n",
      "Epoch 2/50\n",
      "193/192 [==============================] - 64s 329ms/step - loss: 0.6274 - acc: 0.6373 - val_loss: 0.5727 - val_acc: 0.7467\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.77018\n",
      "Epoch 3/50\n",
      "193/192 [==============================] - 64s 330ms/step - loss: 0.6427 - acc: 0.6237 - val_loss: 0.6092 - val_acc: 0.6921\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.77018\n",
      "Epoch 4/50\n",
      "193/192 [==============================] - 64s 330ms/step - loss: 0.6102 - acc: 0.6496 - val_loss: 0.6030 - val_acc: 0.6823\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.77018\n",
      "Epoch 5/50\n",
      "193/192 [==============================] - 64s 331ms/step - loss: 0.6175 - acc: 0.6412 - val_loss: 0.5378 - val_acc: 0.7852\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.77018 to 0.78516, saving model to xception_3.h5\n",
      "Epoch 6/50\n",
      "193/192 [==============================] - 64s 333ms/step - loss: 0.6222 - acc: 0.6464 - val_loss: 0.6027 - val_acc: 0.6934\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.78516\n",
      "Epoch 7/50\n",
      "193/192 [==============================] - 64s 330ms/step - loss: 0.6197 - acc: 0.6470 - val_loss: 0.6166 - val_acc: 0.6732\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.78516\n",
      "Epoch 8/50\n",
      "193/192 [==============================] - 64s 330ms/step - loss: 0.6218 - acc: 0.6360 - val_loss: 0.5436 - val_acc: 0.7734\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.78516\n",
      "Epoch 9/50\n",
      "193/192 [==============================] - 64s 329ms/step - loss: 0.6300 - acc: 0.6218 - val_loss: 0.5268 - val_acc: 0.7812\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.78516\n",
      "Epoch 10/50\n",
      "193/192 [==============================] - 64s 329ms/step - loss: 0.6417 - acc: 0.6282 - val_loss: 0.6168 - val_acc: 0.7005\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.78516\n",
      "Epoch 11/50\n",
      "193/192 [==============================] - 64s 330ms/step - loss: 0.6333 - acc: 0.6192 - val_loss: 0.5227 - val_acc: 0.7878\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.78516 to 0.78776, saving model to xception_3.h5\n",
      "Epoch 12/50\n",
      "193/192 [==============================] - 63s 329ms/step - loss: 0.6137 - acc: 0.6457 - val_loss: 0.5373 - val_acc: 0.7962\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.78776 to 0.79622, saving model to xception_3.h5\n",
      "Epoch 13/50\n",
      "193/192 [==============================] - 64s 331ms/step - loss: 0.6018 - acc: 0.6671 - val_loss: 0.5187 - val_acc: 0.7936\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.79622\n",
      "Epoch 14/50\n",
      "193/192 [==============================] - 64s 330ms/step - loss: 0.6111 - acc: 0.6522 - val_loss: 0.5350 - val_acc: 0.7754\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.79622\n",
      "Epoch 15/50\n",
      "193/192 [==============================] - 65s 336ms/step - loss: 0.6245 - acc: 0.6464 - val_loss: 0.5234 - val_acc: 0.7904\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.79622\n",
      "Epoch 16/50\n",
      "193/192 [==============================] - 64s 330ms/step - loss: 0.6101 - acc: 0.6600 - val_loss: 0.5901 - val_acc: 0.7090\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.79622\n",
      "Epoch 17/50\n",
      "193/192 [==============================] - 64s 330ms/step - loss: 0.6238 - acc: 0.6373 - val_loss: 0.5149 - val_acc: 0.7949\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.79622\n",
      "Epoch 18/50\n",
      "193/192 [==============================] - 63s 328ms/step - loss: 0.6121 - acc: 0.6483 - val_loss: 0.5200 - val_acc: 0.7962\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.79622\n",
      "Epoch 19/50\n",
      "193/192 [==============================] - 63s 327ms/step - loss: 0.6103 - acc: 0.6593 - val_loss: 0.5446 - val_acc: 0.7793\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.79622\n",
      "Epoch 20/50\n",
      "193/192 [==============================] - 63s 329ms/step - loss: 0.6115 - acc: 0.6567 - val_loss: 0.5098 - val_acc: 0.7897\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.79622\n",
      "Epoch 21/50\n",
      "193/192 [==============================] - 63s 328ms/step - loss: 0.6247 - acc: 0.6444 - val_loss: 0.5055 - val_acc: 0.8079\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.79622 to 0.80794, saving model to xception_3.h5\n",
      "Epoch 22/50\n",
      "193/192 [==============================] - 64s 329ms/step - loss: 0.6151 - acc: 0.6528 - val_loss: 0.5113 - val_acc: 0.8145\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.80794 to 0.81445, saving model to xception_3.h5\n",
      "Epoch 23/50\n",
      "193/192 [==============================] - 64s 330ms/step - loss: 0.5995 - acc: 0.6600 - val_loss: 0.5017 - val_acc: 0.8105\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.81445\n",
      "Epoch 24/50\n",
      "193/192 [==============================] - 63s 328ms/step - loss: 0.6148 - acc: 0.6477 - val_loss: 0.5128 - val_acc: 0.8040\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.81445\n",
      "Epoch 25/50\n",
      "193/192 [==============================] - 64s 333ms/step - loss: 0.6077 - acc: 0.6496 - val_loss: 0.5160 - val_acc: 0.8105\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.81445\n",
      "Epoch 26/50\n",
      "193/192 [==============================] - 64s 329ms/step - loss: 0.6053 - acc: 0.6574 - val_loss: 0.5215 - val_acc: 0.7936\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.81445\n",
      "Epoch 27/50\n",
      "193/192 [==============================] - 64s 330ms/step - loss: 0.6166 - acc: 0.6386 - val_loss: 0.5132 - val_acc: 0.8079\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.81445\n",
      "Epoch 28/50\n",
      "193/192 [==============================] - 63s 329ms/step - loss: 0.6099 - acc: 0.6509 - val_loss: 0.5287 - val_acc: 0.8014\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.81445\n",
      "Epoch 29/50\n",
      "193/192 [==============================] - 64s 330ms/step - loss: 0.6103 - acc: 0.6516 - val_loss: 0.5604 - val_acc: 0.8118\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.81445\n",
      "Epoch 30/50\n",
      "193/192 [==============================] - 64s 332ms/step - loss: 0.6010 - acc: 0.6587 - val_loss: 0.5151 - val_acc: 0.7962\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.81445\n",
      "Epoch 31/50\n",
      "193/192 [==============================] - 64s 329ms/step - loss: 0.6207 - acc: 0.6451 - val_loss: 0.5139 - val_acc: 0.8053\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.81445\n",
      "Epoch 32/50\n",
      "193/192 [==============================] - 64s 330ms/step - loss: 0.6152 - acc: 0.6393 - val_loss: 0.5141 - val_acc: 0.7826\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.81445\n",
      "Epoch 33/50\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.6132 - acc: 0.6510"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-60b03057aa2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb_validation_samples\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m callbacks = [checkpoint, early])\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1424\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    209\u001b[0m                             \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m                             max_queue_size=max_queue_size)\n\u001b[0m\u001b[1;32m    212\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                         \u001b[0;31m# No need for try/except because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m   1478\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1480\u001b[0;31m             verbose=verbose)\n\u001b[0m\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(model, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m    323\u001b[0m                                  \u001b[0;34m'or (x, y). Found: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m                                  str(generator_output))\n\u001b[0;32m--> 325\u001b[0;31m             \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtest_on_batch\u001b[0;34m(self, x, y, sample_weight)\u001b[0m\n\u001b[1;32m   1259\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1261\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model \n",
    "model_final.fit_generator(\n",
    "train_generator,\n",
    "steps_per_epoch = nb_train_samples / batch_size,\n",
    "epochs = epochs,\n",
    "validation_data = validation_generator,\n",
    "validation_steps = nb_validation_samples / batch_size,\n",
    "callbacks = [checkpoint, early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set more layers to trainable\n",
    "    \n",
    "for layer in model_final.layers[:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model \n",
    "model_final.compile(optimizer=optimizers.Adam(),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "193/192 [==============================] - 126s 651ms/step - loss: 0.7146 - acc: 0.5557 - val_loss: 0.9671 - val_acc: 0.6654\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.81445\n",
      "Epoch 2/50\n",
      "193/192 [==============================] - 114s 590ms/step - loss: 0.6959 - acc: 0.5201 - val_loss: 0.7055 - val_acc: 0.4902\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.81445\n",
      "Epoch 3/50\n",
      "193/192 [==============================] - 114s 590ms/step - loss: 0.6871 - acc: 0.5395 - val_loss: 0.9127 - val_acc: 0.6113\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.81445\n",
      "Epoch 4/50\n",
      "193/192 [==============================] - 114s 591ms/step - loss: 0.6839 - acc: 0.5609 - val_loss: 0.8430 - val_acc: 0.6595\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.81445\n",
      "Epoch 5/50\n",
      "193/192 [==============================] - 114s 592ms/step - loss: 0.7011 - acc: 0.5525 - val_loss: 0.6949 - val_acc: 0.5052\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.81445\n",
      "Epoch 6/50\n",
      "193/192 [==============================] - 114s 590ms/step - loss: 0.7211 - acc: 0.5110 - val_loss: 0.6937 - val_acc: 0.5039\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.81445\n",
      "Epoch 7/50\n",
      "193/192 [==============================] - 114s 589ms/step - loss: 0.6951 - acc: 0.5032 - val_loss: 0.6932 - val_acc: 0.5052\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.81445\n",
      "Epoch 8/50\n",
      "193/192 [==============================] - 113s 588ms/step - loss: 0.6936 - acc: 0.5052 - val_loss: 0.6927 - val_acc: 0.5052\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.81445\n",
      "Epoch 9/50\n",
      " 95/192 [=============>................] - ETA: 40s - loss: 0.6933 - acc: 0.4961"
     ]
    }
   ],
   "source": [
    "# Train the model \n",
    "model_final.fit_generator(\n",
    "train_generator,\n",
    "steps_per_epoch = nb_train_samples / batch_size,\n",
    "epochs = epochs,\n",
    "validation_data = validation_generator,\n",
    "validation_steps = nb_validation_samples / batch_size,\n",
    "callbacks = [checkpoint, early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = applications.mobilenetv2.MobileNetV2(include_top=False, input_shape = (img_width, img_height, 3), pooling='avg')\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(base_model.output)\n",
    "model_final = Model(base_model.input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1538 images belonging to 2 classes.\n",
      "Found 384 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# compile the model \n",
    "model_final.compile(optimizer=optimizers.Adam(lr=0.00005),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Initiate the train and test generators with data Augumentation \n",
    "train_datagen = ImageDataGenerator(\n",
    "rescale = 1./255,\n",
    "horizontal_flip = True,\n",
    "fill_mode = \"nearest\",\n",
    "zoom_range = 0.3,\n",
    "width_shift_range = 0.3,\n",
    "height_shift_range=0.3,\n",
    "rotation_range=30)\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "rescale = 1./255,\n",
    "horizontal_flip = True,\n",
    "fill_mode = \"nearest\",\n",
    "zoom_range = 0.3,\n",
    "width_shift_range = 0.3,\n",
    "height_shift_range=0.3,\n",
    "rotation_range=30)\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "train_data_dir,\n",
    "target_size = (img_height, img_width),\n",
    "batch_size = batch_size, \n",
    "class_mode = \"binary\")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "validation_data_dir,\n",
    "target_size = (img_height, img_width),\n",
    "class_mode = \"binary\")\n",
    "\n",
    "# Save the model according to the conditions  \n",
    "checkpoint = ModelCheckpoint(\"Mobile_test.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=15, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "193/192 [==============================] - 2418s 13s/step - loss: 0.4142 - acc: 0.8025 - val_loss: 0.3136 - val_acc: 0.8672\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.86719, saving model to Mobile_test.h5\n",
      "Epoch 2/50\n",
      "192/192 [============================>.] - ETA: 2s - loss: 0.3233 - acc: 0.8691 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-60b03057aa2d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mvalidation_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnb_validation_samples\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m callbacks = [checkpoint, early])\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1424\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1425\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1426\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1428\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    209\u001b[0m                             \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m                             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m                             max_queue_size=max_queue_size)\n\u001b[0m\u001b[0;32m    212\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m                         \u001b[1;31m# No need for try/except because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[1;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[0;32m   1478\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1479\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1480\u001b[1;33m             verbose=verbose)\n\u001b[0m\u001b[0;32m   1481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1482\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[1;34m(model, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[0;32m    323\u001b[0m                                  \u001b[1;34m'or (x, y). Found: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m                                  str(generator_output))\n\u001b[1;32m--> 325\u001b[1;33m             \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtest_on_batch\u001b[1;34m(self, x, y, sample_weight)\u001b[0m\n\u001b[0;32m   1259\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1260\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1261\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1262\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1263\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2661\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2662\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2663\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2630\u001b[0m                                 session)\n\u001b[1;32m-> 2631\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2632\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model \n",
    "model_final.fit_generator(\n",
    "train_generator,\n",
    "steps_per_epoch = nb_train_samples / batch_size,\n",
    "epochs = epochs,\n",
    "validation_data = validation_generator,\n",
    "validation_steps = nb_validation_samples / batch_size,\n",
    "callbacks = [checkpoint, early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
